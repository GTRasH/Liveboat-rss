<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News - Last 2 days</title><link>http://site-url-not-set.io/you-can-set-it-in-liveboat-config</link><description></description><item><title>Revisiting interface segregation in Go</title><link>https://www.reddit.com/r/golang/comments/1olzq5m/revisiting_interface_segregation_in_go/</link><author>/u/sigmoia</author><category>dev</category><pubDate>Sat, 1 Nov 2025 21:34:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/sigmoia ]]></content:encoded></item><item><title>introducing coral: a BLAS implementation in Rust for AArch64</title><link>https://github.com/devdeliw/coral</link><author>/u/Zealousideal-End9269</author><category>dev</category><pubDate>Sat, 1 Nov 2025 21:13:46 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[   submitted by    /u/Zealousideal-End9269 ]]></content:encoded></item><item><title>to transaction or not to transaction</title><link>https://www.reddit.com/r/golang/comments/1olxt4z/to_transaction_or_not_to_transaction/</link><author>/u/PancakeWithSyrupTrap</author><category>dev</category><pubDate>Sat, 1 Nov 2025 20:15:00 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Take this simplistic code:func create(name string) error {if err != nil { return err }err := writeToDatabase(name)if err != nil { return err}func newDisk(name) error {name, err := getDisk(name)if err != nil { return err }if name != "" { return nil }if err != nil { return err}This creates a disk and database record.The `newDisk` function idempotently creates a disk. Why ? If writing a database record fails, there is an inconsistency. A real resource is created but there is no record of it. When client receives an error presumably it will retry, so a new disk will not be created and hopefully the database record is written. Now we are in a consistent state.But is this a sensible approach ? In other words, shouldn't we guarantee we are always in a consistent state ? I'm thinking creating the disk and writing a database record should be atomic.]]></content:encoded></item><item><title>Compile from a git repo but make changes</title><link>https://www.reddit.com/r/golang/comments/1olx8jv/compile_from_a_git_repo_but_make_changes/</link><author>/u/No-Confection8657</author><category>dev</category><pubDate>Sat, 1 Nov 2025 19:51:46 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I am running a VPS with ubuntu aarch64 and have go 1.25. I am trying to compile a program from a repo that is written in go but want to also implement a change from a pull request. The repo isn't mine, though I do have a fork of it on my git. I installed task and followed the steps in the contributing.md file. When I "task deps" it did spit out an error that was basically the same as when I was doing it passing go commands manually:I decided to just try ignoring that and running "task" to build it. And it seemed to compile and I have successfully ran it.Here is my issue now - I manually made the changes to the VERSION and internal/tgc/channel_manager.go files locally before running this but I think it just went ahead and used the original versions ignoring my changeswhen I run teldrive version it spits out 1.7.0 and the changes to the version file is 1.7.1 - also the file that got generated is the exact same amount of bytes as the 1.7.0 release. So I think it just made the file with none of the changes I had manually input into the local copies of the files.Then when I run task, it exits with the following error:task: Failed to run task "default": task: Command "go run scripts/release.go --version current" failed: exit status 1not sure what would cause this - when I look at that file, it seems to just reference the VERSION file to get the version number. and it simply says 1.7.1 instead of 1.7.0Am I missing something obvious? Sorry for the long post, I am new at this.]]></content:encoded></item><item><title>EKS 1.33 cause networking issue when running very high mqtt traffic</title><link>https://www.reddit.com/r/kubernetes/comments/1olx02b/eks_133_cause_networking_issue_when_running_very/</link><author>/u/imuskie</author><category>dev</category><pubDate>Sat, 1 Nov 2025 19:42:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Let's say I'm running some high workload on AWS EKS (mqtt traffic from devices). I'm using VerneMQ broker for this. Everything have worked fine until I've upgraded the cluster to 1.33.The flow is like this: mqtt traffic -> ALB (vernemq port) -> vernemq kubernetes service -> vernemq pods.There is another pod which subscribes to a topic and reads something from vernemq (some basic stuff). The issue is that, after the upgrade, that pod fails to reach the vernemq pods. (pod crashes its liveness probe/timeouts). This happens only when I get very high mqtt traffic on ALB (hundreds of thousands of requests). For low traffic everything works fine. One workaround I've found is to edit that container image code to connect to vernemq using external ALB instead of vernemq kubernetes service (with this change, the issue is fixed) but I don't want this. I did not change anything on infrastructure/container code. I'm running on EKS since 1.27.I don't know if the base AMI is the problem or not (like kernel configs have changed).I'm running in AL2023, so with the base AMI on eks 1.32 works fine, but with 1.33 it does not.I'm using amazon aws vpc cni plugin for networking.Are there any tools to inspect the traffic/kernel calls or to better monitor this issue? ]]></content:encoded></item><item><title>Understanding Multi-Platform Docker Builds with QEMU</title><link>https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/</link><author>/u/Helpful_Geologist430</author><category>dev</category><pubDate>Sat, 1 Nov 2025 19:14:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[One intriguing feature of containers and images is their multi-platform support. Docker uses Buildx, which is based on BuildKit, to enable multi-platform builds.The old way (build for the current host’s platform, if you’re on an ARM CPU, you build an ARM image that won’t run on x86, and vice versa):The new way: without a single care, build a multi-platform image that supports both x86 and ARM (and others):docker buildx build  linux/amd64,linux/arm64 How is this sorcery possible? Let’s take a look.Containers, under the hood, are simply processes isolated thanks to Linux’s namespaces. The executables and files of these processes, packaged in layers, are compiled for a specific architecture.Put differently, a container is a bundled runtime. This is what the OCI runtime bundle defines:coolcontainer/
├── config.json
└── rootfs/
    ├── bin/
    ├── lib/
    └── ...
An OCI runtime bundle (used to start a container) is obtained from an OCI image (Docker images are OCI-compliant).1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
apt  umoci skopeo runc


skopeo copy docker://alpine:latest oci:alpine:latest

alpine/

umoci unpack  alpine:latest alpine-runtime-bundle
alpine-runtime-bundle/

runc run  alpine-runtime-bundle mycoco
    yo  /home/greeting
    alpine-runtime-bundle/rootfs/home/greeting
This spec defines what’s needed to run a container. All OCI-compliant container solutions adhere to it (Docker, Podman, etc.). These files are then used to create a container process. The isolation is achieved through Linux namespaces. To the container, it feels like it’s running on its own filesystem, network, PID space, and so on, but in reality, it’s just a process, albeit a well-isolated one.The reference implementation that takes an OCI runtime bundle and starts a container is  (Docker uses it under the hood). It takes all the information and layers in the bundle and creates the container process. Mounts, environment variables, and all kinds of options that can be specified when running a container are handled by .This means that the executables and binaries are destined for a specific OS and architecture:file  alpine-runtime-bundle/rootfs/bin/ls
alpine-runtime-bundle/rootfs/bin/ls: ELF 64-bit LSB executable, ARM aarch64
So the  command inside the container layers is simply a regular executable built for ARM64.You can’t just run an image built for an x86 CPU on an ARM CPU (out of the box). That’s where multi-platform images come into the picture.An image that supports multiple architectures? Say what?skopeo inspect  docker://docker.io/library/ubuntu:latest | jq | 
        ...
1
2
3
4
5
6
7
8
9
10
11
12

skopeo copy  amd64  linux 
  docker://docker.io/library/nginx:latest 
  oci:nginx-amd64:latest

umoci unpack  nginx-amd64:latest nginx-amd64-runtime-bundle


file  nginx-amd64-runtime-bundle/rootfs/bin/ls
J’accuse! Intruder! An x86-64 binary on an ARM machine!./alpine-runtime-bundle/rootfs/bin/ls

./nginx-amd64-runtime-bundle/rootfs/bin/ls

runc run  nginx-amd64-runtime-bundle my-nginx-container
And that’s the heart of the problem when it comes to running containers across different platforms. What to do?QEMU (Quick EMUlator) is quite the remarkable piece of software. It’s both an emulator and a virtualizer, and it also provides user-level emulation. Ehhh, what? Well, that’s what you get when you look up QEMU. Let’s put it in simpler terms:Emulator: It emulates hardware. It simulates entire systems (CPU, memory, disk, network, etc.) in software, meaning it exposes an interface to a guest program similar to actual hardware. Think about it: for an OS, all it sees is a bunch of CPU machine code that interacts with hardware and registers. If those registers and hardware behaviors are simulated in software, the OS is none the wiser and that’s exactly what QEMU does. You can simulate different CPUs (ARM, x86, RISC-V, etc.), run machine code instructions, and update state (registers, flags, program counter, etc.) as if you were running on real hardware, it’s just slower. By emulating CPUs in software, QEMU can run an OS built for the same or a different CPU architecture.Virtualizer: Some CPUs offer hardware-assisted virtualization, basically, the CPU can differentiate between a guest and a host OS. This is a lot faster than using an emulator, but since you’re using the same CPU, you can only run a guest OS built for that CPU (for example, an x86 Linux guest on an x86 Linux host). This is supported in Linux through KVM. QEMU can make use of KVM, so when available, it’s better to use it for faster guest execution.User-space emulation: This allows us to run a binary built for an architecture different from our machine’s by translating machine code and system calls on the fly. For instance,  works on an  CPU as if it were native. It’s truly magical, QEMU decodes ARM instructions and translates them into x86-64 ones, roughly:ARM code:          ADD R0, R1, R2
QEMU intermediate: tcg_gen_add_i32(result, R1, R2)
x86-64 host code:  mov eax,[R1]; add eax,[R2]; mov [R0],eax
So QEMU user-space emulation is the first piece of the cross-platform image puzzle.The second piece is . It stands for Binary Format Miscellaneous (quite the name). The basic idea is that your Linux kernel knows how to run executables built for its own architecture. If you’re on an x86-64 CPU, your kernel can run x86-64 ELF files by default. It can’t run executables built for other architectures (like ARM) or other file types (like Windows  files or scripts). is a kernel feature that allows us to specify an interpreter or program to handle certain files, based on their extension or on a magic byte sequence contained within the file. ARM Linux executables, for instance, have a distinguishable magic sequence:  We can configure  to use  whenever it encounters a file with that magic sequence. Similarly, we can configure it to use  when encountering files with a  extension.1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
 |  /proc/sys/fs/binfmt_misc/register


./HelloWorld.jar 
 +x /usr/local/bin/java-wrapper

 |  /proc/sys/fs/binfmt_misc/register

./HelloWorld.jar
So,  lets the kernel specify a wrapper, interpreter, or command to run certain files based on their magic bytes or file extensions.To recap:  allows us to execute binaries from other architectures, and  is the mechanism that maps those binaries (based on their magic bytes) to the appropriate QEMU user-space command.In Docker’s documentation about multi-platform builds, they explain that Docker Desktop supports multi-platform images with QEMU out of the box. (Docker Desktop is essentially a Linux VM tailored to run Docker, so it already has this configured.)For Docker engine in Linux, we need to run:docker run  tonistiigi/binfmt  all
This registers the  mappings (like we did above for Java and x86) but for all architectures.The image tonistiigi/binfmt contains a Go binary that basically does what we demonstrated earlier, setting up mappings from ELF magic bytes to the appropriate QEMU binary for multiple architectures:1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
In , we find this delightful snippet:Ok, we know how foreign binaries are run. But how does it all tie together? How are we actually building these multi-platform images?The Docker docs have a nice example:
FROM alpine

RUN  /arch
docker buildx build linux/amd64,linux/arm64  letsgo:1.0 

docker run linux/arm64 letsgo:1.0  /arch


docker run linux/amd64 letsgo:1.0  /arch
It’s beautiful! So what happened exactly? By specifying --platform=linux/amd64,linux/arm64, we’re asking Docker to build two images, one for each platform. The pulled base layer () is platform-specific, and the binaries within it are built for each architecture. Let’s verify that:docker run linux/arm64 letsgo:1.0 sh
apk add file 

file  /bin/uname
Nice! The  runs , and that’s where the QEMU magic occurs. Under the hood, each binary in the image layers, compiled for its target architecture, is executed. Docker’s  instruction spawns a new process on the host (isolated within namespaces, but still just a process). Depending on the file’s magic bytes, the appropriate QEMU interpreter is invoked automatically via  and that RUN command works. If it was not for  and QEMU, we’d get a polite .QEMU isn’t the only way to build multi-platform images. Docker Buildx supports using multiple builder nodes (a cluster), and you can use nodes with different architectures to build images natively for their respective platforms. There’s even a cloud offering built around this approach.
docker buildx create  multiarch-builder unix:///var/run/docker.sock

docker buildx create  multiarch-builder ssh://user@arm64-host
docker buildx use multiarch-builder
For compilers that support cross-compilation (compiling code on one platform, the host, to create an executable for a different platform ,the target), like Go, which does so natively by specifying  and , you can build directly for each target architecture without relying on emulation. For example, in a Dockerfile build stage you might run:RUN  go build  server Then, you can simply copy the resulting binary into the runtime stage. Since the Go compiler supports cross-compilation, there’s no need to use QEMU here. Instead, we rely on the  and  environment variables provided automatically by Docker Buildx.]]></content:encoded></item><item><title>Czech police forced to turn off facial recognition cameras at the Prague airport</title><link>https://edri.org/our-work/czech-police-forced-to-turn-off-facial-recognition-cameras-at-the-prague-airport-thanks-to-the-ai-act/</link><author>campuscodi</author><category>dev</category><pubDate>Sat, 1 Nov 2025 18:42:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Airport facial recognition system long criticisedThe Czech Republic Police used a camera system with facial recognition capabilities at Václav Havel Airport in Prague from 2018, until it was shut down in August 2025. The system enabled real-time recognition of the faces of people passing through the airport. Their so-called bio-indexes, or, simply put, facial contours converted into numbers, were compared with a database of wanted or missing persons.EDRi member IuRe drew attention to the situation back in 2021. At the time, IuRe lawyers argued that the processing of biometric data in Czechia is only possible on the basis of explicit permission granted by a special law. IuRe ultimately filed a complaint with the Czech Data Protection Authority (DPA), requesting an investigation. The result of the inspection, which IuRe requested in the summer of 2025 under the Freedom of Information Act, confirmed the suspicion of a violation of personal data protection legislation.Biometric surveillance ended thanks to the AI ActCriticism of the facial recognition system only increased after the AI Act came into force because the law explicitly requires judicial approval for each use of such a system, which wasn’t provided for the airport.Therefore, since the specific portion of the AI Act related to biometric surveillance came into force in February 2025, till the airport facial recognition systém was shut down in August 2025, the police’s use of this system was illegal. It was in operation despite the fact that the police had been repeatedly warned of its illegality and the media had also taken an interest in the matter.Set clear boundaries for the policeThe inspection by the Czech DPA took . During that time, no effective action was taken. However, the results are clear: police need to be given clear guidelines for processing biometric data, which should be enshrined in laws approved by elected representatives of the people and thus subject to public scrutiny. The current situation, apart from violating European legislation, creates a fertile ground for various forms of abuse of these technologies.The police systematically violate laws when processing biometric dataThe Czech police are also ignoring the law in the case of another biometric tool – Digital Personal Image Information System. This was also pointed out by IuRe and subsequently by the Czech DPA. The system works with a reference database of approximately 20 million photographs of all persons who have been issued identity cards or passports, and compares them with photographs of persons of unknown identity.This makes it possible to trace their probable identity retrospectively. According to the police, the system is used, for example, to identify the deceased. However, the same system can also be concievably used to identify people participating in demonstrations.The obvious systemic problems with the use of facial recognition tools by the police should therefore be a matter of concern for the new Czech Minister of the Interior, who should initiate a review of the legislation. The current national legislation  with the European directive in terms of legal safeguards for the processing of biometric data.IuRe will continue to monitor biometric surveillance in Czechia, thanks in part to financial support from the public received through their informational website about biometric surveillance called Czechia is not China. The website was created with the support of EDRi and was linked to a crowdfunding campaign.]]></content:encoded></item><item><title>Claude Code Can Debug Low-Level Cryptography</title><link>https://words.filippo.io/claude-debugging/</link><author>Bogdanp</author><category>dev</category><pubDate>Sat, 1 Nov 2025 18:41:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Over the past few days I wrote a new Go implementation of ML-DSA, a post-quantum signature algorithm specified by NIST last summer. I livecoded it all over four days, finishing it on Thursday evening. Except… Verify was always rejecting valid signatures.$ bin/go test crypto/internal/fips140/mldsa
--- FAIL: TestVector (0.00s)
    mldsa_test.go:47: Verify: mldsa: invalid signature
    mldsa_test.go:84: Verify: mldsa: invalid signature
    mldsa_test.go:121: Verify: mldsa: invalid signature
FAIL
FAIL     crypto/internal/fips140/mldsa   2.142s
FAIL
I was exhausted, so I tried debugging for half an hour and then gave up, with the intention of coming back to it the next day with a fresh mind.On a whim, I figured I would let Claude Code take a shot while I read emails and resurfaced from hyperfocus. I mostly expected it to flail in some maybe-interesting way, or rule out some issues.Instead, it rapidly figured out a fairly complex low-level bug in my implementation of a relatively novel cryptography algorithm. I am sharing this because it made me realize I still don’t have a good intuition for when to invoke AI tools, and because I think it’s a fantastic case study for anyone who’s still skeptical about their usefulness.Full disclosure: Anthropic gave me a few months of Claude Max for free. They reached out one day and told me they were giving it away to some open source maintainers. Maybe it’s a ploy to get me hooked so I’ll pay for it when the free coupon expires. Maybe they hoped I’d write something like this. Maybe they are just nice. Anyway, they made no request or suggestion to write anything public about Claude Code. Now you know.I started Claude Code v2.0.28 with Opus 4.1 and no system prompts, and gave it the following prompt (typos included):I implemented ML-DSA in the Go standard library, and it all works except that verification always rejects the signatures. I know the signatures are right because they match the test vector.YOu can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaLook for potential reasons the signatures don’t verify. ultrathinkI spot-checked and w1 is different from the signing one.Maybe I shouldn’t be surprised! Maybe it would have been clear to anyone more familiar with AI tools that this was a good AI task: a well-scoped issue with failing tests. On the other hand, this is a low-level issue in a fresh implementation of a complex,  algorithm.It figured out that I had merged  and  into a single function for using it from Sign, and then reused it from Verify where  already produces the high bits, effectively taking the high bits of w1 twice in Verify.Looking at the log, it loaded the implementation into the context and then  figured it out, without any exploratory tool use! After that it wrote itself a cute little test that reimplemented half of verification to confirm the hypothesis, wrote a mediocre fix, and checked the tests pass.I threw the fix away and refactored  to take high bits as input, and changed the type of the high bits, which is both clearer and saves a round-trip through Montgomery representation. Still, this 100% saved me a bunch of debugging time.A second synthetic experimentOn Monday, I had also finished implementing signing with failing tests. There were two bugs, which I fixed in the following couple evenings.I figured these would be an interesting way to validate Claude’s ability to help find bugs in low-level cryptography code, so I checked out the old version of the change with the bugs (yay Jujutsu!) and kicked off a fresh Claude Code session with this prompt:I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector it looks like it goes into an infinite loop, probably because it always rejects in the Fiat-Shamir with Aborts loop.You can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaFigure out why it loops forever, and get the tests to pass. ultrathinkIt gave up after fixing that bug even if the tests still failed, so I started a fresh session (on the assumption that the context on the wrong constants would do more harm than good investigating an independent bug), and gave it this prompt:I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector they don’t match.You can run the tests with “bin/go test crypto/internal/fips140/mldsa”You can find the code in src/crypto/internal/fips140/mldsaFigure out what is going on. ultrathinkIt’s interesting how Claude found the “easier” bug more difficult. My guess is that maybe the large random-looking outputs of the failing tests did not play well with its attention.The fix it proposed was updating only the allocation’s length and not its capacity, but whatever, the point is finding the bug, and I’ll usually want to throw away the fix and rewrite it myself anyway.Three out of three one-shot debugging hits with no help is . Importantly, there is no need to trust the LLM or review its output when its job is just saving me an hour or two by telling me where the bug is, for me to reason about it and fix it.As ever, I wish we had better tooling for using LLMs which didn’t look like chat or autocomplete or “make me a PR.” For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it?Enjoy the silliest floof. Surely this will help redeem me in the eyes of folks who consider AI less of a tool and more of something to be hated or loved.My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.)
Here are a few words from some of them!Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.]]></content:encoded></item><item><title>Visible from space, Sudan&apos;s bloodied sands expose a massacre of thousands</title><link>https://www.telegraph.co.uk/world-news/2025/10/28/sudan-bloodied-sands-massacre-thousands/</link><author>wslh</author><category>dev</category><pubDate>Sat, 1 Nov 2025 17:50:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Reports indicated that the RSF was deliberately forcing displaced civilians eastward into areas under its control, away from humanitarian hubs such as Tawila, where some international agencies were operating.According to Jeremy Konyndyk, president of Refugees International, the RSF was preventing people from fleeing the town in other directions, specifically blocking movement south and west, and compelling them to move east, where there was no safety or access to aid.Yvette Cooper, the Foreign Secretary, said: “We are witnessing a deeply disturbing pattern of abuses in El Fasher, including systematic killings, torture, and sexual violence.”The UN Human Rights Office said it had received “multiple, alarming reports that the RSF are carrying out atrocities, including summary executions”.Volker Türk, the UN human rights chief, said the risk of further large-scale, ethnically motivated violations and atrocities in El Fasher was “mounting by the day”.]]></content:encoded></item><item><title>Show HN: Why write code if the LLM can just do the thing? (web app experiment)</title><link>https://github.com/samrolken/nokode</link><author>samrolken</author><category>dev</category><pubDate>Sat, 1 Nov 2025 17:45:18 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I spent a few hours last weekend testing whether AI can replace code by executing directly. Built a contact manager where every HTTP request goes to an LLM with three tools: database (SQLite), webResponse (HTML/JSON/JS), and updateMemory (feedback). No routes, no controllers, no business logic. The AI designs schemas on first request, generates UIs from paths alone, and evolves based on natural language feedback. It works—forms submit, data persists, APIs return JSON—but it's catastrophically slow (30-60s per request), absurdly expensive ($0.05/request), and has zero UI consistency between requests. The capability exists; performance is the problem. When inference gets 10x faster, maybe the question shifts from "how do we generate better code?" to "why generate code at all?"]]></content:encoded></item><item><title>Ubuntu Will Use Rust For Dozens of Core Linux Utilities</title><link>https://news.slashdot.org/story/25/11/01/079206/ubuntu-will-use-rust-for-dozens-of-core-linux-utilities?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><pubDate>Sat, 1 Nov 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[ Ubuntu "is adopting the memory-safe Rust language," reports ZDNet, citing remarks at this year's Ubuntu Summit from Jon Seager, Canonical's VP of engineering for Ubuntu:

. Seager said the engineering team is focused on replacing key system components with Rust-based alternatives to enhance safety and resilience, starting with Ubuntu 25.10. He stressed that resilience and memory safety, not just performance, are the principal drivers: "It's the enhanced resilience and safety that is more easily achieved with Rust ports that are most attractive to me". This move is echoed in Ubuntu's adoption of sudo-rs, the Rust implementation of sudo, with fallback and opt-out mechanisms for users who want to use the old-school sudo command. 


In addition to sudo-rs, Ubuntu 26.04 will use the Rust-based uutils/coreutils for Linux's default core utilities. This setup includes ls, cp, mv, and dozens of other basic Unix command-line tools. This Rust reimplementation aims for functional parity with GNU coreutils, with improved safety and maintainability. 

On the desktop front, Ubuntu 26.04 will also bring seamless TPM-backed full disk encryption. If this approach reminds you of Windows BitLocker or MacOS FileVault, it should. That's the idea. 

In other news, Canonical CEO Mark Shuttleworth said "I'm a believer in the potential of Linux to deliver a desktop that could have wider and universal appeal." (Although he also thinks "the open-source community needs to understand that building desktops for people who aren't engineers is different. We need to understand that the 'simple and just works' is also really important.") 


Shuttleworth answered questions from Slashdot's readers in 2005 and 2012.]]></content:encoded></item><item><title>OpenAI Moves to Complete Potentially the Largest Theft in Human History</title><link>https://thezvi.substack.com/p/openai-moves-to-complete-potentially</link><author>paulpauper</author><category>dev</category><pubDate>Sat, 1 Nov 2025 17:25:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Where to begin with &quot;modern&quot; Emacs?</title><link>https://news.ycombinator.com/item?id=45783376</link><author>weakfish</author><category>dev</category><pubDate>Sat, 1 Nov 2025 17:13:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi all,I’m a longtime Neovim user who’s been EMacs-curious. The hold up for me has been that I’ve been unable to find a source of truth for what’s top-of-the-line as far as plugins are. With Neovim, it’s a safe bet to look at what folks like Folke are doing, but I have struggled to find a similar figure in the Emacs community who gives insight into what’s-what. I know Doom exists, but I want to fully “own” my config and not over complicate it.]]></content:encoded></item><item><title>Studies increasingly find links between air pollutants and dementia</title><link>https://www.nytimes.com/2025/11/01/health/alzheimers-dementia-air-pollution.html</link><author>quapster</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:54:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Chat Control proposal fails again after public opposition</title><link>https://andreafortuna.org/2025/11/01/chat-control-proposal-fails-again-after-massive-public-opposition/</link><author>speckx</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:42:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The  has once again retreated from its controversial Chat Control proposal, a plan that would have required widespread scanning of encrypted messages. The withdrawal by the current  represents yet another chapter in a long-running battle between privacy advocates and lawmakers who believe they can compromise encryption in the name of public safety. While this latest defeat is a victory for digital rights, the fight is far from over, and the fundamental misunderstanding of encryption technology continues to plague policy discussions across Europe.A zombie proposal that refuses to dieSince its introduction in 2022, Chat Control has become what privacy advocates call a , repeatedly resurrected despite consistent opposition from civil society, technical experts, and the public. The Electronic Frontier Foundation and more than 80 civil society organizations have strongly opposed the legislation, which would mandate client-side scanning of encrypted communications under the guise of combating child sexual abuse material.The pattern has become predictable. EU lawmakers introduce the proposal, claiming it includes safeguards for privacy. Technical experts explain why those safeguards are illusory. Public pressure mounts. The proposal is withdrawn or modified. Then, after a brief hiatus, it returns with minor tweaks, and the cycle begins anew. This latest withdrawal by the  follows the same script, but the underlying issues remain unresolved.What makes this particularly frustrating is that the fundamental problem with Chat Control has never been addressed. The proposal seeks to create what privacy experts call a “backdoor” into encryption, allowing authorities to scan messages before they’re encrypted or after they’re decrypted. Proponents argue this preserves encryption while enabling content moderation, but this reveals a dangerous misunderstanding of how encryption actually works. Creating any mechanism to access encrypted content inherently weakens the entire system, making it vulnerable not just to authorized access but to malicious actors as well.The technical impossibility of “safe” scanningThe core issue with Chat Control and similar proposals lies in a fundamental misunderstanding of encryption technology. End-to-end encryption works because only the sender and recipient possess the keys to decrypt messages. Any third party, whether a government agency or a tech company, cannot read the contents. This is not a design choice but a mathematical certainty that ensures the security of billions of communications daily.Client-side scanning, the technical approach favored by Chat Control advocates, attempts to circumvent this limitation by analyzing messages on users’ devices before encryption or after decryption. While this might sound like a clever workaround, it fundamentally breaks the security model of encryption. If a device can scan and report on message content, so can malware, hackers, or authoritarian governments who might compel tech companies to expand the scope of scanning.Security researchers have repeatedly demonstrated that there is no way to create a scanning system that only works for “good guys.”  learned this lesson the hard way in 2021 when it proposed a similar system for detecting child abuse imagery in iCloud photos. The backlash from security experts was swift and devastating, forcing the company to abandon the plan. The same security vulnerabilities that would enable Chat Control would inevitably be exploited by malicious actors, putting everyone at greater risk.Moreover, the scope creep inherent in surveillance technologies is well documented. A system initially designed to detect illegal content could easily be expanded to monitor political dissent, religious expression, or any other communication governments deem problematic. Countries around the world are watching the EU’s actions closely. If Chat Control were to pass, it would set a dangerous precedent that authoritarian regimes would eagerly exploit, claiming they’re simply following Europe’s lead in implementing “reasonable” content moderation.Public pressure and the power of resistanceThe withdrawal of Chat Control demonstrates the critical importance of sustained public engagement in technology policy. Unlike previous instances where technical proposals sailed through legislative processes with little public awareness, this fight has been characterized by unprecedented mobilization from civil society organizations, technology companies, security researchers, and ordinary citizens concerned about their digital rights.Organizations like the Electronic Frontier Foundation, , and numerous national privacy advocacy groups have played a crucial role in educating the public about the risks of Chat Control. Their efforts have included detailed technical explanations, legal analysis, and coordination of opposition campaigns that have reached millions of Europeans. This groundswell of opposition has made it politically toxic for lawmakers to support the proposal, at least in its current form.The effectiveness of this resistance offers important lessons for future policy battles. First, technical expertise matters. When security researchers speak with a unified voice about the impossibility of safe backdoors, it becomes harder for politicians to dismiss concerns as alarmist. Second, coalition-building across different sectors strengthens opposition. When civil liberties groups, tech companies, and individual users all oppose a policy, it suggests the problems are real and widespread. Third, sustained pressure is essential because, as Chat Control demonstrates, bad proposals rarely die on the first attempt.However, this victory should be tempered with realism. The forces pushing for Chat Control have not given up, and the underlying political dynamics that gave rise to the proposal remain unchanged. Politicians face genuine pressure to be seen as “doing something” about online harms, particularly regarding child safety. Until alternative approaches that don’t compromise encryption gain political traction, proposals like Chat Control will continue to resurface.The path forward requires education and alternativesThe repeated resurrection of Chat Control points to a deeper problem in how technology policy is made. Many lawmakers genuinely believe they can have both strong encryption and government access to encrypted content. This belief persists despite unanimous opposition from the cryptographic community because the political incentives favor appearing tough on crime over understanding complex technical realities.Breaking this cycle requires a fundamental shift in how we approach online safety. Rather than seeking technological magic bullets that promise security without trade-offs, policymakers need to invest in solutions that actually work. This includes better funding for law enforcement training and tools that don’t require breaking encryption, improved international cooperation on criminal investigations, and addressing the root causes of online exploitation through social programs and education. also bear responsibility for developing and promoting genuinely privacy-preserving safety features. End-to-end encrypted platforms can implement abuse prevention measures that don’t involve content scanning, such as metadata analysis, user reporting systems, and account-level restrictions for suspicious behavior. While these approaches may be less comprehensive than mass surveillance, they achieve meaningful safety improvements without the catastrophic privacy trade-offs of backdoors.Looking ahead, the privacy community cannot simply celebrate the withdrawal of Chat Control and move on. The next presidency of the EU Council will bring new opportunities for the proposal to resurface in yet another modified form. Sustained vigilance, continued public education, and proactive development of alternative safety measures will be essential. The fight to protect encryption is not a single battle but an ongoing campaign that requires long-term commitment from everyone who values digital privacy and security.The withdrawal of Chat Control is a victory, but it’s a temporary one. The fundamental challenge remains: convincing policymakers that some trade-offs are not worth making, and that breaking encryption to combat illegal content creates far more problems than it solves. Until that message truly sinks in, the zombie proposal will keep rising from the grave, and the privacy community must remain ready to defeat it again and again.]]></content:encoded></item><item><title>Do I need to think in accordance to endianness for SIMD?</title><link>https://www.reddit.com/r/rust/comments/1olsaxo/do_i_need_to_think_in_accordance_to_endianness/</link><author>/u/playbahn</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:34:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[For context, I have never really read about about SIMD, apart for YT etc. But I  fascinated about SIMD, and I came across this article below.In Designing a SIMD Algorithm from Scratch the author is doing all sorts of bit manipulation like reversing the bits and changing their endianness: ``` fn bits(value: u32) -> String { let [b1, b2, b3, b4] = value.reverse_bits().to_le_bytes(); format!("{b1:08b} {b2:08b} {b3:08b} {b4:08b}") }fn decode_pack(input: [u8; 4]) { let mut output = 0u32; for byte in input { output <<= 6; output |= byte as u32; } output <<= 8;println!("{}\n{}\n", bits(u32::from_be_bytes(input)), bits(output)); }decode_pack([0b111111, 0, 0, 0]); decode_pack([0, 0b111111, 0, 0]); decode_pack([0, 0, 0b111111, 0]); decode_pack([0, 0, 0, 0b111111]); ``` I do (kind of) understand where a bit from input will end up in in the output, but why are we doing all this? Why don't we just not reverse the bits, and show them as they are, i.e. Big Endian (I do get our CPUs are mostly LE, but BE is simpler). When writing SIMD code, do we always have to think in terms of LE?]]></content:encoded></item><item><title>DigitalOcean is chasing me for $0.01: What it taught me about automation</title><link>https://linuxblog.io/digitalocean-1-cent-automation/</link><author>/u/modelop</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:29:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There are three kinds of emails that can ruin a quiet Saturday: a security warning, an outage alert, and, apparently, a repeat reminder that you owe a cloud provider one cent, yes, $0.01. I’ve been using DigitalOcean since 2013. Personally, I don’t use it often, but I log in several times a week to support clients hosted there.A chuckle and twelve years of cloud loveOver the past twelve years I have set up and managed countless droplets, and DigitalOcean’s support and uptime have been excellent; this isn’t that kind of post.It’s a lighthearted look at what happens when automation churns out more notifications than the situation may warrant, and why even a penny‑sized bill can teach us bigger lessons about design and efficiency.On Saturday, 25 October 2025, an email with the subject “Payment required: Your pre‑payment has been used” arrived in my inbox. It informed me that my prepaid credit was insufficient to cover the month’s usage and urged me to “make another payment or add an alternate payment method.”There was just one catch: the outstanding balance was $0.01. I chuckled, and went on with my day only to receive the exact message two more times over the coming days. By the time Saturday rolled around, my inbox looked like this:The inbox search screenshot above shows the cadence: identical “Payment required” messages on October 25th, 28th, and 31st, 2025, followed by an email on November 1, 2025, titled “Your 2025‑10 invoice is available.”The invoice email (screenshot also above) contains a table that lists the usage charges for October as $0.01, notes that the payment method will only be charged if the balance exceeds $3.00, and invites me to “View Invoice.” Here’s what those other three messages look like:My immediate reaction was a bit of a chuckle, but by the fourth email, I was more curious than anything: Why does an automated billing system send four emails about a 1-cent balance? DigitalOcean’s billing documentation notes that invoices are generated monthly. In my case, the system sent several “action required” emails, maybe because I don’t have a payment method saved? But in any case, I rarely use my personal DigitalOcean account beyond just quick tests:This experience of multiple emails for 1 cent owed, prompted me to think about the hidden costs of excessive email notifications and how we can design billing and alerting in a more thoughtful way.The True Cost of an EmailEmail feels free because individuals don’t pay per message, but providers do. A 2025 breakdown of email marketing costs notes that the typical cost for a business to send emails is $1–$2 per thousand messages, translating to roughly $0.001–$0.002 per email. Amazon’s Simple Email Service charges $0.10 per 1,000 emails for outbound messages (sending or receiving) and a few cents per gigabyte for attachments.This cost is likely less for DigitalOcean, with the three “Payment required” notices and one invoice with attachment costing the company at most between a tenth and two‑tenths of a cent to send. But multiply that by hundreds of thousands of customers, and it highlights how easy it is to use resources to clutter inboxes over microbalances.The monetary cost is only part of the picture. Email has an environmental footprint because electricity powers servers, networks, and client devices. Researchers estimate that more than 306 billion emails were sent in 2021, and the total is expected to hit almost 400 billion this year, thanks to DigitalOcean. jk!!According to Mike Berners‑Lee, a short text email can produce 0.2–0.3 g of CO₂, while a longer message with attachments can produce 17 g; an email blast to 100 people may generate 26 g or more. Email‑related emissions accounted for approximately 150 million tons of CO₂e in 2019. That’s about 0.3% of the world’s carbon footprint. But more importantly, about 25% added to users’ annoyance levels – Source: Notification fatigue and design principlesIt isn’t just about costs or the environment. Usability tests consistently show that frequent alerts are one of the top user complaints. In fact, it’s been proven by Facebook and others that sending fewer notifications can be better for both engagement and retention.Good notification design also recognizes levels of severity: high‑attention alerts (e.g., security breaches or failed payments) should prompt immediate action, while low‑attention messages (informational updates) can be bundled or deferred. Services like Slack, for example, adapt notification frequency automatically when channels become very active.Looking at DigitalOcean’s billing reminders, it’s easy to see opportunities for improvement. A one‑cent balance does not warrant three emails + an invoice. The first message could have been informational (“heads up, your balance is low”), the second might wait until the balance crosses a predetermined threshold (say $1 or $3), and the third could be a month or 3 months later.Alternatively, DigitalOcean could incorporate a small balance waiver similar to the one many credit card issuers use. Banks recognize that it’s not cost‑effective to chase pennies; they round down or apply a credit adjustment on the next statement. The same logic could help cloud providers reduce overhead and user frustration.It’s not just DigitalOcean: micro‑balances happen everywhereDigitalOcean isn’t alone in sending tiny bills. Back in 2013, an Optus customer in Australia posted on Whirlpool forums that a billing error left them with a one‑cent overdue notice after receiving a reimbursement. One commenter wrote that it would cost the company “more in personnel overheads to deal with this stupid billing error, than what it’s worth”, while another explained, “It’s an automated system, mate. Just relax.”The moral of the story is that most companies rely on automated billing scripts, and without sensible thresholds, they’ll dutifully produce statements for even the most trivial amounts.In practice, if you owe 99 cents or less, many companies apply a credit adjustment and report a zero balance. The banking industry has recognized that goodwill and efficiency outweigh the pennies left on the ledger. If major financial institutions can swallow a dollar, cloud platforms with higher margins can too.What this taught me and how I’ve been guilty tooAs someone who deploys systems and manages mail servers, I can’t throw stones without acknowledging my own missteps. Earlier this year, I built dewedda.com, a storm‑watch website for the Eastern Caribbean. Part of which was to send automatic email alerts to subscribers when storms approached islands within specific distances and directions.In testing, everything looked great: the algorithm computed wind fields, adjusted for intensity, and tracked dozens of scenarios. But the first time a real storm approached, my code started hammering subscribers with unnecessary alerts. It didn’t account for storms that curved away or systems that were still unnamed, resulting in duplicates, so people kept receiving warnings even when there was no threat or duplicate emails. I had to scramble to adjust the logic.The experience taught me humility and the importance of edge cases, and it makes me more sympathetic to DigitalOcean’s engineers. Building resilient billing and notification systems is complex. Edge cases arise when accounts straddle billing cycles, use promotional credits, or move between team and personal billing. Legacy code and third‑party integrations can behave unpredictably. What matters is how we learn from these events.Conclusion (yes, I paid that one cent)I paid the 1 cent balance owed to DigitalOcean. But who covers that transaction cost?In the end, I did what any responsible business owner would do: I logged into my account and paid the one‑cent balance. Because, it would sit there for months, I only used a droplet for ~1 hour to test something. My personal DigitalOcean account goes mostly unused. So paying this invoice also means no recurring emails to pay my bill. Maybe that’s their plan? Ha!I hope this article highlights the hidden inefficiencies that creep into automation, whether it’s cloud invoices, marketing emails, or storm alerts.I still recommend DigitalOcean to friends and clients. They offer a great product at a fair price, with transparent billing. Being able to spin up a droplet in a few seconds makes life easy for Linux nerds like me. This one‑cent episode doesn’t change that; it simply underscores the value of thoughtful notification design. There are no affiliate links in this post either.In summary, as repeatedly proven, sending fewer, more relevant notifications improves user satisfaction and retention. The environmental data also shows that unnecessary emails carry hidden costs, and financial industry practices demonstrate that forgiving tiny balances can be cheaper than collecting them.A bit of humor on a Saturday morning turned into a lesson for all of us on building better systems. And yes, just in case the automated script is listening, I can confirm that as of writing this, my DigitalOcean account balance is zero.]]></content:encoded></item><item><title>GHC now runs in the browser</title><link>https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169</link><author>kaycebasques</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:29:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Convex Optimization (or Mathematical Programming) in Go</title><link>https://www.reddit.com/r/golang/comments/1ols1gn/convex_optimization_or_mathematical_programming/</link><author>/u/RobotCyclist23</author><category>dev</category><pubDate>Sat, 1 Nov 2025 16:23:44 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Do you write a lot of Convex (or similar) Optimization problems and have been yearning for a way to model them in Go? MatProInterface.go can help you (and needs your input to gain more maturity)! Feel free to try it and let me know what you think!]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/chibiace</author><category>dev</category><pubDate>Sat, 1 Nov 2025 15:02:11 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Updated practice for review articles and position papers in ArXiv CS category</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>dw64</author><category>dev</category><pubDate>Sat, 1 Nov 2025 14:58:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cycle-accurate 6502 emulator as coroutine in Rust</title><link>https://github.com/bagnalla/6502</link><author>/u/bagnalla</author><category>dev</category><pubDate>Sat, 1 Nov 2025 14:44:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>i&apos;m a zoomer on cachyOS but it seems to run in the family; my father has a jacket with a sun microsystems embroider on the front</title><link>https://www.reddit.com/r/linux/comments/1olp7wt/im_a_zoomer_on_cachyos_but_it_seems_to_run_in_the/</link><author>/u/bonzibuddy_official</author><category>dev</category><pubDate>Sat, 1 Nov 2025 14:28:52 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[he's mentioned having a good amount of experience in red hat mostly for his career, we ended up finding this in storage recently. it also has another larger embroidery of the java logo on the back. it's comfortable and fits me still which also rocks. i started using linux (mint) around 2021/2022 for hobbyist and general purposes, had to mostly run windows for college using adobe, no longer doing all of that so i'm back on cachy since it seems promising enough for an arch derivative. thought this would be neat to share on here. thank you unix for being the foundation for the funny little penguin kernal that's sure to sweep any year now :P]]></content:encoded></item><item><title>Go&apos;s Context Logger</title><link>https://github.com/pablovarg/contextlogger?tab=readme-ov-file#examples</link><author>/u/PurityHeadHunter</author><category>dev</category><pubDate>Sat, 1 Nov 2025 14:24:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello Gophers! A while ago, I started using contextual logging in my projects and found it made debugging significantly easier. Being able to trace request context through your entire call stack is a game-changer for understanding what's happening in your system.This project started as a collection of utility functions I copy-pasted between projects. Eventually, it grew too large to maintain that way, so I decided to turn it into a proper library and share it with the community. https://github.com/PabloVarg/contextloggerContext Logger is a library that makes it easy to propagate your logging context through Go's  and integrates seamlessly with Go's standard library, mainly  and . If this is something that you usually use or you're interested on using it for your projects, take a look at some Usage Examples.For a very simple example, here you can see how to:Embed a logger into your contextUpdate the context (this can be done many times before logging)Log everything that you have included in your context so farctx = contextlogger.EmbedLogger(ctx) contextlogger.UpdateContext(ctx, "userID", user.ID) contextlogger.LogWithContext(ctx, slog.LevelInfo, "done") ]]></content:encoded></item><item><title>Async Rust explained without Tokio or Smol</title><link>https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD</link><author>/u/Gisleburt</author><category>dev</category><pubDate>Sat, 1 Nov 2025 14:00:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>unsupportedConfigOverrides USAGE</title><link>https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/</link><author>/u/BigBprofessional</author><category>dev</category><pubDate>Sat, 1 Nov 2025 13:52:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)</title><link>https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/</link><author>/u/Dependent_Concert446</author><category>dev</category><pubDate>Sat, 1 Nov 2025 13:22:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’m looking for some advice on how to organize our Helm charts and Bitbucket repos for a growing  setup.We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).For our , we created  that’s used for microservices. We don’t have separate repos for each microservice — all are managed under the same project.Here’s a simplified view of the repo structure:app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments The following  are also in the same repo right now:Project Contour (Ingress)(and other cluster-level tools like k3s, Longhorn, etc.)These are not tied to the application project — they’re might shared and deployed across multiple clusters and environments.Should I move these shared infra components into a separate “infra” Bitbucket repo (including their Helm charts, Terraform, and Ansible configs)?For GitOps with , would it make more sense to split things like this:  → all microservices + base Helm chart → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps? Used AI to help write and format this post for grammar and readability.]]></content:encoded></item><item><title>CharlotteOS – An Experimental Modern Operating System</title><link>https://github.com/charlotte-os/Catten</link><author>ementally</author><category>dev</category><pubDate>Sat, 1 Nov 2025 13:12:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SQLite concurrency and why you should care about it</title><link>https://jellyfin.org/posts/SQLite-locking/</link><author>HunOL</author><category>dev</category><pubDate>Sat, 1 Nov 2025 12:59:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.SQLite is a file-based database engine running within your application and allows you to store data in a relational structure.
Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so.
Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.So an application that wants to use SQLite as its database needs to be the only one accessing it.
Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL).
The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file.
This allows multiple parallel writes to take place and get enqueued into the WAL.
When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly.
This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.A transaction is supposed to ensure two things.
Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction.
This is where it gets spicy and we come to the real reason why I am writing this blog post.
For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes.
This seems to be a not uncommon issue and there are many reports to be found on the issue.The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug.
From the reports this issue happens across all operating systems, drive speeds and with or without virtualization.
So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite.
During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal.
In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle.
While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error.
That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level.
EF Core supports a way of hooking into every command execution or transaction by creating Interceptors.
With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller.
The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary.
So, I decided on three locking strategies:As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override  in .Optimistic locking behavior​Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely.
This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.Jellyfin uses the  library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.Pessimistic locking behavior​Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed.
This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode.
While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.The future Smart locking behavior​In the future we might also consider combining both modes, to get the best of both worlds.Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here.
There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query.
Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.]]></content:encoded></item><item><title>Do you know that there is an HTML tables API?</title><link>https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/</link><author>begoon</author><category>dev</category><pubDate>Sat, 1 Nov 2025 12:58:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an old, forgotten  ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:let table 
let b  document.
let t  document.
b.t
table.rowri
  let r  t.ri
  row.li
    let c  r.i
    c. llet table = [
  ['one','two','three'],
  ['four','five','six']
];
let b = document.body;
let t = document.createElement('table');
b.appendChild(t);
table.forEach((row,ri) => {
  let r = t.insertRow(ri);
  row.forEach((l,i) => {
    let c = r.insertCell(i);
    c.innerText = l;  
  })
});You can then access each table cell with an index (with t being a reference to the table):console.t..console.log(t.rows[1].cells[1]);
// => <td>five</td>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:t.
t..
t...t.insertRow(-1);
t.rows[2].insertCell(0);
t.rows[2].cells[0].innerText = 'foo';There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.]]></content:encoded></item><item><title>Hard Rust requirements from May onward for all Debian ports</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/pyeri</author><category>dev</category><pubDate>Sat, 1 Nov 2025 12:32:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Hard Rust requirements from May onward (for Debian&apos;s package manager, APT)</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/DeleeciousCheeps</author><category>dev</category><pubDate>Sat, 1 Nov 2025 12:24:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Python refuses $1.5M grant, Unity&apos;s in trouble, AUR attacked again - Linux Weekly News</title><link>https://tilvids.com/videos/watch/02a038db-fdd0-46d4-8cb2-1f0b1b0bd04d</link><author>/u/Pure_Toe6636</author><category>dev</category><pubDate>Sat, 1 Nov 2025 12:08:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I created Open Source Kubernetes tool called Forkspacer to fork entire environments + dataplane, it is like git but for kubernetes.</title><link>https://www.reddit.com/r/kubernetes/comments/1olk9we/i_created_open_source_kubernetes_tool_called/</link><author>/u/Laughing-Dawg</author><category>dev</category><pubDate>Sat, 1 Nov 2025 10:15:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I created an open-source tool that lets you create, fork, and hibernate entire Kubernetes environments.With , you can fork your deployments while also migrating your data.. not just the manifests, but the entire data plane as well. We support different modes of forking: by default, every fork spins up a managed, dedicated virtual cluster, but you can also point the destination of your fork to a self-managed cluster. You can even set up multi-cloud environments and fork an environment from one provider (e.g., AWS) to another (e.g., GKE, AKE, or on-prem).You can clone full setups, test changes in isolation, and automatically hibernate idle workspaces to save resources all declaratively, with GitOps-style reproducibility.It’s especially useful for spinning up dev, test, pre-prod, and prod environments, and for teams where each developer needs a personal, forked environment from a shared baseline.License is Apace 2.0 and it is written in Go using Kubebuilder SDKPlease give it a try let me know, thank you]]></content:encoded></item><item><title>Monthly: Certification help requests, vents, and brags</title><link>https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/</link><author>/u/thockin</author><category>dev</category><pubDate>Sat, 1 Nov 2025 10:01:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did you pass a cert? Congratulations, tell us about it!Did you bomb a cert exam and want help? This is the thread for you.Do you just hate the process? Complain here.(Note: other certification related posts will be removed)]]></content:encoded></item><item><title>Monthly: Who is hiring?</title><link>https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/</link><author>/u/gctaylor</author><category>dev</category><pubDate>Sat, 1 Nov 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[This monthly post can be used to share Kubernetes-related job openings within  company. Please include:Location requirements (or lack thereof)At least one of: a link to a job posting/application page or contact detailsIf you are interested in a job, please contact the poster directly. Common reasons for comment removal:Not meeting the above requirementsRecruiter post / recruiter listingsNegative, inflammatory, or abrasive tone   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)</title><link>https://ar5iv.labs.arxiv.org/html/1901.09056</link><author>/u/Zomgnerfenigma</author><category>dev</category><pubDate>Sat, 1 Nov 2025 09:27:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Challenge of Benchmarking WebAssemblyThe aforementioned suite of 24 benchmarks is the PolybenchC benchmark
suite , which is designed to measure the effect of
polyhedral loop optimizations in compilers. All the benchmarks in the
suite are small scientific computing kernels rather than full
applications (e.g., matrix multiplication and LU Decomposition); each is
roughly 100 LOC. While WebAssembly is designed to accelerate scientific
kernels on the Web, it is also explicitly designed for a much richer set
of full applications.The WebAssembly documentation highlights several intended use
cases , including scientific kernels, image editing,
video editing, image recognition, scientific visualization, simulations,
programming language interpreters, virtual machines, and POSIX applications.
Therefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC
do not imply that it will perform well given a different kind of application.We argue that a more comprehensive evaluation of WebAssembly should rely on an
established benchmark suite of large programs, such as the SPEC CPU benchmark
suites. In fact, the SPEC CPU 2006 and 2017 suite of
benchmarks include several applications that fall under the intended use cases of
WebAssembly: eight benchmarks are scientific applications (e.g., ,
, , , and
), two benchmarks involve image and video processing
( and ), and all of the benchmarks are POSIX
applications.Unfortunately, it is not possible to simply compile a sophisticated
native program to WebAssembly. Native programs, including the programs in
the SPEC CPU suites, require operating system services, such as a
filesystem, synchronous I/O, and processes, which WebAssembly and the
browser do not provide. The SPEC benchmarking harness itself requires
a file system, a shell, the ability to spawn processes, and other Unix
facilities. To overcome these limitations when porting native
applications to the web, many programmers painstakingly modify their
programs to avoid or mimic missing operating system
services. Modifying well-known benchmarks, such as SPEC CPU, would not
only be time consuming but would also pose a serious threat to
validity.The standard approach to running these applications today is to use
Emscripten, a toolchain for compiling C and C++ to
WebAssembly . Unfortunately, Emscripten only supports
the most trivial system calls and does not scale up to large-scale
applications. For example, to enable applications to use synchronous
I/O, the default Emscripten  filesystem loads the entire
filesystem image into memory before the program begins executing. For
SPEC, these files are too large to fit into memory.A promising alternative is to use , a framework that enables
running unmodified, full-featured Unix applications in the
browser .  implements
a Unix-compatible kernel in JavaScript, with full support for
processes, files, pipes, blocking I/O, and other Unix features.
Moreover, it includes a C/C++ compiler (based on Emscripten)
that allows programs to run in the browser
unmodified. The  case studies include complex applications,
such as , which runs entirely in the browser without any
source code modifications.Unfortunately,  is a JavaScript-only solution, since it was
built before the release of
WebAssembly. Moreover,  suffers from high performance overhead,
which would be a significant confounder while benchmarking. Using ,
it would be difficult to tease apart the poorly performing benchmarks
from performance degradation introduced by .]]></content:encoded></item><item><title>You can&apos;t refuse to be scanned by ICE&apos;s facial recognition app, DHS document say</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>nh43215rgb</author><category>dev</category><pubDate>Sat, 1 Nov 2025 08:58:54 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>rkta</author><category>dev</category><pubDate>Sat, 1 Nov 2025 07:31:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Programming Language Agnostic Naming Conventions</title><link>https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide</link><author>/u/Distinct-Panic-246</author><category>dev</category><pubDate>Sat, 1 Nov 2025 07:30:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton"There are only two hard things in Computer Science: cache invalidation and naming things"(Or the slight variation of "There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors")But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.Why we spend time naming things correctlyIf you see a variable called , you can probably assume it is a boolean.  or  are not clear.Avoid Negative variable names:Negative names can lead to double negatives, which are confusing.Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.(Although  is probably an exception where it should always be used over ).Pick a language and always use thatIf you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)Make booleans obvious by using is/has prefixIf you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a booleanWords like , ,  are too generic. Try to avoid these termsPick a convention for naming things, and use those everywhere.calculateAmountBad 👎:  and Good 👍:  and Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice) for class names for most other variables for static constantsCommon names for specific thingsIf you're taking some data and  it to a different shape or different values then  is a common and accurate name.If you need to check if data is valid/correct, then its almost always called a validator.Used when describing the shape of some data structure. Often used with database designs.When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very relatedFor code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that dataWhen you have some functionality with a specific interface, and you need to convert it to another interface/shape.It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)When you need to make data uniform in scale, format, or structure]]></content:encoded></item><item><title>Prevent laptop&apos;s temp raises significantly during compiling</title><link>https://www.reddit.com/r/rust/comments/1olfwxy/prevent_laptops_temp_raises_significantly_during/</link><author>/u/blade_012</author><category>dev</category><pubDate>Sat, 1 Nov 2025 05:22:55 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[When compiling Fyrox for the first time, my laptop temperature raised significantly from 40°C to 90°C and stays in 90°C for long time until the compilation done. Is there any way to cap the compilation activity so that it won't use up all my CPU during the process? I don't mind having the process take a bit longer as long it's safe for my poor small Dell Latitude 7290.]]></content:encoded></item><item><title>Well a old school flex i guess</title><link>https://www.reddit.com/r/linux/comments/1olftbt/well_a_old_school_flex_i_guess/</link><author>/u/Puzzleheaded-Car4883</author><category>dev</category><pubDate>Sat, 1 Nov 2025 05:16:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This old Red Hat Linux 8.0 manual’s been gathering dust on my shelf. I used to read it as a kid — didn’t understand a single word back then. Fast forward to age 19, 3 years into using Linux daily... and everything suddenly makes sense.Btw this is one of those first thing that introduced me to linux ]]></content:encoded></item><item><title>Happy Halloween, nerds</title><link>https://www.reddit.com/r/linux/comments/1olf21x/happy_halloween_nerds/</link><author>/u/feelingsupersonic</author><category>dev</category><pubDate>Sat, 1 Nov 2025 04:29:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Java Virtual Threads VS GO routines</title><link>https://www.reddit.com/r/golang/comments/1oldyoo/java_virtual_threads_vs_go_routines/</link><author>/u/gamecrow77</author><category>dev</category><pubDate>Sat, 1 Nov 2025 03:25:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I recently had a argument with my tech lead about this , my push was for Go since its a new stack , new learning for the team and Go is evolving , my assumption is that we will find newer gen of devs who specialise in Go. Was i wrong here ? the argument was java with virtual threads is as efficient as go ]]></content:encoded></item><item><title>The profitable startup</title><link>https://linear.app/now/the-profitable-startup</link><author>doppp</author><category>dev</category><pubDate>Sat, 1 Nov 2025 03:18:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>IRS open-sourced the fact graph it uses for tax law</title><link>https://github.com/IRS-Public/fact-graph</link><author>/u/R2_SWE2</author><category>dev</category><pubDate>Sat, 1 Nov 2025 00:48:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Steinberg, creators of VST technology and the ASIO protocol, have released the SDKs for VST 3 and ASIO as Open Source.</title><link>https://www.reddit.com/r/linux/comments/1ola786/steinberg_creators_of_vst_technology_and_the_asio/</link><author>/u/fenix0000000</author><category>dev</category><pubDate>Sat, 1 Nov 2025 00:05:42 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/fenix0000000 ]]></content:encoded></item><item><title>Show HN: Strange Attractors</title><link>https://blog.shashanktomar.com/posts/strange-attractors</link><author>shashanktomar</author><category>dev</category><pubDate>Fri, 31 Oct 2025 23:23:59 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.The Basics: Dynamical Systems and Chaos TheoryDynamical Systems are a mathematical way to understand how things . Imagine you have a system, which
could be anything from the movement of planets to the growth of a population. In this system, there are rules that
determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is
happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a
population (we are all too familiar with this one), and stock market.There are two primary things to understand about this system:: This is like a big collection of all the possible states the system can be in. Each state is like a
snapshot of the system at a specific time. This is also called the  or the .: These are the rules that takes one state of the system and moves it to the next state. It can be
represented as a function that transforms the system from now to later.For instance, when studying population growth, a phase-space (world-state) might consist of the current population size
and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population
dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate
the changes in population size over time.Another way of saying this is that the dynamical systems describe how things change over time, in a space of
possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,
study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in
relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.There are different ways of classifying dynamical systems, and one of the most interesting is the classification into
chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to
chaotic systems which exhibit randomness and unpredictability. is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional
deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop
of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,
pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow
deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,
might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of
the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex
systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information
to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially
observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a
completely different outcome, a phenomenon known as the . In this post, we will try to see the
butterfly effect in action but before that, let's talk about .To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems
are all about . During this change, the system moves through different possible states (remember the
phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,
towards which it is . It's like a magnet that pulls the system towards it.For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the
bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of
the reasons why different states get attracted towards attractors:: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay
there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This
loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is
what happens in the case of the pendulum.: In some regions of the phase space, the system's dynamics cause trajectories to converge. This
contraction effect means that nearby states will tend to come closer together over time, eventually being drawn
towards the attractor.Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These
nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These
complex attractors are categorised as . They are called "strange" due to their unique
characteristics.: Strange attractors often have a fractal-like structure, meaning they display intricate
patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial
conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known
as the "butterfly effect".Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting
non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by
deterministic rules.Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order.
Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the
butterfly effect.A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small
changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the
. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to
drastically different outcomes.In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.There is another way of observing the butterfly effect in this visualisation. Change the  from  to
 in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a
technique called . This method handles iterative updates of particle systems directly on the GPU,
minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One
stores the current state of particles and render them on the screen, while the other calculates the next state.Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,  and , to hold the current and
next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use
of GPU resources.Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to
each particle. Following is the attractor function which update the particle positions based on the attractor equation.Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's
equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously
inactive buffer becomes active, and vice versa.This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The  discussion can be found here.]]></content:encoded></item><item><title>S.A.R.C.A.S.M: Slightly Annoying Rubik&apos;s Cube Automatic Solving Machine</title><link>https://github.com/vindar/SARCASM</link><author>chris_overseas</author><category>dev</category><pubDate>Fri, 31 Oct 2025 23:03:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Addiction Markets</title><link>https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate</link><author>toomuchtodo</author><category>dev</category><pubDate>Fri, 31 Oct 2025 17:42:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Use DuckDB-WASM to query TB of data in browser</title><link>https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/</link><author>mlissner</author><category>dev</category><pubDate>Fri, 31 Oct 2025 17:37:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the user’s web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collection’s discoverability suffers.For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.Why We Explored a New ApproachWhen LIL began thinking about how to provide discovery for the Data.gov Archive, we decided that building a lightweight and easily maintained access point from the beginning would be worth our team’s effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.This approach builds on our recent experience when the Caselaw Access Project (CAP) hit a transition moment. At that time, we elected to switch case.law to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what she’s looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?Our Experiment: Rich Discovery, No Server RequiredRecent advancements in client-side data analysis led us to try something new. Tools like DuckDB-Wasm, sql.js-httpvfs, and Protomaps, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.We developed Data.gov Archive Search on the same model. Here’s how it works: We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting. Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the user’s browser. When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, we’re continuing to explore speedy alternatives like hyparquet and Arquero to further improve performance.Still, we’re pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.Why This Matters for Libraries, Digital Humanities Projects, and BeyondThis new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes: By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.Reduced technical overhead: With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern. Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.Knowing that we are not the only group interested in approaching access in this way, we’re sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world: If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting. Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.This project is still evolving, and we invite others—particularly those in libraries and digital cultural heritage—to explore these possibilities with us. We’re committed to open sharing as we refine our tools, and we welcome collaboration or feedback at lil@law.harvard.edu.]]></content:encoded></item><item><title>Just use a button</title><link>https://gomakethings.com/just-use-a-button/</link><author>moebrowne</author><category>dev</category><pubDate>Fri, 31 Oct 2025 16:59:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[One of the weirdest “debates” I seem to perpetually have with framework-enthusiastic developers is whether or not a  is “just as good” as a . it’s not. Let’s dig in.Among the React crowd, and also among people who seem to enjoy HTMX, I see a lot this…
	Open Modal
This element does not announce itself as an interactive element to screen reader users.You can’t focus on a  with a keyboard.The event only fires on , not when the  or  keys are pressed (again, keyboard users).I’ve had arguments with a very prominent React thought leader whose name starts with R who insisted that using a  was “more accessible” than using a , and that Twitter made the right decision in using this pattern in their app.It’s wrong. It’s all wrong.Many HTML elements have  that tell assistive tech like screen readers what they do.The  element is one of them. It has an implicit  of , which tells screen reader users it can be interacted with and will trigger some type of behavior in the app.The HTML  attribute can be used to add or modify the role of an element. And so, folks like React Ry–thought-leader-guy will say stuff like (I’m paraphrasing)…That attribute exists for a reason. You can add  to a  to give it the correct semantics.OK, that addresses one issue.That role doesn’t affect focusability (or lack thereof) or keyboard behavior. Visually impaired users and people who navigate with a keyboard still can’t use it.“No worries!” they say. “We can fix that, too!”You can make the element focusable with the  attribute.
	Open Modal
You , though! Seriously, just don’t fuck with focus order.It’s way too easy to go down this path and then fuck it up and have folks jumping all over the page instead of navigating through in the normal and expected order.And again, still no keyboard interactivity.But don’t fear! You can add that, too. You just need to listen for all  events, and then filter them out by  so that you only run your code if the  or  keys were pressed (the latter means checking for a literal space: ).That can’t run on the element, either. You’ve got to attach that even to the  and figure out which element has focus.So um… ok, I guess it is technically a fix, but…You’ve just recreated all of the functionality a  gives you for freeSeriously, WTF would you do that?!?All of these hoops to write this HTML…
	Open Modal
When you could write this HTML instead…
	Open Modal
Has the correct  implicitly.Is automatically focusable.Fires a  event in response to  and  presses when it has focus.Look, I’m a lazy developer.And I suspect, if you’re someone who loves tools like React, you probably are, too. It’s cool, I get it! The best code is the code you didn’t write and all that.Use the correct element for the job, and avoid writing a bunch of extra code!]]></content:encoded></item><item><title>Futurelock: A subtle risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>bcantrill</author><category>dev</category><pubDate>Fri, 31 Oct 2025 16:49:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Another European agency shifts off US Tech as digital sovereignty gains steam</title><link>https://www.zdnet.com/article/another-european-agency-ditches-big-tech-as-digital-sovereignty-movement-gains-steam/</link><author>CrankyBear</author><category>dev</category><pubDate>Fri, 31 Oct 2025 16:39:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Austria's Ministry of Economy has migrated to a Nextcloud platform.It's the latest move in a European trend to shift away from Big Tech.European governments and agencies want to control sensitive data.This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers. European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance. Open-source software is seen as combining the virtues of faster development and better security, while providing companies and governments with more control, as general manager Thierry Carrez of the OpenInfra Foundation recently suggested: "Open infrastructure allows nations and organizations to maintain control over their applications, their data, and their destiny while benefiting from global collaboration."  While the US may not like it, with NextCloud's help, BMWET completed its migration in just four months. Although BMWET had already begun adopting Microsoft 365 and Teams before the project's start, the shift was still considered a success. That's because instead of reversing its path, the ministry implemented a hybrid architecture: Nextcloud handles internal collaboration and secure data management, while Teams remains available for external meetings.The project emphasized integration with existing workflows, including seamless integration with Outlook email and calendar via Sendent's Outlook app. This approach minimized disruption and ensured user acceptance. However, not all migrations progress so well. For example, in Austria, the Ministry of Justice decided to replace Office with LibreOffice. Yet the transition has run into trouble. It appears that the move of 20,000 desktops, which was prompted by a desire to reduce spending on Microsoft licenses, has been, as one person reported, an "unprofessional, rushed operation." Some offices are still on Office, others on LibreOffice, and they're running into incompatible document format problems and misfires in e-mail systems. The moral of the story is that any switch from one software suite to another requires careful handling by the IT department and helpdesk staff. Otherwise, you end up with unhappy users.That said, BMWET's bold shift to Nextcloud appears to have gone well. This initiative demonstrates that adopting sovereign cloud solutions can be practical, user-friendly, and rapid in the public sector. However, as Austria's Justice Ministry experience has shown, simply shifting to an open-source approach without careful planning can get in the way of getting work done. ]]></content:encoded></item><item><title>AI scrapers request commented scripts</title><link>https://cryptography.dog/blog/AI-scrapers-request-commented-scripts/</link><author>ColinWright</author><category>dev</category><pubDate>Fri, 31 Oct 2025 15:44:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop</title><link>https://news.ycombinator.com/item?id=45771870</link><author>threeturn</author><category>dev</category><pubDate>Fri, 31 Oct 2025 13:39:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Dear Hackers,
I’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud/enterprise SaaS). Specifically:Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant/integration (for example, a VS Code plugin) you’re using?What laptop hardware do you have (CPU, GPU/NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well / where it falls short).I'm conducting my own investigation, which I will be happy to share as well when over.]]></content:encoded></item><item><title>Attention lapses due to sleep deprivation due to flushing fluid from brain</title><link>https://news.mit.edu/2025/your-brain-without-sleep-1029</link><author>gmays</author><category>dev</category><pubDate>Fri, 31 Oct 2025 13:14:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Nearly everyone has experienced it: After a night of poor sleep, you don’t feel as alert as you should. Your brain might seem foggy, and your mind drifts off when you should be paying attention.A new study from MIT reveals what happens inside the brain as these momentary failures of attention occur. The scientists found that during these lapses, a wave of cerebrospinal fluid (CSF) flows out of the brain — a process that typically occurs during sleep and helps to wash away waste products that have built up during the day. This flushing is believed to be necessary for maintaining a healthy, normally functioning brain.When a person is sleep-deprived, it appears that their body attempts to catch up on this cleansing process by initiating pulses of CSF flow. However, this comes at a cost of dramatically impaired attention.“If you don’t sleep, the CSF waves start to intrude into wakefulness where normally you wouldn’t see them. However, they come with an attentional tradeoff, where attention fails during the moments that you have this wave of fluid flow,” says Laura Lewis, the Athinoula A. Martinos Associate Professor of Electrical Engineering and Computer Science, a member of MIT’s Institute for Medical Engineering and Science and the Research Laboratory of Electronics, and an associate member of the Picower Institute for Learning and Memory.Lewis is the senior author of the study, which appears today in . MIT visiting graduate student Zinong Yang is the lead author of the paper.Although sleep is a critical biological process, it’s not known exactly why it is so important. It appears to be essential for maintaining alertness, and it has been well-documented that sleep deprivation leads to impairments of attention and other cognitive functions.During sleep, the cerebrospinal fluid that cushions the brain helps to remove waste that has built up during the day. In a 2019 study, Lewis and colleagues showed that CSF flow during sleep follows a rhythmic pattern in and out of the brain, and that these flows are linked to changes in brain waves during sleep.That finding led Lewis to wonder what might happen to CSF flow after sleep deprivation. To explore that question, she and her colleagues recruited 26 volunteers who were tested twice — once following a night of sleep deprivation in the lab, and once when they were well-rested.In the morning, the researchers monitored several different measures of brain and body function as the participants performed a task that is commonly used to evaluate the effects of sleep deprivation.During the task, each participant wore an electroencephalogram (EEG) cap that could record brain waves while they were also in a functional magnetic resonance imaging (fMRI) scanner. The researchers used a modified version of fMRI that allowed them to measure not only blood oxygenation in the brain, but also the flow of CSF in and out of the brain. They also measured each subject’s heart rate, breathing rate, and pupil diameter.The participants performed two attentional tasks while in the fMRI scanner, one visual and one auditory. For the visual task, they had to look at a screen that had a fixed cross. At random intervals, the cross would turn into a square, and the participants were told to press a button whenever they saw this happen. For the auditory task, they would hear a beep instead of seeing a visual transformation.Sleep-deprived participants performed much worse than well-rested participants on these tasks, as expected. Their response times were slower, and for some of the stimuli, the participants never registered the change at all.During these momentary lapses of attention, the researchers identified several physiological changes that occurred at the same time. Most significantly, they found a flux of CSF out of the brain just as those lapses occurred. After each lapse, CSF flowed back into the brain.“The results are suggesting that at the moment that attention fails, this fluid is actually being expelled outward away from the brain. And when attention recovers, it’s drawn back in,” Lewis says.The researchers hypothesize that when the brain is sleep-deprived, it begins to compensate for the loss of the cleansing that normally occurs during sleep, even though these pulses of CSF flow come with the cost of attention loss.“One way to think about those events is because your brain is so in need of sleep, it tries its best to enter into a sleep-like state to restore some cognitive functions,” Yang says. “Your brain’s fluid system is trying to restore function by pushing the brain to iterate between high-attention and high-flow states.”The researchers also found several other physiological events linked to attentional lapses, including decreases in breathing and heart rate, along with constriction of the pupils. They found that pupil constriction began about 12 seconds before CSF flowed out of the brain, and pupils dilated again after the attentional lapse.“What’s interesting is it seems like this isn’t just a phenomenon in the brain, it’s also a body-wide event. It suggests that there’s a tight coordination of these systems, where when your attention fails, you might feel it perceptually and psychologically, but it’s also reflecting an event that’s happening throughout the brain and body,” Lewis says.This close linkage between disparate events may indicate that there is a single circuit that controls both attention and bodily functions such as fluid flow, heart rate, and arousal, according to the researchers.“These results suggest to us that there’s a unified circuit that’s governing both what we think of as very high-level functions of the brain — our attention, our ability to perceive and respond to the world — and then also really basic fundamental physiological processes like fluid dynamics of the brain, brain-wide blood flow, and blood vessel constriction,” Lewis says.In this study, the researchers did not explore what circuit might be controlling this switching, but one good candidate, they say, is the noradrenergic system. Recent research has shown that this system, which regulates many cognitive and bodily functions through the neurotransmitter norepinephrine, oscillates during normal sleep.The research was funded by the National Institutes of Health, a National Defense Science and Engineering Graduate Research Fellowship, a NAWA Fellowship, a McKnight Scholar Award, a Sloan Fellowship, a Pew Biomedical Scholar Award, a One Mind Rising Star Award, and the Simons Collaboration on Plasticity in the Aging Brain.]]></content:encoded></item><item><title>How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise</title><link>https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html</link><author>reaperducer</author><category>dev</category><pubDate>Fri, 31 Oct 2025 13:03:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Sam Altman, the chief executive of OpenAI, says that technological revolutions are driven by more than just technology. They are also driven, he argues, by new ways of paying for them.“There is always a lot of focus on technological innovation. What really drives a lot of progress is when people also figure out how to innovate on the financial model,” he recently said at the site of a data center that OpenAI is building in Abilene, Texas.Over the last several years, Mr. Altman’s company has found unusual and creative ways of paying for the computing power needed to fuel its ambitions.Many of the deals OpenAI has struck — with chipmakers, cloud computing companies and others — are strangely circular. OpenAI receives billions from tech companies before sending those billions back to the same companies to pay for computing power and other services.Industry experts and financial analysts have welcomed the start-up’s creativity. But these unorthodox arrangements have also fueled concerns that OpenAI is helping to inflate a potential financial bubble as it builds what is still a highly speculative technology.Here are unusual financial agreements helping to drive the ambitions of OpenAI, the poster child of the artificial intelligence revolution.From 2019 through 2023, Microsoft was OpenAI’s primary investor. The tech giant pumped more than  into the start-up. Then OpenAI funneled most of those billions back into Microsoft, buying  needed to fuel the development of new A.I. technologies.(The New York Times has sued OpenAI and Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit’s claims.)By the summer of last year, OpenAI could not get all the computing power it wanted from Microsoft. So it started signing cloud computing contracts with other companies, including Oracle and little-known start-ups with names like CoreWeave.Across three different deals signed this year, OpenAI agreed to pay CoreWeave, a company that builds A.I. data centers, more than  for computing power. As part of these agreements, OpenAI received  in CoreWeave stock, which could ultimately help pay for this computing power.OpenAI also struggled to get the additional investment dollars it wanted from Microsoft. So, it turned to other investors. Earlier this year, the Japanese conglomerate SoftBank led a  investment in OpenAI.At the same time, OpenAI has been working with various companies to build its own computing data centers, rather than rely on cloud computing deals. This also includes SoftBank, which is known for highly speculative technological bets that don’t always pay off. The company is raising  to help OpenAI build data centers in Texas and Ohio.Similarly, Oracle, a software and cloud computing giant, has agreed to spend  building new data centers for OpenAI in Texas, New Mexico, Michigan and Wisconsin. OpenAI will then pay Oracle roughly the same amount to use these  over the next several years.The United Arab Emirates was part of an OpenAI’s fund-raising round in October 2024. Now, G42, a firm with close ties to the Emirati government, is building a roughly  for OpenAI in the Emirates.Last month, Nvidia announced that it intended to invest  in OpenAI over the next several years. This could help OpenAI pay for its new data centers. As OpenAI buys or leases specialized chips from Nvidia, Nvidia will pump billions back into OpenAI.Two weeks later, OpenAI signed an agreement with AMD that allows OpenAI to buy up to  in the chipmaker at a penny per share. That translates to roughly a 10 percent stake in the company. This stock could supply OpenAI with additional capital as it works to build new data centers.OpenAI pulls in billions of dollars in revenue each year from customers who pay for ChatGPT, computer programming tools and other technologies. But it still loses more money than it makes, according to a person familiar with the company’s finances.If the company can use its new data centers to significantly improve A.I. technologies and expand its revenue over the next several years, it can become a viable business, as Mr. Altman believes it will. If technology progress stalls, OpenAI – and its many partners – could lose enormous amounts of money. Smaller companies like CoreWeave, which are taking on enormous amounts of debt to build new data centers, could go bankrupt.In some cases, companies are hedging their bets. Nvidia and AMD, for instance, have the option of reducing the cash and stock they send to OpenAI if the A.I. market does not expand as quickly as expected. But others would be left with enormous debt, which could send ripples across the larger economy.]]></content:encoded></item><item><title>My Impressions of the MacBook Pro M4</title><link>https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/</link><author>secure</author><category>dev</category><pubDate>Fri, 31 Oct 2025 10:13:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I have been using a MacBook Pro M4 as my portable computer for the last half a
year and wanted to share a few short impressions. As always, I am not a
professional laptop reviewer, so in this article you won’t find benchmarks, just
subjective thoughts!Back in 2021, I wrote about the MacBook Air
M1, which was the first computer I used that
contained Apple’s own ARM-based CPU. Having a silent laptop with long battery
life was a game-changer, so I wanted to keep those properties.When the US government announced tariffs, I figured I would replace my 4-year
old MacBook Air M1 with a more recent model that should last a few more
years. Ultimately, Apple’s prices remained stable, so, in retrospect, I could
have stayed with the M1 for a few more years. Oh well.The nano-textured displayI went to the Apple Store to compare the different options in
person. Specifically, I was curious about the display and whether the increased
weight and form factor of the MacBook Pro (compared to a MacBook Air) would be
acceptable. Another downside of the Pro model is that it comes with a fan, and I
really like absolutely quiet computers. Online, I read from other MacBook Pro
owners that the fan mostly stays off.In general, I would have preferred to go with a MacBook Air because it has
enough compute power for my needs and I like the case better (no ventilation
slots), but unfortunately only the MacBook Pro line has the better displays.Why aren’t all displays nano-textured? The employee at the Apple Store presented
the trade-off as follows: The nano texture display is great at reducing
reflections, at the expense of also making the picture slightly less vibrant.I could immediately see the difference when placing two laptops side by side:
The bright Apple Store lights showed up very prominently on the normal display
(left), and were almost not visible at all on the nano texture display (right):Personally, I did not perceive a big difference in “vibrancy”, so my choice was
clear: I’ll pick the MacBook Pro over the MacBook Air (despite the weight) for
the nano texture display!After using the laptop in a number of situations, I am very happy with this
choice. In normal scenarios, I notice no reflections at all (where my previous
laptop did show reflections!). This includes using the laptop on a train (next
to the window), or using the laptop outside in daylight.(When I chose the new laptop, Apple’s M4 chips were current. By now, they have
released the first devices with M5 chips.)I decided to go with the MacBook Pro with M4 chip instead of the M4  chip
because I don’t need the extra compute, and the M4 needs less cooling — the M4
Pro apparently runs hotter. This increases the chance of the fan staying off.Here are the specs I ended up with:14" Liquid Retina XDR Display with nano textureApple M4 Chip (10 core CPU, 10 core GPU)32 GB RAM (this is the maximum!), 2 TB SSD (enough for this computer)One thing I noticed is that the MacBook Pro M4 sometimes gets warm, even when it
is connected to power, but is suspended to RAM (and has been fully charged for
hours). I’m not sure why.Luckily, the fan indeed stays silent. I think I might have heard it spin up once
in half a year or so?The battery life is amazing! The previous MacBook Air M1 had amazing all-day
battery life already, and this MacBook Pro M4 lasts even longer. For example,
watching videos on a train ride (with VLC) for 3 hours consumed only 10% of
battery life. I generally never even carry the charger.Because of that, Apple’s re-introduction of MagSafe, a magnetic power connector
(so you don’t damage the laptop when you trip over it), is nice-to-have but
doesn’t really make much of a difference anymore. In fact, it might be better to
pack a USB-C cable when traveling, as that makes you more flexible in how you
use the charger.I was curious whether the 120 Hz display would make a difference in practice. I
mostly notice the increased refresh rate when there are animations, but not,
for example, when scrolling.One surprising discovery (but obvious in retrospect) is that even non-animations
can become faster. For example, when running a Go web server on , I
noticed that navigating between pages by clicking links felt faster on the 120
Hz display!The following illustration shows why that is, using a page load that takes 6ms
of processing time. There are three cases (the illustration shows an average
case and the worst case):Best case: Page load finishes  the next frame is displayed: no delay.Worst case: Page load finishes  a frame is displayed: one frame of delay.Most page loads are somewhere in between. We’ll have 0.x to 1.0 frames of delayAs you can see, the waiting time becomes shorter when going from 60 Hz (one
frame every 16.6ms) to 120 Hz (one frame every 8.3ms). So if you’re working with
a system that has <8ms response times, you might observe actions completing (up
to) twice as fast!I don’t notice going back to 60 Hz displays on computers. However, on phones,
where a lot more animations are a key part of the user experience, I think 120
Hz displays are more interesting.My ideal MacBook would probably be a MacBook Air, but with the nano-texture display! :)I still don’t like macOS and would prefer to run Linux on this laptop. But
Asahi Linux still needs some work before it’s usable
for me (I need external display output, and M4 support). This doesn’t bother me
too much, though, as I don’t use this computer for serious work.]]></content:encoded></item><item><title>Reasoning models reason well, until they don&apos;t</title><link>https://arxiv.org/abs/2510.22371</link><author>optimalsolver</author><category>dev</category><pubDate>Fri, 31 Oct 2025 09:23:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD could enter ARM market with Sound Wave APU built on TSMC 3nm process</title><link>https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/</link><author>walterbell</author><category>dev</category><pubDate>Fri, 31 Oct 2025 03:07:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[AMD is expanding its processor portfolio beyond the x86 architecture with its first ARM-based APU, internally known as “Sound Wave.” The chip’s existence was uncovered through customs import records, confirming several details about its design and purpose. Built with a BGA-1074 package measuring 32 mm × 27 mm, the processor fits within standard mobile SoC dimensions, making it suitable for thin and light computing platforms. It employs a 0.8 mm pitch and FF5 interface, replacing the FF3 socket previously used in Valve’s Steam handheld devices, further hinting at a new generation of compact AMD-powered hardware.
                                    According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on  and aims for a  range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future  products scheduled for release in 2026. four RDNA 3.5 compute unitsmachine learning accelerationMemory support is another highlight: the chip integrates a 128-bit LPDDR5X-9600 controller and will reportedly include , aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s fourth-generation AI engine, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>azhenley</author><category>dev</category><pubDate>Fri, 31 Oct 2025 02:34:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ground stop at JFK due to staffing</title><link>https://www.fly.faa.gov/adv/adv_otherdis?advn=13&amp;adv_date=10312025&amp;facId=JFK&amp;title=ATCSCC%20ADVZY%20013%20JFK/ZNY%2010/31/2025%20CDM%20GROUND%20STOP&amp;titleDate=10/31/2025</link><author>akersten</author><category>dev</category><pubDate>Fri, 31 Oct 2025 01:48:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE and the Smartphone Panopticon</title><link>https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon</link><author>fortran77</author><category>dev</category><pubDate>Fri, 31 Oct 2025 01:13:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last week, as  raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless documentation has proved something of an effective tool against President Donald Trump’s empowerment of ; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping  agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard in Chicago earlier this month, the governor of Illinois, J. B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti- opposition is taking advantage of the internet,  and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent  sweeps of the area.  itself is also working to monitor social media. The investigative outlet  found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in ,  also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown  agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where  agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of  abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an  field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.]]></content:encoded></item><item><title>Show HN: Quibbler – A critic for your coding agent that learns what you want</title><link>https://github.com/fulcrumresearch/quibbler</link><author>etherio</author><category>dev</category><pubDate>Fri, 31 Oct 2025 00:43:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kimi Linear: An Expressive, Efficient Attention Architecture</title><link>https://github.com/MoonshotAI/Kimi-Linear</link><author>blackcat201</author><category>dev</category><pubDate>Fri, 31 Oct 2025 00:07:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>