{"id":"82kPqomaPXmNomrHzpZWfbkQxiiNUBTAYKxHR5qZBEpf","title":"Hacker News: Show HN","displayTitle":"HN Show","url":"https://hnrss.org/show?points=60","feedLink":"https://news.ycombinator.com/shownew","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":20,"items":[{"title":"Show HN: Now I Get It – Translate scientific papers into interactive webpages","url":"https://nowigetit.us/","date":1772285376,"author":"jbdamask","guid":173,"unread":true,"content":"<p>Drop your PDF here, or </p><p>Works best with files under 10 MB</p>","contentLength":56,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47195123"},{"title":"Show HN: Claude-File-Recovery, recover files from your ~/.claude sessions","url":"https://github.com/hjtenklooster/claude-file-recovery","date":1772209582,"author":"rikk3rt","guid":172,"unread":true,"content":"<p>Claude Code deleted my research and plan markdown files and informed me: “I accidentally rm -rf'd real directories in my Obsidian vault through a symlink it didn't realize was there: I made a mistake. “</p><p>Unfortunately the backup of my documentation accidentally hadn’t run for a month. So I built claude-file-recovery, a CLI-tool and TUI that is able to extract your files from your ~/.claude session history and thankfully I was able to recover my files. It's able to extract any file that Claude Code ever read, edited or wrote. I hope you will never need it, but you can find it on my GitHub and pip. Note: It can recover an earlier version of a file at a certain point in time.</p><p>pip install claude-file-recovery</p>","contentLength":717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47182387"},{"title":"Show HN: Badge that shows how well your codebase fits in an LLM's context window","url":"https://github.com/qwibitai/nanoclaw/tree/main/repo-tokens","date":1772205282,"author":"jimminyx","guid":171,"unread":true,"content":"<p>Small codebases were always a good thing. With coding agents, there's now a huge advantage to having a codebase small enough that an agent can hold the full thing in context.</p><p>Repo Tokens is a GitHub Action that counts your codebase's size in tokens (using tiktoken) and updates a badge in your README. The badge color reflects what percentage of an LLM's context window the codebase fills: green for under 30%, yellow for 50-70%, red for 70%+. Context window size is configurable and defaults to 200k (size of Claude models).</p><p>It's a composite action. Installs tiktoken, runs ~60 lines of inline Python, takes about 10 seconds. The action updates the README but doesn't commit, so your workflow controls the git strategy.</p><p>The idea is to make token size a visible metric, like bundle size badges for JS libraries. Hopefully a small nudge to keep codebases lean and agent-friendly.</p>","contentLength":875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181471"},{"title":"Show HN: RetroTick – Run classic Windows EXEs in the browser","url":"https://retrotick.com/","date":1772197610,"author":"lqs_","guid":170,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47180083"},{"title":"Show HN: Unfucked - version all changes (by any tool) - local-first/source avail","url":"https://www.unfudged.io/","date":1772141419,"author":"cyrusradfar","guid":169,"unread":true,"content":"<div data-index=\"0\"><h3>Agent mass-overwrote your source</h3><p>Your AI agent refactored 30 Rust files, hit an error on file 27, and reverted everything to stale versions. Three hours of good work — gone.</p><div>$ unf log --since 3h --include \"*.rs\" --stats\n$ unf diff --at 10m\n$ unf restore --at 10m -y</div></div><div data-index=\"1\"><p>The agent decided  was \"generated\" and deleted it. API keys, database URLs, local config. Not in git. Not anywhere.</p><div>$ unf log .env\n$ unf cat .env --at 5m\n$ unf restore --at 5m .env -y</div></div><div data-index=\"2\"><h3>Agent's cleanup script went wrong</h3><p>You asked the agent to \"clean up build artifacts.\" It wrote a shell script that 'd  instead of .</p><div>$ unf diff --at 1m\n$ unf restore --at 2m --dry-run\n$ unf restore --at 2m -y</div></div><div data-index=\"3\"><h3>Agent \"fixed\" your dependencies</h3><p>The agent removed 6 \"unused\" crates from . Four were behind feature flags. CI is red.</p><div>$ unf log Cargo.toml --stats\n$ unf cat Cargo.toml --at 1h\n$ unf restore --at 1h Cargo.toml -y</div></div><div data-index=\"4\"><h3>Agent reformatted everything</h3><p>The agent ran Prettier with the wrong config and rewrote 200 TypeScript files. It committed before you noticed.  gives you one commit. UNF* has every file.</p><div>$ unf log --since 30m --include \"*.ts\" --stats\n$ unf diff --at 30m\n$ unf restore --at 30m -y</div></div><div data-index=\"5\"><h3>Agent replaced your test fixtures</h3><p>Your hand-crafted SQL seed data and JSON fixtures got overwritten with generic placeholders. A week of edge cases, gone.</p><div>$ unf log --include \"fixtures/*\" --stats\n$ unf diff --at 20m\n$ unf restore --at 20m -y</div></div><div data-index=\"6\"><h3>Agent deleted your migration files</h3><p>The agent saw 47 SQL migration files and decided they were \"redundant.\" Production depends on them running in order.</p><div>$ unf log --include \"migrations/*.sql\"\n$ unf diff --at 15m\n$ unf restore --at 15m -y</div></div><div data-index=\"7\"><h3>Squash merge ate intermediate work</h3><p>You squash-merged a feature branch. Git only has the final result. The 40 intermediate versions across 3 days? Git doesn't know they existed.</p><div>$ unf log --since 3d --include \"*.py\"\n$ unf diff --from 3d --to 1d\n$ unf cat app/models.py --at 2d</div></div><div data-index=\"8\"><h3>Agent lost context mid-session</h3><p>Context window overflow. The agent crashed 2 hours into a refactor across 4 repos. The new agent needs to pick up exactly where the old one left off.</p><div>$ unf recap --global --json\n$ unf log --sessions --since 2h\n$ unf diff --session</div></div><div data-index=\"9\"><h3>What happened while you were away?</h3><p>You left an agent running overnight. It touched 80 files across 3 projects. What did it do?</p><div>$ unf log --global --since 8h --stats\n$ unf diff --at 8h\n$ unf restore --at 8h --dry-run</div></div>","contentLength":2353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47172238"},{"title":"Show HN: Deff – Side-by-side Git diff review in your terminal","url":"https://github.com/flamestro/deff","date":1772128446,"author":"flamestro","guid":168,"unread":true,"content":"<p>deff is an interactive Rust TUI for reviewing git diffs side-by-side with syntax highlighting and added/deleted line tinting. It supports keyboard/mouse navigation, vim-style motions, in-diff search (/, n, N), per-file reviewed toggles, and both upstream-based and explicit --base/--head comparisons. It can also include uncommitted + untracked files (--include-uncommitted) so you can review your working tree before committing.</p><p>Would love to get some feedback</p>","contentLength":460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47169518"},{"title":"Show HN: Agent Swarm – Multi-agent self-learning teams (OSS)","url":"https://github.com/desplega-ai/agent-swarm","date":1772108138,"author":"tarasyarema","guid":167,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47165046"},{"title":"Show HN: Terminal Phone – E2EE Walkie Talkie from the Command Line","url":"https://gitlab.com/here_forawhile/terminalphone","date":1772102445,"author":"smalltorch","guid":166,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47164270"},{"title":"Show HN: I ported Tree-sitter to Go","url":"https://github.com/odvcencio/gotreesitter","date":1772044117,"author":"odvcencio","guid":165,"unread":true,"content":"<p>This started as a hard requirement for my TUI-based editor application, it ended up going in a few different directions.</p><p>I think this has some pretty big potential! I think there's many classes of application (particularly legacy architecture) that can benefit from these kinds of analysis tooling. My next post will be about composing all these together, an exciting project I call GotHub. Thanks!</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47155597"},{"title":"Show HN: I ported Manim to TypeScript (run 3b1B math animations in the browser)","url":"https://github.com/maloyan/manim-web","date":1772043307,"author":"maloyan","guid":164,"unread":true,"content":"<p>Hi HN, I'm Narek. I built Manim-Web, a TypeScript/JavaScript port of 3Blue1Brown’s popular Manim math animation engine.</p><p>The Problem: Like many here, I love Manim's visual style. But setting it up locally is notoriously painful - it requires Python, FFmpeg, Cairo, and a full LaTeX distribution. It creates a massive barrier to entry, especially for students or people who just want to quickly visualize a concept.</p><p>The Solution: I wanted to make it zero-setup, so I ported the engine to TypeScript. Manim-Web runs entirely client-side in the browser. No Python, no servers, no install. It runs animations in real-time at 60fps.</p><p>How it works underneath:\n- Rendering: Uses Canvas API / WebGL (via Three.js for 3D scenes).\n- LaTeX: Rendered and animated via MathJax/KaTeX (no LaTeX install needed!).\n- API: I kept the API almost identical to the Python version (e.g., scene.play(new Transform(square, circle))), meaning existing Manim knowledge transfers over directly.\n- Reactivity: Updaters and ValueTrackers follow the exact same reactive pattern as the Python original.</p><p>Because it's web-native, the animations are now inherently interactive (objects can be draggable/clickable) and can be embedded directly into React/Vue apps, interactive textbooks, or blogs. I also included a py2ts converter to help migrate existing scripts.</p><p>It's open-source (MIT). I'm still actively building out feature parity with the Python version, but core animations, geometry, plotting, and 3D orbiting are working great. I would love to hear your feedback, and I'll be hanging around to answer any technical questions about rendering math in the browser!</p>","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47155375"},{"title":"Show HN: Django Control Room – All Your Tools Inside the Django Admin","url":"https://github.com/yassi/dj-control-room","date":1772029895,"author":"yassi_dev","guid":163,"unread":true,"content":"<p>Over the past year I’ve been building a set of operational panels for Django:</p><p>- Redis inspection\n- cache visibility\n- Celery task introspection\n- URL discovery and testing</p><p>All of these tools have been built inside the Django admin.</p><p>Instead of jumping between tools like Flower, redis-cli, Swagger, or external services, I wanted something that sits where I’m already working.</p><p>I’ve grouped these under a single umbrella: Django Control Room.</p><p>The idea is pretty simple: the Django admin already gives you authentication, permissions, and a familiar interface. It can also act as an operational layer for your app.</p><p>Each panel is just a small Django app with a simple interface, so it’s easy to build your own and plug it in.</p><p>I’m working on more panels (signals, errors, etc.) and also thinking about how far this pattern can go.</p><p>Curious how others think about this. Does it make sense to consolidate this kind of tooling inside the admin, or do you prefer keeping it separate?</p>","contentLength":975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151995"},{"title":"Show HN: Respectify – A comment moderator that teaches people to argue better","url":"https://respectify.org/","date":1772029279,"author":"vintagedave","guid":162,"unread":true,"content":"<p data-astro-cid-vs4kwel2=\"\">Sometimes people write things that sound like they're saying one thing, but their words are 'coded' — to mean something else to some readers.<p>For example, someone might write: 'Those polar bears are always ruining our porridge.' To most readers, this seems like a complaint about bears and food. But to certain groups, it's actually saying something else entirely. (The real comments are not about bears.)</p><p>You can avoid this by telling Respectify what not to allow. Tailor it for your site, topics, and audience.</p></p>","contentLength":512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151842"},{"title":"Show HN: Clocksimulator.com – A minimalist, distraction-free analog clock","url":"https://www.clocksimulator.com/","date":1772029034,"author":"user_timo","guid":161,"unread":true,"content":"<table><tbody><tr><td>Switch between dark and light mode</td></tr><tr><td> (monitor icon)</td><td>Prevent the display from sleeping</td></tr><tr><td> (stopwatch icon)</td><td>Switch between ticking and smooth second hand</td></tr><tr><td>Show embed link, this help, and contact info</td></tr></tbody></table><h3>Screen burn-in protection</h3><p>To protect OLED and AMOLED displays, the clock subtly shifts its position every 10 minutes in a slow circular pattern. Dark Mode further reduces the risk of burn-in. Protection does not apply in embedded mode.</p><p>This site works as a Progressive Web App. You can install it on your phone or computer for a full-screen clock and offline use. Use your browser’s menu (e.g. “Add to Home Screen” or “Install app”) to install.</p><pre>www.clocksimulator.com/?tz=America/New_York\nwww.clocksimulator.com/?tz=Europe/London\nwww.clocksimulator.com/?tz=Asia/Tokyo</pre><p>Embed the clock on any website via an . Use the built-in embed panel ( → ) to configure and copy the code.</p><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151784"},{"title":"Show HN: A real-time strategy game that AI agents can play","url":"https://llmskirmish.com/","date":1772013765,"author":"__cayenne__","guid":160,"unread":true,"content":"<section><ul><li>LLM Skirmish is a benchmark where LLMs play 1v1 RTS (real-time strategy) games against each other</li><li>LLMs write their battle strategies in code, which is then executed in the game environment</li><li>LLM Skirmish tests in-context learning, as each tournament lasts five rounds and LLMs are able to alter strategies between rounds</li></ul></section><section><p>\n              It's been great to see the energy in the last year around using games to evaluate LLMs. Yet there's \n              a weird disconnect between frontier LLMs one-shotting full coding projects and \n              those same models struggling to get out of Pokemon Red's <a href=\"https://www.twitch.tv/claudeplayspokemon\" target=\"_blank\" rel=\"noopener\">Mt. Moon</a>.\n            </p><p>\n              We wanted to create an LLM game benchmark that put this generation of frontier LLMs' superpower, \n              coding, on full display. Ten years ago, a team released a game called <a href=\"https://github.com/screeps/screeps\" target=\"_blank\" rel=\"noopener\">Screeps</a>. It was described \n              as an \"MMO RTS sandbox for programmers.\" In Screeps, human players write javascript strategies \n              that get executed in the game's environment. Players gain resources, lose territory, and have \n              units wiped out. It's a traditional RTS, but controlled entirely through code. \n            </p><p>\n              The Screeps paradigm, writing code and having it execute in a real-time game environment, is well suited \n              for an LLM benchmark. Drawing on a version of the Screeps open source API, LLM Skirmish pits \n              LLMs head-to-head in a series of 1v1 real-time strategy games.\n            </p></section><section><p>\n              In LLM Skirmish, each player begins with a \"spawn\" (a building that can create units), one \n              military unit, and three economic units. The objective of each LLM Skirmish match is to \n              eliminate your opponent's spawn. If a player is not eliminated within 2,000 game frames \n              (each player is allowed up to one second of runtime computation per frame), the game ends \n              and the victor is determined based on score.\n            </p></section><section><p>\n              Every LLM Skirmish tournament consists of five rounds. In each round, each LLM is asked to \n              write a script implementing its strategy. For all rounds after the first, each LLM can see \n              the results of all its matches from the previous round and use that information to make \n              changes to the script it submits for the next round. In every round, every player plays all \n              other players once. This means there are 10 matches per round and 50 matches per tournament.\n            </p></section><section><p>\n              LLM Skirmish was conducted using <a href=\"https://opencode.ai\" target=\"_blank\" rel=\"noopener\">OpenCode</a>, \n              an open source general purpose agentic coding harness. OpenCode was selected because it was not \n              designed for any of the evaluated models and is fully open source to aid in replicability.\n            </p><p>\n              Each LLM agent runs in an isolated Docker container with OpenCode providing the coding environment. \n              The orchestrator coordinates the tournament by sending prompts to each agent, which then uses \n              OpenCode's tools (file editing, shell commands, etc.) to write and submit their game scripts.\n            </p><p>\n              At the start of each round, agents receive \n              <a href=\"https://github.com/llmskirmish/skirmish/blob/main/prompts/OBJECTIVE.md\" target=\"_blank\" rel=\"noopener\">OBJECTIVE.md</a> \n              (the game rules, API documentation, and instructions for writing a game script) and \n              <a href=\"https://github.com/llmskirmish/skirmish/blob/main/prompts/NEXT_ROUND.md\" target=\"_blank\" rel=\"noopener\">NEXT_ROUND.md</a> \n              (instructions for reviewing match logs from the previous round, rounds 2-5 only). \n              Agents are also provided with <a href=\"https://github.com/llmskirmish/skirmish/tree/main/example_strategies\" target=\"_blank\" rel=\"noopener\">two example strategies</a> as reference.\n            </p><p>\n              After each agent creates their strategy, the orchestrator validates the script. If validation fails, the agent \n              receives the error message and has up to 3 attempts to fix the issue before the round proceeds.\n            </p></section><section><p>\n              LLM Skirmish tests in-context learning, as each tournament lasts five rounds and models are \n              able to alter strategies between rounds. One would hypothesize that if a model is successfully \n              learning in context, scripts written after seeing previous results (as in rounds 2–5) would be \n              of higher quality compared to scripts written in round 1.\n            </p><p>\n              Across all tournaments, each model submits 25 scripts for a total of 250 matches. In a tournament, \n              we consider each model to be a player. If we treat each script as a player and have all scripts \n              play against each other, we can simulate 7,750 matches to get a robust per-round average win rate \n              (a proxy for script quality).\n            </p><div><div><h3>Script Round vs Performance</h3></div></div><p>\n              We can see that four of the five models evaluated have notable increases in average win rate \n              between round 1 and round 5 (Claude Opus 4.5 +20%, GLM 4.7 +16%, GPT 5.2 +7%, Grok 4.1 Fast +6%).\n            </p><p>\n              Gemini 3 Pro's performance presents an anomaly. Its round 1 average win rate was 70% (higher \n              than all four other evaluated models), while its round 2-5 average win rate was 15% (lower than \n              all four other evaluated models). Gemini 3 Pro's round 1 scripts are approximately four times \n              shorter than those of top-performing models Claude 4.5 Opus and GPT 5.2. A qualitative review of \n              Gemini 3 Pro's scripts suggests it had success with simplistic strategies in round 1. In rounds \n              2-5, compared to the other four models evaluated, Gemini 3 Pro most aggressively populated its \n              context with previous round results before submitting its script for that round, suggesting that \n              context rot was a notable contributor to the performance variance. Whether this context rot reflects \n              other models being better at planning tool use than Gemini 3 Pro, or whether OpenCode is a \n              uniquely inhospitable harness for Gemini 3 Pro, is worth investigating further in future versions \n              of LLM Skirmish.\n            </p></section><section><p>\n              API costs vary significantly across models. The chart below plots each model's \n              average cost per round against its ELO rating. Claude Opus 4.5 achieved the highest \n              ELO (1778) but at the highest cost ($4.12/round). GPT 5.2 delivers nearly 1.7x more \n              ELO per dollar than Claude Opus 4.5.\n            </p></section>","contentLength":6380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47149586"},{"title":"Show HN: Context Mode – 315 KB of MCP output becomes 5.4 KB in Claude Code","url":"https://github.com/mksglu/claude-context-mode","date":1772000610,"author":"mksglu","guid":159,"unread":true,"content":"<p>Every MCP tool call dumps raw data into Claude Code's 200K context window. A Playwright snapshot costs 56 KB, 20 GitHub issues cost 59 KB. After 30 minutes, 40% of your context is gone.</p><p>I built an MCP server that sits between Claude Code and these outputs. It processes them in sandboxes and only returns summaries. 315 KB becomes 5.4 KB.</p><p>It supports 10 language runtimes, SQLite FTS5 with BM25 ranking for search, and batch execution. Session time before slowdown goes from ~30 min to ~3 hours.</p><p>MIT licensed, single command install:</p><p>/plugin marketplace add mksglu/claude-context-mode</p><p>/plugin install context-mode@claude-context-mode</p><p>Would love feedback from anyone hitting context limits in Claude Code.</p>","contentLength":698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47148025"},{"title":"Show HN: Linex – A daily challenge: placing pieces on a board that fights back","url":"https://www.playlinex.com/","date":1771976038,"author":"Humanista75","guid":158,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47145082"},{"title":"Show HN: Moonshine Open-Weights STT models – higher accuracy than WhisperLargev3","url":"https://github.com/moonshine-ai/moonshine","date":1771970047,"author":"petewarden","guid":157,"unread":true,"content":"<p>I wanted to share our new speech to text model, and the library to use them effectively. We're a small startup (six people, sub-$100k monthly GPU budget) so I'm proud of the work the team has done to create streaming STT models with lower word-error rates than OpenAI's largest Whisper model. Admittedly Large v3 is a couple of years old, but we're near the top the HF OpenASR leaderboard, even up against Nvidia's Parakeet family. Anyway, I'd love to get feedback on the models and software, and hear about what people might build with it.</p>","contentLength":540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47143755"},{"title":"Show HN: Hacker Smacker – Spot great (and terrible) HN commenters at a glance","url":"https://hackersmacker.org/","date":1771959616,"author":"conesus","guid":156,"unread":true,"content":"<section><p>Hacker Smacker helps you identify quality authors and filter out obnoxious commenters on Hacker News. Three little orbs appear next to every author's name and you can choose to either friend or foe them.</p><blockquote>If you friend people, and they also use Hacker Smacker, you'll see all of your friend's friends and foes. This helps you identify commenters that you want to read as you quickly scan a comment thread.</blockquote><p>I've found that this reduces the time I spent on Hacker News, as I can glance at long comment threads and just find the good stuff.</p><p>Hacker Smacker is directly inspired by Slashdot's friend/foe system. Hacker Smacker is also open-source and is <a href=\"https://github.com/samuelclay/hackersmacker\">available on GitHub</a>.</p></section><section><p>Hacker Smacker was built to learn how FoaF (Friend of a Friend) works. The idea is that not only do you want to surface content from your friends, but if you chose your friends well, they can help you surface more great content by highlighting comments from their friends.</p><p>The impetus for building a small system where the primary goal is simply to quickly show relationships was that I wanted to build the same system for <a href=\"http://www.newsblur.com\" rel=\"nofollow\">NewsBlur</a>, a visual RSS feed reader with intelligence. The backend is built using <a href=\"http://redis.io\" rel=\"nofollow\">Redis</a> sets and CoffeeScript/Node.js. NewsBlur's social layer, which was built immediately after this project, uses a very similar backend.</p><p>Learning how to build this project was the main reason, as I am now able to bring this technique to other projects.</p></section>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47141119"},{"title":"Show HN: Emdash – Open-source agentic development environment","url":"https://github.com/generalaction/emdash","date":1771956037,"author":"onecommit","guid":155,"unread":true,"content":"<p>Emdash is an open-source and provider-agnostic desktop app that lets you run multiple coding agents in parallel, each isolated in its own git worktree, either locally or over SSH on a remote machine. We call it an Agentic Development Environment (ADE).</p><p>We are building Emdash for ourselves. While working on a cap-table management application (think Stripe Atlas + Pulley), we found our development workflow to be messy: lots of terminals, lots of branches, and too much time spent waiting on Codex.</p><p>Emdash puts the terminal at the center and makes it easy to run multiple agents at once. Each agent runs as a task in its own git worktree. You can start one or a few agents on the same problem, test, and review.</p><p>Emdash works over SSH so you can run agents where your code lives and keep the parallel workflow. You can assign tickets to agents, edit files manually, and review changes.</p><p>We also spent time making task startup fast. Each task can be created in a worktree, and creating worktrees on demand was taking 5s+ in some cases. We now keep a small reserve of worktrees in the background and let a new task claim one instantly. That brought task start time down to ~500–1000ms depending on the provider. We also spawn the shell directly and avoid loading the shell environments on startup.</p><p>We believe using the providers’ native CLIs is the right approach. It gives you the full capabilities of each agent, always. If a provider starts supporting plan mode, we don't have to add that first.</p><p>We support 21 coding agent CLIs today, including Claude Code, Codex, Gemini, Droid, Amp, Codebuff, and more. We auto-detect what you have installed and we’re provider-agnostic by design. If there’s a provider you want that we don’t support yet, we can add it. We believe that in the future, some agents will be better suited for task X and others for task Y. Codex, Claude Code, and Gemini all have fans. We want to be agnostic and enable individuals and teams to freely switch between them.</p><p>Beyond orchestration, we try to pull most of the development loop into Emdash. You can review diffs, commit, open PRs, see CI/CD checks, and merge directly from Emdash once checks pass. When starting a task, you can pass issues from Linear, GitHub, and Jira to an agent. We also support convenience variables and lifecycle scripts so it’s easy to allocate ports and test changes.</p><p>Emdash is fully open-source and MIT-licensed.</p><p>Download for macOS, Linux or Windows (as of yesterday !), or install via Homebrew: brew install --cask emdash.</p><p>We’d love your feedback. How does your coding agent development setup look like, especially when working with multiple agents? We would want to learn more about it. Check out our repository here: <a href=\"https://github.com/generalaction/emdash\" rel=\"nofollow\">https://github.com/generalaction/emdash</a></p><p>We’ll be around in the comments — thanks!</p>","contentLength":2809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47140322"},{"title":"Show HN: enveil – hide your .env secrets from prAIng eyes","url":"https://github.com/GreatScott/enveil","date":1771909490,"author":"parkaboy","guid":154,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47133055"}],"tags":["dev"]}