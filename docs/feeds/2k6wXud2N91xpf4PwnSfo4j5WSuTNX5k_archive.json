{"id":"2k6wXud2N91xpf4PwnSfo4j5WSuTNX5k","title":"Tech News - Last 2 days","displayTitle":"Tech News - Last 2 days","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":75,"items":[{"title":"Race for All-Solid-State EV Batteries Heats Up with New Samsung SDI/BMW/Solid Power Partnership","url":"https://tech.slashdot.org/story/25/11/01/2158238/race-for-all-solid-state-ev-batteries-heats-up-with-new-samsung-sdibmwsolid-power-partnership?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762037940,"author":"EditorDavid","guid":257,"unread":true,"content":"All-solid-state batteries (ASSBs) \"are widely viewed as the 'holy grail' of EV battery tech,\" writes Electrek, \"promising to double driving range, halve charging times, and reduce costs.\" Toyota hopes to launch its first production EV powered by the batteries in 2027 or 2028, and Mercedes-Benz and Volkswagen are also testing the technology.\n \n\nBut now Samsung SDI is teaming up with BMW and US-based battery company Solid Power for their own effort at commercializing all-solid-state EV batteries \"in what's expected to be a trilateral powerhouse.\"\n\n\nBMW and Solid Power have been working together to develop the next-gen battery tech since 2022...\nUnder the new agreement signed this week, Samsung will supply all-solid-state battery cells. Samsung will use Solid Power's Sulfide-Based Solid Electrolyte solution, while BMW will develop the battery pack and modules. \n\nThe strategic alliance aims to take the lead in commercializing all-solid-state batteries (ASSBs). Together, they've created a real-world system for producing ASSB cells, pooling their expertise in batteries, automaking, and materials to bring it closer to mass production. Solid Power's electrolyte solution is designed for stability and maximum conductivity. By teaming up with BMW and Samsung SDI, the company said it aims to bring all-solid-state batteries closer to widespread adoption. \n\n\"By pooling resources, BMW, Samsung SDI, and Solid Power have a real shot...\" argues Electrek.","contentLength":1460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Could a Faint Glow in the Milky Way Be Dark Matter?","url":"https://science.slashdot.org/story/25/11/01/2118210/could-a-faint-glow-in-the-milky-way-be-dark-matter?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762033800,"author":"EditorDavid","guid":256,"unread":true,"content":"\"A nearby galaxy once thought to be dominated by dark matter seems to have a surprise supermassive black hole at its centre,\" reports New Scientist. \n\nYet scientists \"are convinced dark matter is out there,\" writes Space.com. \"The quest to detect it arguably remains both one of the most frustrating and most exhilarating challenges in modern physics.\" \n\nAnd now they report that the century-old mystery of dark matter — the invisible glue thought to hold galaxies together — \"just got a modern clue.\"\n\nScientists say they may be one step closer to confirming the existence of this elusive material, thanks to new simulations suggesting that a faint glow at the center of the Milky Way could be dark matter's long-sought signature. \"It's very hard to actually prove, but it does seem likely,\" Moorits Muru of the Leibniz Institute for Astrophysics Potsdam in Germany, who led the new study, told Space.com... \n\nThe findings, show that dark matter near the Milky Way's center might not form a perfect sphere as scientists long thought. Instead, it appears flattened, almost egg-shaped, and that shape closely mirrors the pattern of mysterious gamma rays observed by NASA's Fermi Gamma-ray Space Telescope... Using powerful supercomputers, [the researchers] recreated how the Milky Way formed, including billions of years of violent collisions and mergers with smaller galaxies. Those violent events, the researchers found, left deep \"fingerprints\" on the way dark matter is distributed in the galactic core.... matching the pattern of gamma-ray emission Fermi has observed, the new study reports... \n\nIf the excess truly arises from dark matter collisions, it would mark the first indirect evidence that weakly interacting massive particles [WIMPs], a leading dark matter candidate, really exist...\n \n\n\"We have run dozens of direct detection experiments around the globe hunting for WIMPS,\" notes\nPhys.org, in an article titled \"The Empty Search for Dark Matter.\"\n\nWe have run dozens of direct detection experiments around the globe hunting for WIMPS — dark matter particles in this particular mass range. And they're not all the same kind of experiments. There are also the scintillators, which use a giant vat of liquefied noble gas, like several tons of xenon. They wait for a dark matter particle to strike the xenon and cause it to scintillate, which is a fancy science word for \"sparkle.\" We see the sparkle; we detect dark matter... \n\nThey're just one example of a broader class of dark matter candidates, with delightful names like Q-balls, WIMPzillas, and sterile neutrinos. We've tuned our different experiments to capture different mass ranges or interaction strengths to cover as much of that wide dark matter spectrum as possible. We've even tried to manufacture various kinds of dark matter in our particle collider experiments.\n \nAnd we've found nothing.\n\n","contentLength":2876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Debian's APT Will Soon Begin Requiring Rust: Debian Ports Need To Adapt Or Be Sunset","url":"https://www.phoronix.com/news/Debian-APT-Will-Require-Rust","date":1762032202,"author":"Michael Larabel","guid":701,"unread":true,"content":"<article>Debian developer Julian Andres Klode sent out a message on Halloween that may give some Debian Linux users and developers a spook: the APT packaging tool next year will begin requiring a Rust compiler. This will place a hard requirement by Debian Linux on Rust support for all architectures. Debian CPU architectures with ports currently but lacking Rust support will either need to see support worked on or be sunset...</article>","contentLength":420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Employees Are the New Hackers: 1Password Warns AI Use Is Breaking Corporate Security","url":"https://it.slashdot.org/story/25/11/01/2047217/employees-are-the-new-hackers-1password-warns-ai-use-is-breaking-corporate-security?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762030200,"author":"EditorDavid","guid":255,"unread":true,"content":"Slashdot reader BrianFagioli writes: Password manager 1Password's 2025 Annual Report: The Access-Trust Gap exposes how everyday employees are becoming accidental hackers in the AI era. The company's data shows that 73% of workers are encouraged to use AI tools, yet more than a third admit they do not always follow corporate policies. Many employees are feeding sensitive information into large language models or using unapproved AI apps to get work done, creating what 1Password calls \"Shadow AI.\" At the same time, traditional defenses like single sign-on (SSO) and mobile device management (MDM) are failing to keep pace, leaving gaps in visibility and control. The report warns that corporate security is being undermined from within. More than half of employees have installed software without IT approval, two-thirds still use weak passwords, and 38% have accessed accounts at previous employers. Despite rising enthusiasm for passkeys and passwordless authentication, 1Password says most organizations still depend on outdated systems that were never built for cloud-native, AI-driven work. The result is a growing \"Access-Trust Gap\" that could allow AI chaos and employee shortcuts to dismantle enterprise security from the inside.","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elon Musk wants you to know that Sam Altman got a refund for his Tesla Roadster","url":"https://techcrunch.com/2025/11/01/elon-musk-wants-you-to-know-that-sam-altman-got-a-refund-for-his-tesla-roadster/","date":1762027270,"author":"Anthony Ha","guid":184,"unread":true,"content":"<article>Elon Musk and Sam Altman are still taking swipes at each other on Musk’s social media platform X.</article>","contentLength":99,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NASA Seeks Backup Plan for Carrying Astronauts to the Moon","url":"https://science.slashdot.org/story/25/11/01/1737240/nasa-seeks-backup-plan-for-carrying-astronauts-to-the-moon?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762025640,"author":"EditorDavid","guid":254,"unread":true,"content":"An anonymous reader shared this report from CNN:\n[C]iting delays in Starship's development and competitive pressure from China, NASA asked SpaceX and Blue Origin — which holds a separate lunar lander contract with the space agency — to submit plans to expedite development of their respective spacecraft by October 29. Both companies have responded. But the space agency is also asking the broader commercial space industry to detail how they might get the job done more quickly, hinting that NASA leadership is prepared to sideline its current partners. CNN spoke with half a dozen companies about how they plan to respond to NASA's call to action, which the agency will formally issue once the government shutdown ends, according to a source familiar with the matter. \n\nOne possibility is Lockheed Martin...\n\n\nNotably, as a legacy NASA contractor, the company built the $20.4 billion Orion spacecraft that astronauts will ride when they take off from Earth... Now, Lockheed says it can piece together a two-stage lunar lander that uses spare parts harvested from Orion. The company would make use of Space Shuttle-era OMS-E engines — which are also used on Orion — to serve as the propulsion for an \"ascent stage\" of the lunar lander, providing the thrust for the vehicle to lift off the moon after a mission is completed. But the vehicle also needs a descent stage to get down to the lunar surface in the first place... \n\nOther commercial space companies contacted by CNN — including Firefly Aerospace and Northrop Grumman — said simply that they were \"ready to support\" NASA in its endeavor to find a faster way to complete the Artemis III mission. They did not confirm whether they would formally respond to the space agency's anticipated request for companies to submit proposals. \n\nThe more important goal, argue some experts, is to pave the way for a permanent lunar base where astronauts can live and work...\n\n[P]erhaps the true winner will be the country that is able to build lasting infrastructure, experts say.\n\"It makes great press fodder to frame this as competition,\" said one space policy source, who was among several that spoke to CNN on the condition of anonymity to discuss controversial issues. \"But this is about the long game and the sustainability.\"\n","contentLength":2287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GodLoader Malware Loader: What You Need to Be Aware of","url":"https://hackernoon.com/godloader-malware-loader-what-you-need-to-be-aware-of?source=rss","date":1762023609,"author":"Godot Engine (Technical Documentation)","guid":300,"unread":true,"content":"<p>Security researchers at <a href=\"https://research.checkpoint.com/\">Check Point Research</a> have <a href=\"https://research.checkpoint.com/2024/gaming-engines-an-undetected-playground-for-malware-loaders/\">published a report</a> about GodLoader, a malware loader using Godot as its runtime to execute malicious code and infect unaware users with known malware. Based on the report, affected users thought they were downloading and executing cracks for paid software, but instead executed the malware loader.</p><p>\\\nAs the report states, the vulnerability is not specific to Godot. The Godot Engine is a programming system with a scripting language. It is akin to, for instance, the Python and Ruby runtimes. It is possible to write malicious programs in any programming language. We do not believe that Godot is particularly more or less suited to do so than other such programs.</p><p>\\\n<strong>If you downloaded a Godot game or the editor from a reliable source, you don’t have to do anything. You are not at risk.</strong> We encourage people to only execute software from trusted sources – whether it’s written using Godot or any other programming system.</p><p>\\\nFor some more technical details:</p><p>Godot does not register a file handler for  files. This means that a malicious actor always has to ship the Godot runtime ( file) together with a  file. The user will always have to unpack the runtime together with the  to the same location and then execute the runtime. There is no way for a malicious actor to create a “one click exploit”, barring other OS-level vulnerabilities. If such an OS-level vulnerability were used then Godot would not be a particularly attractive option due to the size of the runtime.</p><p>\\\nThis is similar to writing malicious software in Python or Ruby, the malicious actor will have to ship a  or  together with their malicious program.</p><p>We would like to take this opportunity to remind users about some good security practices when it comes to downloading and executing software.</p><ul><li>Only download and execute software (including game mods) from trusted sources:</li><li>Official project website. Confirm it by checking the URL, and verify with a search engine that this seems to be the most frequently referenced website for this software.</li><li>Trusted distribution platform: Steam, Epic Games Store, Windows Store, Google Play, Apple Store, etc.</li><li>People you know, after confirming that they are who they claim to be if the communication is text-based (see below).</li><li>On Windows and macOS, verify that the executable is signed (and notarized, on macOS) by a trusted party.</li><li>Be wary of executing cracked software, which is a prime attack vector for malicious actors.</li><li>Be wary of executing software even from people you know, if you can’t confirm that their account hasn’t been compromised. A very common attack vector targeting specifically game developers is for Discord accounts to get hacked, and then malicious actors use them to send malicious downloads to their friends in private messages (“hey will you try my game?”). Make sure to confirm the identity of your contacts before executing such software.</li></ul><h2>Reporting security issues</h2><p>We thank Check Point Research for following the security guidelines of responsible disclosure, which let us confirm that this attack vector, while unfortunate, is not specific to Godot and does not expose a vulnerability in the engine or for its users.</p>","contentLength":3204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scientists Say 'Dueling Dinosaurs' Fossil Confirms a Smaller Tyrannosaur Species, Not a Teenaged T. Rex","url":"https://science.slashdot.org/story/25/11/01/0740245/scientists-say-dueling-dinosaurs-fossil-confirms-a-smaller-tyrannosaur-species-not-a-teenaged-t-rex?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762022040,"author":"EditorDavid","guid":253,"unread":true,"content":"An anonymous reader shared this report from NPR:\n\nIt's known as the \"Dueling Dinosaurs\" fossil: A triceratops and a tyrannosaur, skeletons entangled, locked in apparent combat right up until the moment of their mutual demise... That discovery in 2006 now appears to have overturned decades of dinosaur dogma about Tyrannosaurus rex, the fearsome giant long thought to be the sole top predator stalking the late Cretaceous. In a paper in the journal Nature, paleontologists Lindsay Zanno and James Napoli conclude that some of the bones from that specimen belong not to a teenage T. rex, but to a fully grown individual of a different tyrannosaur species — Nanotyrannus lancensis.... \n\n\nOne of the first of those red flags in the new specimen was the arm bones. They looked completely different than T. rex's puny appendages... \"These are powerful arms with large claws, large hands. They were using them for prey capture.\" Contrast that with T. rex, \"an animal that's a mouth on legs.\" There were additional clues. The animal had fewer tail vertebrae and more teeth than T. rex. Zanno and Napoli considered other lines of evidence. They created 3D models of numerous purported T. rexes against which they compared their specimen. They looked at the growth stages of the cranial nerves and sinuses of close living relatives of dinosaurs, features that were visible in the fossilized skeleton. \n\n\"But maybe the most important and damning thing that we did was we were able to figure out that our animal is not a juvenile at all,\" she says. This conclusion was based on slicing through the fossil's limb bones to examine the growth rings. That work demonstrated that this animal was mature and done growing when it died around the age of 20. \"That means it's half the size and a tenth of the mass of a full grown Tyrannosaurus rex,\" says Zanno... In addition, while making models of all those other alleged T. rex skeletons, Zanno says they identified another new species of tyrannosaur, one they're calling Nanotyrannus lethaeus... \n\n\"It tells us that these end-Cretaceous ecosystems right before the asteroid hit were flourishing,\" says Zanno. \"They had an abundance of different predators. And refutes this idea that dinosaurs were in decline before the asteroid struck.\"\n\n","contentLength":2275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Post Mindfully: A Guide","url":"https://hackernoon.com/how-to-post-mindfully-a-guide?source=rss","date":1762018211,"author":"The Markup","guid":299,"unread":true,"content":"<p>I’m Aaron Sankin, a reporter here at The Markup. And I’m here to talk about how it’s kind of your fault that the internet sucks. But also it isn’t your fault at all, and you should probably be nicer to yourself about it.</p><p>\\\nBack in 2021, when Meta whistleblower Frances Haugen <a href=\"https://www.wsj.com/articles/the-facebook-files-11631713039\">leaked</a> a trove of the tech giant’s internal documents, much of the focus was on how the company’s products could be harmful to mental health—especially for young people. Instagram, the documents showed, had a tendency to make teen girls feel <a href=\"https://www.wsj.com/articles/facebook-knows-instagram-is-toxic-for-teen-girls-company-documents-show-11631620739?mod=article_inline\">worse</a> about their bodies. A 2018 shift in Facebook’s algorithm intended to strengthen social bonds instead <a href=\"https://www.wsj.com/articles/facebook-algorithm-change-zuckerberg-11631654215?mod=article_inline\">stoked</a> users’ rage. And an internal report found that one in eight users <a href=\"https://www.wsj.com/articles/facebook-bad-for-you-360-million-users-say-yes-company-documents-facebook-files-11636124681?mod=article_inline\">reported</a> having an unhealthy relationship with the platform.</p><p>\\\nThe reports sparked a public firestorm, not to mention <a href=\"https://time.com/6104070/facebook-whistleblower-congressional-hearing-takeaways/\">congressional hearings</a>, but for Whitney Phillips and Ryan Milner, Haugen’s smuggled documents confirmed what their years of research had already showed definitively: the online world can be a real drag.</p><p>\\\nPhillips and Milner—who teach media literacy at University of Oregon and College of Charleston, respectively—have spent their academic careers looking at toxic effects of online culture in books like  and . They began by looking at online enclaves infamous for digital pollution, like troll communities that delight in spreading toxic mischief far and wide. But they’ve come to see many of the bad feelings engendered by social media as algorithmically-intensified versions of the same set of anxieties that have plagued humanity since before two computers first traded data packets.</p><p>\\\nEveryone who uses the internet has, at some point, probably done something crummy and been the victim of someone else doing something crummy. It’s the circle of posts and it moves us all. But, luckily, interrupting this cycle is possible—if you use the internet a little more slowly, a little more mindfully.</p><p>\\\nI spoke with Phillips and Milner about how people can better regulate their emotions while scrolling, and to know when to pause, take a breath, and not get drawn into an online flame war that will only serve to make life worse for everyone involved.</p><p>\\\n<em>The following conversation has been edited for length and clarity.</em></p><p> One of the things I liked about <em>Share Better and Stress Less</em> is that even though it’s framed in a simple and straightforward way that young people can understand, a lot of this advice is fairly universal for anyone trying to stay sane while using the internet. What prompted you to write the book?</p><p>\\\n My own experience with this conversation about mental health and the role it plays in how we’re sharing and what our online spaces look like, a lot of that was really crystallized as COVID ramped up.</p><p>\\\nAs that was happening, more of the conversations [among students in my media literacy class] were shifting to mental health. In part, because the world was getting increasingly more stressful, both politically and then also COVID. And that, of course, became political very quickly</p><p>\\\nIn the classroom, the more I saw my students struggling, the more they were admitting to sharing in ways they recognized as being problematic. Students were starting to hear that maybe the university would be shut down. They’d be like, “All I’m doing is sharing rumors. I know most of them are probably false. I know that it’s probably bad that I’m doing it. But it makes me feel like I’m doing something and I cannot help it.”</p><p>\\\nThey were trying to process information and hitting retweet, or whatever platform they were on, that was part of their process of coping.</p><p>\\\n So much of the book is about regulating your own emotions, work that’s happening on this side of the screen, happening inside of your head. It’s thinking about your own relationship to social media as you are using social media and putting thoughtfulness into your moment-to-moment experience.</p><p>\\\n A lot of that is not just your relationship to social media, it’s your relationship to yourself. How well are you able to identify what is happening in your body? People were not talking about embodied experiences when they were talking about the problem of mis- and dis-information. But those issues are intimately connected.</p><p>\\\nWe all know what it feels like to be stressed out. That’s not something that’s talked about in a systematic way. This isn’t just about social media. This is about meat, our bones and our nervous system. Where has that been in this conversation about media literacy?</p><p>\\\n As we went along in our research, we started thinking and talking more about the cognitive and psychological elements. It was happening in our classroom conversations, for sure, but it was also happening in my own life.</p><p>\\\nDuring lockdown, when we were remote teaching, I would teach my couple classes in the morning and then during the afternoon, I wouldn’t have a ton to do. So I’d just melt into the couch and scroll through Twitter.</p><p>\\\nWe use one example in the book of me scrolling through Twitter, seeing a video, overreacting to it, sending it to my brother, freaking out about it, and having to stop, fact check it, and then walk it all back. If, in the moment, I had stopped and said, “Where am I at? How am I doing? What am I doing? Is this the best for me right now?” If I would have had that moment of slowness and self-reflection, it would have done me a lot of good.</p><p>\\\n Can you walk me through that feedback loop? Using the internet and coming across content that triggers you in a way that creates behaviors that, when you look back at them, you’re like, “I was not my best self in that moment.”</p><p>\\\n I’m not a neuroscientist. So I’m not going to I’m not going to claim that level of expertise. But in reading and listening to people who are experts, a good way of looking at your brain is as a closed fist. The front part of your brain, the prefrontal cortex, is where all of your higher-level decision-making takes place. It’s what allows you to distinguish between real threats and imagined threats. Some scientists refer to it as the upstairs brain.</p><p>\\\nThe downstairs part of the brain is where you have a lizard functioning. That’s the fight/flight/freeze, the limbic system. That’s what gets activated, what lights up when something overloads your system and you suddenly get overwhelmed, get scared, get angry.</p><p>\\\nThe best way to think about it is that essentially you flip your lid—your prefrontal cortex becomes dislodged from the bottom part of your brain. You’re no longer able to do all of that really important higher-order functioning that that can help you identify, “that’s not actually a threat to me right now,” or “I can try to see where someone else is coming from,” or “I’m going to do something that’s going to help me take care of myself.”</p><p>\\\nAll of those really important skills literally go offline. One of the things that activates the limbic system is when you get overwhelmed by too much information. Too much is coming at you. It’s too scary. It’s confusing. Once you lose that, the ability to have thoughtful communication goes out the door. Your ability to take care of yourself in appropriate ways goes out the door. Your ability to think about other people as people goes out the door.</p><p>\\\nAll of those things are what push people into the screaming matches, to share things that under different circumstances when their lid was not flipped, they would maybe think twice about. We just don’t have the ability to be our most thoughtful, generous, flexible selves when we move into this space of stress.</p><p>\\\nOnline spaces, because they’re designed to inundate us with information, are designed to inundate us with stress. Online spaces are just not conducive to this kind of thinking. That’s a problem because this kind of thinking is actually how we have important, difficult, challenging, thoughtful communication with other people. A lot goes wrong when you flip your lid and it just so happens the spaces we spend the most time in are designed to make it very difficult to keep your lid screwed on tight.</p><p>\\\n When thinking of a younger audience, I think it’s even more of an important lesson, even more important to remember. Because, as we know, the upstairs brain isn’t fully developed. The prefrontal cortex isn’t as fully there. Young people in general are more inclined to shortsighted thinking, impulsiveness, and all the stereotypes about teenagers.</p><p>\\\nIt really becomes this perfect storm where you have people who haven’t thought about the stuff as much yet, their brain isn’t quite there. And then you’ve got, like Whitney was just saying, an ecosystem designed to feed on those kinds of impulses.</p><p>\\\n How would someone develop a practice that would allow them to disrupt this mechanism?</p><p>\\\n These days, when I have conversations about technology, almost immediately, the conversation stops being about technology. A lot of what drives anxious behavior is fear of rejection. It’s feeling like you don’t have something that you need.</p><p>\\\nWhen people say, what should we do about these problems with technology? My short answer, and I’m not really being glib, is: everybody should be in therapy. But then, also recognizing that not everyone can go to therapy, not everyone has access to therapy.</p><p>\\\nBut if you can’t, for whatever reason, how can you shift that relationship with yourself so that you are listening in a more compassionate way? Oftentimes when people get really frazzled and angry, they think what they’re feeling is anger and they’re actually wrong. It’s something else. It’s something that’s softer. We don’t always know how to talk to that part of ourselves and particularly how it manifests in our body.</p><p>\\\nThe need for connection and the anxiety that emerges when we don’t get what we need as humans, as social beings. That isn’t new. That’s been around forever. But the technology has really shifted some of the dynamics and intensified a lot of the things that have already existed. they create more screens that people aren’t responding to us on.</p><p>\\\nI think it’s really important for people to think about some of the negative or potentially negative consequences of technologies. But also getting so focused on just the technologies themselves takes away both the embodied conversation and takes away from the fact that being a person is hard. It’s just really hard. Being an adult is really hard. Being a kid is really hard. We never fully figure things out. And every time we get a little older and get a little bit more mature, we realize other areas where there’s still a lot to learn.</p><p>\\\n You make this argument in your book that everyone is in some way responsible for online pollution. If you exist online, you are going to do things from time to time that make the internet a slightly worse place. That really shifts the dynamic of the conversation, which I think is largely focused around the idea that the really toxic trolls, the apex predators of internet toxicity, are the problem.</p><p>\\\n The initial assumption that a lot of people would make going into a social media how-to guide for middle schoolers would be more about how to protect yourself from cyber bullies, from catfishes, from predators. How to put on your armor and put up your defenses. Ways that you can shelter yourself and insulate yourself from the big bad world out there.</p><p>\\\nBut I think young people hear a lot of that and, in my experience having three kids, they get a lot of that. A lot of that is pretty intuitive after a little bit of conversation. When you’re gaming online, don’t give out your personal information. Don’t add random people on Snapchat.</p><p>\\\nThe mentality is not one of pointing a finger, not one of saying “you’re messing this up” or “you’re doing this wrong” or “you’re being mean.” But instead, even if you think you’re right, think about the unintended consequences of being a person online.</p><p>\\\nWe all know the story of how social media platforms get their money. The problem is that those ways to get money are very often directly at odds with what we need to be our best selves in these places. They’re at odds with being slow or being mindful. They reward that lizard brain sharing. They reward the anger. They reward the outrageous and sensational. There’s all kinds of tricks and schemes to get you to keep refreshing, keep scrolling.</p><p>\\\nThe norms of social media now—the algorithmic docency, the slot machine dynamics, the reward for sharing and consuming outrage—none of those align well with the practices, mediated or otherwise, that get us to a less stressed kind of place.</p><p>\\\n It’s important to get people to reflect on the less obvious parts of this conversation. If you’re approaching the issue as the problem online is people who intend to do harm. They’re setting out to cause harm. They are mean on purpose. If so, the solution to the problem is just don’t be mean, and that’s all you need to do.</p><p>\\\nIf you shift how you are thinking to be about where polluted information, or problematic information, comes from, to think about the ways that we can potentially contribute to it, or that we’re just all part of this process, then nobody can step outside of the ecosystem, we’re all contributing something. That is an incentive to really take care of ourselves.</p><p>\\\nEspecially aiming this message at younger people, God help them, they’re going to be inheriting this soon enough. And the question is what kinds of systems are they going to build? What kind of systems are they going to accept?</p><p>\\\nThey won’t be able to know how to resist that or build something different if they don’t know what the problems are. You’ve got to have conversations about the individual and conversations about the collective, big and small, happening simultaneously to try to think about what kind of world could we build together.</p><p>Take a deep breath and check in with yourself,</p>","contentLength":13999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coinbase CEO Brian Armstrong trolls the prediction markets","url":"https://techcrunch.com/2025/11/01/coinbase-ceo-brian-armstrong-trolls-the-prediction-markets/","date":1762016348,"author":"Anthony Ha","guid":183,"unread":true,"content":"<article>While Armstrong may have helped some Kalshi and Polymarket users make a little money, he was also illustrating how easily these markets can be manipulated.</article>","contentLength":155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Did a Weather Balloon, Not a Mysterious Space Object, Strike That United Airlines Flight?","url":"https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysterious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762014840,"author":"EditorDavid","guid":251,"unread":true,"content":"Slashdot reader joshuark shares this report from SFGate:\n\n\nThe mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... \n\nWindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... \"At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further,\" [WindBorne's CEO John Dean posted on social media...]\n WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. \n\nWindBorne \"has conducted more than 4,000 launches,\" the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus \"The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight.\"\n\n\nWe are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.","contentLength":1642,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rising energy prices put AI and data centers in the crosshairs","url":"https://techcrunch.com/2025/11/01/rising-energy-prices-put-ai-and-data-centers-in-the-crosshairs/","date":1762013700,"author":"Tim De Chant","guid":182,"unread":true,"content":"<article>A majority of consumers say they’re worried about data centers driving up electricity costs. Is the industry prepared for a possible backlash?</article>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Elaborate Hoaxes in the Age of AI (11/1/2025)","url":"https://hackernoon.com/11-1-2025-newsletter?source=rss","date":1762012930,"author":"Noonification","guid":298,"unread":true,"content":"<p>🪐 What’s happening in tech today, November 1, 2025?</p><p>By <a href=\"https://hackernoon.com/u/jacoblandry\">@jacoblandry</a> [ 4 Min read ] We know theres a lot of unethical ways to use AI but at what point are we not even going to know AI was used? <a href=\"https://hackernoon.com/elaborate-hoaxes-in-the-age-of-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hacker-Antho\">@hacker-Antho</a> [ 5 Min read ] On-policy distillation is more than just another training technique; its a foundational shift in how we create specialized, expert AI.  <a href=\"https://hackernoon.com/beyond-brute-force-4-secrets-to-smaller-smarter-and-dramatically-cheaper-ai\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elaborate Hoaxes in the Age of AI","url":"https://hackernoon.com/elaborate-hoaxes-in-the-age-of-ai?source=rss","date":1762012807,"author":"Jacob Landry","guid":297,"unread":true,"content":"<p>This week, I’ve seen a lot of over-dramatization of very simple factual events that seem to be fueled by AI in many ways. Now, these aren’t “caused” by AI; I’m not referring to hoaxes that people have used AI specifically to spread, but things that AI has made worse by the ease with which fake information can be made to look very, very real.</p><p>Fifteen years ago, this problem existed, but in my opinion, was severely muted. The concept of biased news is not new by any means and has been an issue for as long as the news has existed. There have always been audiences that are more susceptible to believing in these invented stories, scenarios, and scams, and the media has always catered to them, guiding them to the water they wish them to drink. </p><p>\\\nMy concern, and reason for this brain dump, is that with AI, these evil parties seem to be able to cast a much wider net than they could before. They can twist real news into something it’s not with fake videos made by AI; they can pump the internet full of AI-generated content that says whatever they want and cites other AI-generated sources, and they can mobilize an army of influencers that spread their filth like wildfire in an instant.</p><p>\\\nI’ve found that recently, a huge chunk of my time when consuming any form of media is spent asking, “Is this real?” I consistently have to find multiple sources and manually scan them, looking for clues that it was AI-generated, a task that is getting harder and harder by the week. The videos are getting more realistic, the content is written better, and the sources I'm used to relying on are less and less trustworthy.</p><p>A group of protestors (in this case, trolls) showed up at Chicago’s Bean with claims that there was a man trapped inside. The protestors claimed that they had found evidence that a wealth of life-support systems had been purchased during the making of the Bean statue, and also attempted to make a connection to a potential missing person (a baby, I believe) around the time of its construction. Trolls exist. They always have and always will. That’s not the issue at hand here. The issue is what happened next. This group was clearly trying to be funny, just causing a stir with some radical idea for their own amusement, but the internet used AI to take the country by storm.</p><p>\\\nWhile scrolling, I started to see dozens of videos with screenshots of these purchase records, x-ray footage of a person floating inside the Bean, and “eyewitness” reports from someone who claimed they could hear knocking or scratching coming from inside the structure. There were also videos of the Bean being constructed, where you could clearly see the equipment being placed inside. Most, if not all, of these were generated by AI and are completely fake. I knew this, being a sensible human, but I had to admit that the quality created compelling evidence. With less common sense, I would have been easily duped.</p><p>The recent discovery of 3I/ATLAS has been a goldmine for AI generators and the conspiracy-loving masses. From what I could find, which wasn’t much because it seems a lot of this information is being controlled to stop the spread of disinformation, all we know is that a comet is passing through our solar system. This comet looks like a comet and acts like a comet, but is slightly faster and is not orbiting our sun. One scientist ventured a challenge to the “it’s just a comet” consensus to encourage more critical thinking, theorizing that it was, of course, possible for it to be an alien craft. This set the AI-loving conspiracy nuts on fire.</p><p>\\\nMy Instagram feed was on absolute fire with fake videos of this comet with lights being emitted from the sides like a ship, with exhaust clearly venting into space, and fake X-ray shots that showed the internal ship structure and beings inside. Countless videos of influencers pretended to be experts on the matter and talked about potential alien invasions. The worst part of all of this was that each one cited different sources and pulled from different video content. It was easy to assume that a “potential alien invasion” was fake; however, I have to admit, the video content they provided was stunningly believable. The “experts” talking were confident and had plenty of “research” to back them up.</p><p>\\\nThe most alarming thing about this entire situation is how fast this misinformation is able to spread and how absolutely believable it can make it. I know this isn’t a controlled situation, and we’ve always had irresponsible people running social media accounts to susceptible individuals, but the use of AI in these fields is making the problem more abundant and harder to discern.</p><h2><strong>It’s not all bad… but it’s pretty bad.</strong></h2><p>AI is a wonderful tool that has the potential to make our lives easier. I don’t believe that it is ready for constant use yet, despite it being shoved down our throats around every turn. It consistently hallucinates and makes false claims; it slows my work down more than it speeds it up, and has become a barrier to productivity in most situations I’ve tried to use it. </p><p>\\\nHowever, I can admit it has potential, and there are small automation tasks that I do find small uses for it. That being said, there’s always going to be a heated conversation around ethics and how we should be using AI. </p><p>\\\nI don’t believe there will be any disagreement, however, that the above cases are the wrong way to use AI. The use of AI to generate content to fuel conspiracy theories and spread them to the masses as facts is dangerous and, frankly, terrifying. I believe we’re only seeing the tip of the iceberg, and I worry that we’re headed for a future where we’ll no longer be able to discern the difference between truth and fiction.</p>","contentLength":5778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Holes Found in OpenAI's ChatGPT Atlas Browser (and Perplexity's Comet)","url":"https://it.slashdot.org/story/25/11/01/054213/security-holes-found-in-openais-chatgpt-atlas-browser-and-perplexitys-comet?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762011240,"author":"EditorDavid","guid":250,"unread":true,"content":"The address bar/ChatGPT input window in OpenAI's browser ChatGPT Atlas \"could be targeted for prompt injection using malicious instructions disguised as links,\" reports SC World, citing a report from AI/agent security platform NeuralTrust:\n\nNeuralTrust found that a malformed URL could be crafted to include a prompt that is treated as plain text by the browser, passing the prompt on to the LLM. A malformation, such as an extra space after the first slash following \"https:\" prevents the browser from recognizing the link as a website to visit. Rather than triggering a web search, as is common when plain text is submitted to a browser's address bar, ChatGPT Atlas treats plain text as ChatGPT prompts by default. \n\nAn unsuspecting user could potentially be tricked into copying and pasting a malformed link, believing they will be sent to a legitimate webpage. An attacker could plant the link behind a \"copy link\" button so that the user might not notice the suspicious text at the end of the link until after it is pasted and submitted. These prompt injections could potentially be used to instruct ChatGPT to open a new tab to a malicious website such as a phishing site, or to tell ChatGPT to take harmful actions in the user's integrated applications or logged-in sites like Google Drive, NeuralTrust said. \n\nLast month browser security platform LayerX also described how malicious prompts could be hidden in URLs (as a parameter) for Perplexity's browser Comet. And last week SquareX Labs demonstrated that a malicious browser extension could spoof Comet's AI sidebar feature and have since replicated the proof-of-concept (PoC) attack on Atlas. \n\n\n\nBut another new vulnerability in ChatGPT Atlas \"could allow malicious actors to inject nefarious instructions into the artificial intelligence (AI)-powered assistant's memory and run arbitrary code,\" reports The Hacker News, citing a report from browser security platform LayerX:\n\n\n\n\"This exploit can allow attackers to infect systems with malicious code, grant themselves access privileges, or deploy malware,\" LayerX Security Co-Founder and CEO, Or Eshed, said in a report shared with The Hacker News. The attack, at its core, leverages a cross-site request forgery (CSRF) flaw that could be exploited to inject malicious instructions into ChatGPT's persistent memory. The corrupted memory can then persist across devices and sessions, permitting an attacker to conduct various actions, including seizing control of a user's account, browser, or connected systems, when a logged-in user attempts to use ChatGPT for legitimate purposes.... \n\n\"What makes this exploit uniquely dangerous is that it targets the AI's persistent memory, not just the browser session,\" Michelle Levy, head of security research at LayerX Security, said. \"By chaining a standard CSRF to a memory write, an attacker can invisibly plant instructions that survive across devices, sessions, and even different browsers. In our tests, once ChatGPT's memory was tainted, subsequent 'normal' prompts could trigger code fetches, privilege escalations, or data exfiltration without tripping meaningful safeguards....\" \n\n\nLayerX said the problem is exacerbated by ChatGPT Atlas' lack of robust anti-phishing controls, the browser security company said, adding it leaves users up to 90% more exposed than traditional browsers like Google Chrome or Microsoft Edge. In tests against over 100 in-the-wild web vulnerabilities and phishing attacks, Edge managed to stop 53% of them, followed by Google Chrome at 47% and Dia at 46%. In contrast, Perplexity's Comet and ChatGPT Atlas stopped only 7% and 5.8% of malicious web pages. \n\nFrom The Conversation:\n\nSandboxing is a security approach designed to keep websites isolated and prevent malicious code from accessing data from other tabs. The modern web depends on this separation. But in Atlas, the AI agent isn't malicious code — it's a trusted user with permission to see and act across all sites. This undermines the core principle of browser isolation. \n\n\nThanks to Slashdot reader spatwei for suggesting the topic.","contentLength":4094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Brute Force: 4 Secrets to Smaller, Smarter, and Dramatically Cheaper AI","url":"https://hackernoon.com/beyond-brute-force-4-secrets-to-smaller-smarter-and-dramatically-cheaper-ai?source=rss","date":1762009207,"author":"Anthony Laneau","guid":296,"unread":true,"content":"<p>Large Language Models (LLMs) are incredibly powerful generalists, but transforming them into specialized experts is a major challenge. The process of training a model on new, specific knowledge like internal company documents or a complex reasoning task is notoriously expensive, time-consuming, and fraught with pitfalls. We want smaller, more efficient models that can master a domain without the compute budget of a tech giant.</p><p>\\\nThe core idea behind making smaller models smarter is a concept called \"distillation.\" In this process, a smaller \"student\" model learns from a larger, more capable \"teacher\" model. The student doesn't just learn from a static textbook of examples; it learns to mimic the teacher's thought process. This is a powerful shortcut for transferring expertise.</p><p>\\\nUntil now, however, engineers have faced a frustrating trade-off. One approach, on-policy reinforcement learning (RL), forces the student to learn from its own mistakes, which is relevant but painfully slow. The alternative, off-policy distillation, is much faster but dangerously flawed; the student learns from the teacher's ideal examples, which often occur in contexts the student will never encounter on its own, causing errors to compound. This has been the bottleneck for creating specialized AI; until now.</p><p>\\\nA powerful technique called \"on-policy distillation\" combines the best of both worlds. By having a teacher model provide dense, token-by-token feedback on the student model's own attempts, we can achieve breakthroughs in training efficiency and capability. Here are the four most surprising and impactful takeaways from this approach.</p><h3>A Smarter Feedback Loop Makes AI Training Up to 100x Cheaper</h3><p>The fundamental difference between Reinforcement Learning (RL) and Distillation lies in the density of the feedback. To understand this, imagine learning to play chess.</p><ul><li> is like learning chess by only being told if you won or lost at the very end of a match. The feedback is directly related to your actions, but it's sparse. You know you lost, but you don't know if it was because of your opening, a mid-game blunder, or a weak endgame.</li><li> is like watching a grandmaster play. You observe brilliant moves, but they are made in complex board positions that you, as a novice, will rarely find yourself in. The feedback is dense, but the context is often irrelevant to your own learning path.</li><li> provides the best of both worlds. It's like having an expert coach who grades every single one of your moves in your own games, telling you if a move was a \"blunder,\" \"inaccuracy,\" or \"brilliant.\" The feedback is both dense and perfectly relevant to your current skill level.</li></ul><p>\\\nThis smarter feedback loop has a massive impact on efficiency. In a direct back-to-back comparison where a student model learned from a teacher trained via RL, on-policy distillation allowed the student to reach the teacher's performance level 7-10 times faster in terms of gradient steps. This translates to a staggering 50-100x improvement in cumulative compute efficiency.</p><p>\\\nThe reason for this dramatic speedup is that on-policy distillation provides more useful information (more \"bits per episode\") for the model to learn from. Because this dense, token-level feedback reduces gradient noise, it allows for training with shorter contexts and smaller, more efficient batch sizes, further slashing the overall computational cost.</p><h3>You Can Cure “AI Amnesia” When Teaching New Knowledge</h3><p>A common and frustrating problem in AI is \"catastrophic forgetting.\" When you take a pre-trained model and fine-tune it on new, specialized information (like your company's internal knowledge base), it often degrades or completely forgets its original, general-purpose skills, such as the ability to follow instructions.</p><p>\\\nConsider an experiment to create an \"internal assistant.\" Researchers started with the Qwen3-8B model, which had a strong instruction-following score of 85%. After fine-tuning it on a 70-30 mix of internal company documents and general chat data:</p><ul><li>Its knowledge about the documents improved significantly (from 18% to 36% on a QA evaluation).</li><li>However, its instruction-following skill degraded badly, dropping from 85% down to 79%.</li></ul><p>\\\nThe solution was a brief phase of on-policy distillation after the initial fine-tuning. By using the original version of the model as the teacher, researchers could restore the lost behavior. The results were powerful:</p><ul><li>Instruction-following performance was almost fully recovered, jumping back up to 83%.</li><li>Crucially, this happened without losing the newly acquired knowledge. In fact, the knowledge score even improved slightly to 41%.</li></ul><p>\\\nThis finding is a game-changer for \"continual learning,\" aka the ability to update models with new information over time without having to perform expensive, full-scale retraining from scratch. It provides a reliable way to teach an AI new facts without it forgetting its core skills.</p><h3>An AI Can Master a Reasoning Skill From Just One Example</h3><p>This finding is highly counterintuitive. In most AI training methods, repeatedly training a model on the exact same prompt is a recipe for failure; the model simply memorizes the answer instead of learning the underlying skill.</p><p>\\\nHowever, an experiment with on-policy distillation turned this assumption on its head. Researchers trained a student model on a math reasoning task using only a single, randomly chosen prompt. They trained on this one prompt for 20 consecutive steps, each with a batch of 256 rollouts, generating 5,120 total learning sequences.</p><p>\\\nThe remarkable outcome turns conventional wisdom on its head: the student model was able to approximately match the performance of the expert teacher model on the AIME'24 math benchmark, despite only ever having seen that one problem.</p><p>\\\nThis works because on-policy distillation teaches the model to approximate the teacher's entire thought process; its full probability distribution for what the next best token should be at every step, rather than just memorizing a final answer. This means that for certain skills, the bottleneck isn't finding thousands of examples, but creating a single, perfectly-guided learning experience.</p><h3>Why \"Practicing\" on Its Own Samples Can Make an AI Dumber</h3><p>It seems logical that if a model produces a high-quality output, you could feed that output back into its training data to reinforce good behavior. This method, known as supervised fine-tuning (SFT) on on-policy data, is like having the model \"practice\" on its own best work.</p><p>\\\nBut researchers found the opposite to be true. When they trained a model using a dataset composed of its own samples, its performance on an instruction-following evaluation actually degraded.</p><p>\\\nThe technical reason for this failure is subtle but critical. While the dataset of the model's own outputs might be perfectly on-policy on average, every finite batch of data exhibits a slightly different distribution. Training on these batches causes the model’s internal policy to drift away from its original state. This process turns training on its own samples into a form of off-policy training over time, leading to the same compounding error and divergence seen in other flawed methods.</p><p>\\\nIn contrast, on-policy distillation is completely stable in this self-distillation scenario. Because the teacher model remains a fixed, consistent target, the student can robustly converge on the desired behavior without degrading. This further cements on-policy distillation as a superior and more reliable tool for behavior refinement and continual learning.</p><h3>The Future of AI is Smaller, Faster, and More Personal</h3><p>On-policy distillation is more than just another training technique; it's a foundational shift in how we create specialized, expert AI. By combining the direct relevance of learning from one's own actions with the incredible efficiency of dense, token-by-token feedback, it solves some of the biggest challenges in applied AI.</p><p>\\\nThe benefits are clear: massive compute savings, a cure for catastrophic forgetting, and unbelievable data efficiency. This is a key enabling technology that lowers the barrier to entry, unlocking the ability for more teams to build and maintain custom models that possess deep domain knowledge without sacrificing core capabilities. This democratization of expert AI will fuel new business models and create competitive advantages previously reserved for frontier labs.</p>","contentLength":8401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI researchers ’embodied’ an LLM into a robot – and it started channeling Robin Williams","url":"https://techcrunch.com/2025/11/01/ai-researchers-embodied-an-llm-into-a-robot-and-it-started-channeling-robin-williams/","date":1762009200,"author":"Julie Bort","guid":181,"unread":true,"content":"<article>AI researchers at Andon Labs embedded various LLMs in a vacuum robot to test how ready they were to be embodied. And hilarity ensued.</article>","contentLength":133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Kernel Ported To WebAssembly - Demo Lets You Run It In Your Web Browser","url":"https://www.phoronix.com/news/Linux-Kernel-WebAssembly","date":1762008052,"author":"Michael Larabel","guid":700,"unread":true,"content":"<article>Open-source developer Joel Severin today announced his work on porting the Linux kernel to WebAssembly and has successffully gotten the kernel up and running within WASM-capable web browsers...</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIT Physicists Find a Way To See Inside Atoms That May Aid Search For Antimatter","url":"https://science.slashdot.org/story/25/11/01/0545231/mit-physicists-find-a-way-to-see-inside-atoms-that-may-aid-search-for-antimatter?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762007640,"author":"EditorDavid","guid":249,"unread":true,"content":"\"Traditionally, exploring the interior of atomic nuclei requires enormous particle accelerators that stretch for kilometers and propel beams of electrons at extremely high speeds,\" writes SciTechDaily. \n\nBut MIT physicists have unveiled a groundbreaking alternative that \"used the atom's own electrons as probes to momentarily enter the nucleus...\"\n\n\n\nIn research published in Science, a team of MIT physicists achieved exceptionally precise measurements of the energy of electrons orbiting a radium atom that had been chemically bonded with a fluoride atom to form radium monofluoride. By studying these molecules, the researchers created a kind of miniature particle collider. Within this environment, the electrons surrounding the radium atom were confined closely enough to occasionally slip into the nucleus before returning to their usual orbits... When those electrons returned to their outer paths, they retained the altered energy, effectively carrying a \"message\" from within the nucleus that could be decoded to reveal its internal arrangement... \n\n[The researchers] trapped and cooled the molecules and sent them through a system of vacuum chambers, into which they also sent lasers, which interacted with the molecules. In this way, the researchers were able to precisely measure the energies of electrons inside each molecule. When the researchers analyzed their measurements, they noticed that the electrons carried slightly different energies than expected if they had remained outside the nucleus. The difference was incredibly small, only about one millionth of the energy of the laser photon used to excite the molecules, but it was clear evidence that the electrons had entered the radium nucleus and interacted with its protons and neutrons... \n\nThe researchers plan to use this new technique to create a detailed map of how forces are distributed inside the nucleus... to chart the nucleus with greater precision and search for possible violations of fundamental symmetries in nature. \n\n\"It is thought that additional sources of fundamental symmetry violation are required to explain the almost complete absence of antimatter in our universe,\" the article points out. \"Such violations could be seen within the nuclei of certain atoms such as radium... \n\n\n\"Unlike most atomic nuclei, which are spherical in shape, the radium atom's nucleus has a more asymmetrical configuration, similar to a pear. Scientists predict that this pear shape could significantly enhance their ability to sense the violation of fundamental symmetries, to the extent that they may be potentially observable.\"","contentLength":2606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Ledger of Code: Tracking the Carbon Debt Inside Our Software","url":"https://hackernoon.com/the-hidden-ledger-of-code-tracking-the-carbon-debt-inside-our-software?source=rss","date":1762005619,"author":"Jacob Wolinsky","guid":295,"unread":true,"content":"<ul><li>Every line of code carries an invisible cost. As software scales, so does the energy it consumes and the emissions it generates.</li><li>This growing footprint forms what many engineers now call carbon debt: the accumulation of energy waste caused by inefficient architecture, redundant compute, or neglected cleanup.</li><li>The problem isn’t limited to theory anymore. Global data workloads are rising faster than the efficiency gains meant to offset them, and few teams have the tools to measure what their systems actually emit.</li><li>Because engineers control how and where code runs, real progress starts inside development workflows, not in boardrooms.</li><li>As carbon visibility moves closer to the code itself, software projects may soon be judged not only by speed and stability, but by how responsibly they use the power behind them.</li></ul><p>Teams talk about technical debt every sprint. They track code smells, refactoring needs, module complexity, and build bloat. But almost no one tracks the energy drain built into their systems, and this makes that blind spot real.</p><p>\\\nEvery inefficiency in code, like extra loops, redundant database fetches, and idle background tasks, translates into power use. Run thousands or millions of times per day, and what feels trivial becomes measurable emissions. Researchers have begun quantifying this: for example, the <a href=\"https://arxiv.org/pdf/2007.07610\">Green Algorithms framework</a> shows that compute time, memory usage, and data center efficiency can be converted into carbon equivalent estimates for any computational task.</p><p>\\\nAt the data center scale, inefficiencies amplify. One white-paper found that servers may draw <a href=\"https://www.nrdc.org/sites/default/files/NRDC_WSP_Cloud_Computing_White_Paper.pdf\">60% to 90%</a> of their peak power even while idle. Multiply that across dozens of servers, and weeks of wasted cycles become dozens of kilograms of CO2 equivalent.</p><p>\\\nEvery product team now operates with an invisible balance sheet, one that records carbon alongside complexity.</p><p>The term carbon debt originates in environmental accounting, where it describes the <a href=\"https://globalclimateinitiatives.com/en/quest-ce-que-la-dette-carbone/\">accumulated emissions</a> a system or entity has “borrowed” against future budgets with insufficient offsets. (It’s rooted in the broader notion of ecological or climate debt.) Now, technologists are borrowing that phrase to describe software systems whose inefficiencies accrue hidden energy costs over time.</p><p>\\\nIn software, carbon debt grows when layers of redundant code, over-provisioned infrastructure, and heavy frameworks persist unchecked. A module that spawns unnecessary background jobs, or a service that overfetches data, burns CPU cycles, which burn power.</p><p>\\\nWhen infrastructure is sized with generous headroom “just in case,” that slack often stays underutilized, yet still draws baseline power. Servers and services often draw between <a href=\"https://gulfbusiness.com/why-idle-servers-not-ai-are-the-true-sustainability-threat/\">27% and 36%</a> of peak power even under light load.</p><p>\\\nAs your system advances with more users, more services, and more replicas, each inefficiency multiplies. What once was a single wasted cycle becomes thousands per second. That energy waste endures unless addressed, compounding like interest owed on an invisible balance.</p><p>\\\nNext, we’ll trace how code builds up emissions so you can see where the debt really comes from.</p><h2>How Code Accrues Emissions</h2><p>The energy footprint of software often hides in the smallest details of its logic. A loop that runs one step too long or a recursive function that never terminates efficiently can keep processors active far longer than needed. Each extra millisecond of compute draws power, and the effect multiplies when thousands of users trigger the same function at once.</p><h3>How Tiny Loops Turn Into Big Costs</h3><p>Research on mobile software shows that energy code smells can dramatically increase consumption, and in some cases, they can consume up to <a href=\"https://fpalomba.github.io/pdf/Journals/J13.pdf\">87x more energy</a> than clean versions. Follow-up work found that fixing these patterns delivered <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC11479295/\">4% to 30%</a> efficiency gains in practice. These results reinforce the broader point: repetitive, seemingly minor patterns accumulate real power draw over time.</p><p>\\\nSimilar waste appears in everyday engineering habits: redundant database queries, unnecessary front-end re-renders, and dormant API endpoints all keep processors active, drawing power without improving performance.</p><p>\\\nOver-sized build artifacts and idle background tasks deepen the impact by holding memory and storage resources active long after they’re useful. When these patterns run across millions of daily transactions, the emissions scale from grams to kilograms of CO2. Quantifying that footprint is the next challenge, and few teams yet have the tools to do it precisely.</p><h2>Measuring What We Don’t See</h2><p>Tracking how much energy software really uses is harder than it sounds. The <a href=\"https://sci-guide.greensoftware.foundation/\">Software Carbon Intensity (SCI)</a> framework from the Green Software Foundation is one of the first real attempts to make that measurable, like mapping compute time, memory use, and data transfer against actual energy data.</p><p>\\\nTools such as <a href=\"https://www.cloudcarbonfootprint.org/\">Cloud Carbon Footprint</a> and CodeCarbon are now taking that formula a step further, embedding energy estimates directly into build pipelines and dashboards so developers can see environmental impact alongside performance metrics. This aligns with broader conversations inside the DevOps community, where teams are beginning to explore practical ways to <a href=\"https://hackernoon.com/code-green-the-pragmatists-guide-to-eco-friendly-devops\">embed sustainability</a> into build and deployment workflows.</p><p>\\\nThe challenge is translating code execution into physical terms. Every watt drawn depends on processor type, cooling efficiency, and the carbon intensity of the grid that powers the data center. The same workload might have a fraction of the emissions on renewable-heavy infrastructure compared to fossil-fueled grids.</p><p>\\\nThe logic behind these tools isn’t far from how predictive analytics is being used to expose hidden operational costs in other industries, turning guesswork into measurable insight. Until this kind of visibility becomes standard in developer environments, most teams will keep optimizing performance while staying blind to the energy behind it.</p><h2>The Governance Gap: Why Carbon Isn’t Yet a Coding Metric</h2><p>Sustainability still sits outside most engineering workflows. In many companies, carbon reporting lives with facilities or operations teams, not with the people writing or deploying code.</p><p>\\\nAs a result, the energy cost of a release is rarely discussed in sprint planning or post-mortems. Agile ceremonies track velocity, story points, and error rates, but not emissions.</p><p>Few DevOps environments include “carbon sprints” or carbon budgets, even though they could be tracked the same way as uptime or latency. A <a href=\"https://greensoftware.foundation/articles/green-software-foundation-releases-first-ever-state-of-green-software-report\">report</a> based on responses from over <a href=\"https://stateof.greensoftware.foundation/en/methodology/\">2,000 software practitioners</a> has found that most organizations are still in the early stages of measuring software-related emissions. Others echoed this, noting that sustainability metrics remain largely absent from continuous-integration and delivery pipelines.</p><p>\\\nThat gap is beginning to close. Some open-source communities have started experimenting with “green commits” to tag energy-efficient changes, and enterprise dashboards are beginning to surface sustainability data next to performance KPIs. As this visibility improves, design priorities are shifting toward decay and restraint by building systems that know when to slow down, scale back, or shut off entirely.</p><h2>Designing for Decay: Making Efficiency a Default</h2><p>Architects concerned with long-lived systems often speak of <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/smr.2423\">architectural erosion</a> or design decay, like the gradual divergence between intended structure and runtime reality. Architecture erosion is a well-known risk in systems as features accumulate and shortcuts proliferate. One way to counter that drift is to build systems that self-optimize or sunset unused processes automatically, pruning inactive modules or trimming underutilized services based on real usage signals.</p><p>Treating code decay as a feature means embedding routines that perform periodic cleanup: archiving stale APIs, retiring dormant modules, or enforcing dependency hygiene. Frameworks may require that libraries unused for X releases be flagged or removed. Over time, the shift moves from “unlimited scaling” toward sustainable scaling, systems designed to shrink or sleep when load is low rather than running flat out forever.</p><p>\\\nEngineers can use runtime profiling, build monitoring, and garbage-collection heat maps as signals. If a microservice’s CPU utilization stays near zero for weeks, it raises a refactor or archive flag. If build artifacts grow without change, they are flagged for pruning.</p><p>\\\nThis philosophy sets the stage for what’s next: making carbon visibility part of everyday decision-making, and bringing engineering metrics and emissions metrics into the same ecosystem.</p><h2>The Road to Carbon Transparency</h2><p>Imagine an IDE where each file, function, or commit carries a live “emissions counter”; you write a loop, and you see how much energy it might cost. That’s the direction software tooling is heading. Build tools could come to flag carbon-heavy changes before they’re merged.</p><p>\\\n<a href=\"https://www.researchgate.net/publication/379003736_Carbon-Awareness_in_CICD\">CI/CD pipelines</a> will evolve to flag carbon-intensive builds, perhaps even rejecting code that spikes emissions far above baseline. With tighter integration, carbon metrics will merge with performance dashboards, showing build time, throughput, and CO2 cost in one pane.</p><h3>Cloud Dashboards &amp; Deployment Transparency</h3><p>Cloud providers may expose per-deployment carbon cost insights, mapping workload emissions to regions, instance types, and schedules. The same principle underpins the idea of <a href=\"https://hackernoon.com/carbon-aware-computing-next-green-breakthrough-or-new-greenwashing\">carbon-aware computing</a>, where workloads shift dynamically to regions or times with cleaner grids. Integrating that into the same console where devs monitor CPU, bandwidth, and billing makes sustainability part of everyday trade-offs.</p><p>\\\nWith visibility in place, engineers will begin to optimize not just for latency or memory, but for carbon as a first-class metric. Those insights will shape budgeting decisions, drive architecture choices (edge, serverless, off-peak scheduling), and enforce sustainable defaults in code.</p><p>\\\nAhead lies a time when your pull request comes with a carbon delta and teams judge changes not only by correctness or performance, but by how much energy they add or save.</p><h2>Engineering Accountability</h2><p>Sustainability in software doesn’t start in a server farm, but it starts at the keyboard. Every query, commit, and deployment decision shapes the energy profile of the systems we run. For years, efficiency meant speed and uptime, and now it also means restraint.</p><p>\\\nAcross the industry, teams are beginning to treat carbon debt the same way they treat technical debt: as something that compounds if ignored. Cleaning up unused code, right-sizing infrastructure, or pausing idle jobs are no longer side tasks; they’re acts of maintenance that protect performance and the planet.</p><p>\\\nAs tooling matures, carbon visibility will become part of normal governance, sitting next to reliability and security in every build report. The responsibility won’t rest with operations alone but with every engineer who touches code. Because in modern software, clean code and clean energy belong to the same conversation, and writing one well means caring about the other.</p>","contentLength":11106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go: Can It Mitigate Supply Chain Attacks?","url":"https://hackernoon.com/go-can-it-mitigate-supply-chain-attacks?source=rss","date":1762005611,"author":"Go [Technical Documentation]","guid":294,"unread":true,"content":"<p>Modern software engineering is collaborative, and based on reusing Open Source software. That exposes targets to supply chain attacks, where software projects are attacked by compromising their dependencies.</p><p>\\\nDespite any process or technical measure, every dependency is unavoidably a trust relationship. However, the Go tooling and design help mitigate risk at various stages.</p><p>There is no way for changes in the outside world—such as a new version of a dependency being published—to automatically affect a Go build.</p><p>\\\nUnlike most other package managers files, Go modules don’t have a separate list of constraints and a lock file pinning specific versions. The version of every dependency contributing to any Go build is fully determined by the  file of the main module.</p><p>\\\nSince Go 1.16, this determinism is enforced by default, and build commands (, , , , …) <a href=\"https://go.dev/ref/mod#go-mod-file-updates\">will fail if the go.mod is incomplete</a>. The only commands that will change the  (and therefore the build) are  and . These commands are not expected to be run automatically or in CI, so changes to dependency trees must be made deliberately and have the opportunity to go through code review.</p><p>\\\nThis is very important for security, because when a CI system or new machine runs , the checked-in source is the ultimate and complete source of truth for what will get built. There is no way for third parties to affect that.</p><p>\\\nMoreover, when a dependency is added with , its transitive dependencies are added at the version specified in the dependency’s  file, not at their latest versions, thanks to <a href=\"https://go.dev/ref/mod#minimal-version-selection\">Minimal version selection</a>. The same happens for invocations of <code>go install example.com/cmd/devtoolx@latest</code>, <a href=\"https://research.swtch.com/npm-colors\">the equivalents of which in some ecosystems bypass pinning</a>. In Go, the latest version of  will be fetched, but then all the dependencies will be set by its  file.</p><p>\\\nIf a module gets compromised and a new malicious version is published, no one will be affected until they explicitly update that dependency, providing the opportunity to review the changes and time for the ecosystem to detect the event.</p><h2>Version contents never change</h2><p>Another key property necessary to ensure third parties can’t affect builds is that the contents of a module version are immutable. If an attacker that compromises a dependency could re-upload an existing version, they could automatically compromise all projects that depend on it.</p><p>\\\nThat’s what the  file is for. It contains a list of cryptographic hashes of each dependency that contributes to the build. Again, an incomplete  causes an error, and only  and  will modify it, so any changes to it will accompany a deliberate dependency change. Other builds are guaranteed to have a full set of checksums.</p><p>\\\nThis is a common feature of most lock files. Go goes beyond it with the <a href=\"https://go.dev/ref/mod#checksum-database\">Checksum Database</a> (sumdb for short), a global append-only cryptographically-verifiable list of go.sum entries. When  needs to add an entry to the  file, it fetches it from the sumdb along with cryptographic proof of the sumdb integrity. This ensures that not only every build of a certain module uses the same dependency contents, but that every module out there uses the same dependency contents!</p><p>\\\nThe sumdb makes it impossible for compromised dependencies or even Google-operated Go infrastructure to target specific dependents with modified (e.g. backdoored) source. You’re guaranteed to be using the exact same code that everyone else who’s using e.g. v1.9.2 of  is using and has reviewed.</p><p>\\\nFinally, my favorite features of the sumdb: it doesn’t require any key management on the part of module authors, and it works seamlessly with the decentralized nature of Go modules.</p><h2>The VCS is the source of truth</h2><p>Most projects are developed through some version control system (VCS) and then, in other ecosystems, uploaded to the package repository. This means there are two accounts that could be compromised, the VCS host and the package repository, the latter of which is used more rarely and more likely to be overlooked. It also means it’s easier to hide malicious code in the version uploaded to the repository, especially if the source is routinely modified as part of the upload, for example to minimize it.</p><p>\\\nIn Go, there is no such thing as a package repository account. The import path of a package embeds the information that <a href=\"https://pkg.go.dev/cmd/go#hdr-Remote_import_paths\">needs in order to fetch its module</a> directly from the VCS, where tags define versions.</p><p>\\\nWe do have the <a href=\"https://go.dev/blog/module-mirror-launch\">Go Module Mirror</a>, but that’s only a proxy. Module authors don’t register an account and don’t upload versions to the proxy. The proxy uses the same logic that the  tool uses (in fact, the proxy runs ) to fetch and cache a version. Since the Checksum Database guarantees that there can be only one source tree for a given module version, everyone using the proxy will see the same result as everyone bypassing it and fetching directly from the VCS. (If the version is not available anymore in the VCS or if its contents changed, fetching directly will lead to an error, while fetching from the proxy might still work, improving availability and protecting the ecosystem from <a href=\"https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm\">“left-pad” issues</a>.)</p><p>\\\nRunning VCS tools on the client exposes a pretty large attack surface. That’s another place the Go Module Mirror helps: the  tool on the proxy runs inside a robust sandbox and is configured to support every VCS tool, while <a href=\"https://go.dev/ref/mod#vcs-govcs\">the default is to only support the two major VCS systems</a> (git and Mercurial). Anyone using the proxy can still fetch code published using off-by-default VCS systems, but attackers can’t reach that code in most installations.</p><h2>Building code doesn’t execute it</h2><p>It is an explicit security design goal of the Go toolchain that neither fetching nor building code will let that code execute, even if it is untrusted and malicious. This is different from most other ecosystems, many of which have first-class support for running code at package fetch time. These “post-install” hooks have been used in the past as the most convenient way to turn a compromised dependency into compromised developer machines, and to <a href=\"https://en.wikipedia.org/wiki/Computer_worm\">worm</a> through module authors.</p><p>\\\nTo be fair, if you’re fetching some code it’s often to execute it shortly afterwards, either as part of tests on a developer machine or as part of a binary in production, so lacking post-install hooks is only going to slow down attackers. (There is no security boundary within a build: any package that contributes to a build can define an  function.) However, it can be a meaningful risk mitigation, since you might be executing a binary or testing a package that only uses a subset of the module’s dependencies. For example, if you build and execute  on macOS there is no way for a Windows-only dependency or a dependency of <code>example.com/cmd/othertool</code> to compromise your machine.</p><p>\\\nIn Go, modules that don’t contribute code to a specific build have no security impact on it.</p><h2>“A little copying is better than a little dependency”</h2><p>The final and maybe most important software supply chain risk mitigation in the Go ecosystem is the least technical one: Go has a culture of rejecting large dependency trees, and of preferring a bit of copying to adding a new dependency. It goes all the way back to one of the Go proverbs: <a href=\"https://youtube.com/clip/UgkxWCEmMJFW0-TvSMzcMEAHZcpt2FsVXP65\">“a little copying is better than a little dependency”</a>. The label “zero dependencies” is proudly worn by high-quality reusable Go modules. If you find yourself in need of a library, you’re likely to find it will not cause you to take on a dependency on dozens of other modules by other authors and owners.</p><p>\\\nThat’s enabled also by the rich standard library and additional modules (the  ones), which provide commonly used high-level building blocks such as an HTTP stack, a TLS library, JSON encoding, etc.</p><p>\\\nAll together this means it’s possible to build rich, complex applications with just a handful of dependencies. No matter how good the tooling is, it can’t eliminate the risk involved in reusing code, so the strongest mitigation will always be a small dependency tree.</p><p>\\\n<em>This article is available on&nbsp;&nbsp;under a CC BY 4.0 DEED license.</em></p>","contentLength":8072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Archinstall 3.0.12 & Pacman 7.1 Released For Arch Linux Users","url":"https://www.phoronix.com/news/Archinstall-3.0.12-Pacman-7.1","date":1762002319,"author":"Michael Larabel","guid":699,"unread":true,"content":"<article>Kicking off November for Arch Linux users happen to be the releases of Pacman 7.1 as well as Archinstall 3.0.12...</article>","contentLength":114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chips Need to Chill Out","url":"https://spectrum.ieee.org/thermal-management-chips","date":1762002002,"author":"Harry Goldstein","guid":147,"unread":true,"content":"<p>The semiconductor industry seeks radical cooling solutions</p>","contentLength":58,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1NzcyNS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxMjg5Nzk0M30.oEWlOSimqjs-0YhG6yHcN93FhULhj37OplNQHCtEdkQ/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Samsung Building Facility With 50,000 Nvidia GPUs To Automate Chip Manufacturing","url":"https://hardware.slashdot.org/story/25/10/31/2352207/samsung-building-facility-with-50000-nvidia-gpus-to-automate-chip-manufacturing?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762002000,"author":"BeauHD","guid":248,"unread":true,"content":"An anonymous reader quotes a report from CNBC: Korean semiconductor giant Samsung said Thursday that it plans to buy and deploy a cluster of 50,000 Nvidia graphics processing units to improve its chip manufacturing for mobile devices and robots. The 50,000 Nvidia GPUs will be used to create a facility Samsung is calling an \"AI Megafactory.\" Samsung didn't provide details about when the facility would be built. It's the latest splashy partnership for Nvidia, whose chips remain essential for building and deploying advanced artificial intelligence. [...]\n \nOn Thursday, Nvidia representatives said they will work with Samsung to adapt the Korean company's chipmaking lithography platform to work with Nvidia's GPUs. That process will results in 20 times better performance for Samsung, the Nvidia representatives said. Samsung will also use Nvidia's simulation software called Omniverse. Known for its mobile phones, Samsung also said it would use the Nvidia chips to run its own AI models for its devices. In addition to being a partner and customer, Samsung is also a key supplier for Nvidia. Samsung makes the kind of high-performance memory Nvidia uses in large quantities, alongside its AI chips, called high bandwidth memory. Samsung said it will work with Nvidia to tweak its HBM4 memory for use in AI chips.","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PCI Resizable BAR Improvements Heading To Linux 6.19","url":"https://www.phoronix.com/news/PCI-ReBAR-Better-Linux-6.19","date":1762000551,"author":"Michael Larabel","guid":698,"unread":true,"content":"<article>Restructuring to the Linux kernel's PCI Resizable BAR \"ReBAR\" support is set to be submitted for the upcoming Linux 6.19 kernel cycle...</article>","contentLength":136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.18 Kernel Happenings, Python 3.14, NTFSPLUS & Other October Highlights","url":"https://www.phoronix.com/news/October-2025-Highlights","date":1761993411,"author":"Michael Larabel","guid":697,"unread":true,"content":"<article>During the month of October on Phoronix were 305 original news articles around Linux/open-source and another 21 featured Linux hardware reviews / multi-page featured benchmark articles. There was an exciting mix of software and hardware happenings over the past month. Here is a look back at what excited readers the most...</article>","contentLength":324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD Acknowledges RDSEED Failure On AMD Zen 5 With Software Fix Coming","url":"https://www.phoronix.com/news/AMD-SB-7055-RDSEED-Zen-5","date":1761992857,"author":"Michael Larabel","guid":696,"unread":true,"content":"<article>In mid-October a Meta engineer uncovered an RDSEED architectural issue with AMD Zen 5 CPUs. A patch in turn was sent out to the Linux kernel mailing list to disable RDSEED usage on affected Zen 5 processors. AMD this week issued a security bulletin to acknowledge the issue and report that a microcode fix is coming...</article>","contentLength":318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KDE Plasma 6.6 To Support Intel's Adaptive Sharpness Feature","url":"https://www.phoronix.com/news/Plasma-6.6-Adaptive-Sharpness","date":1761991744,"author":"Michael Larabel","guid":695,"unread":true,"content":"<article>KDE Plasma developers continue to be busy landing more fixes for the recently introduced Plasma 6.5 while also lining up more new features for Plasma 6.6...</article>","contentLength":156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Falling Panel Prices Lead To Global Solar Boom, Except For the US","url":"https://hardware.slashdot.org/story/25/10/31/2340238/falling-panel-prices-lead-to-global-solar-boom-except-for-the-us?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761991200,"author":"BeauHD","guid":247,"unread":true,"content":"Longtime Slashdot reader AmiMoJo shares a report from the Financial Times: Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses. But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. \"We will not approve wind or farmer destroying [sic] Solar,\" he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.\n \nThousands of miles away on the other side of the Pacific Ocean, it is a different story. China has laid solar panels across an area the size of Chicago high up on the Tibetan Plateau, where the thin air helps more sunlight get through. The Talatan Solar Park is part of China's push to double its solar and wind generation capacity over the coming decade. \"Green and low-carbon transition is the trend of our time,\" President Xi Jinping told delegates at a UN summit in New York last month. China's vast production of solar panels and batteries has also pushed down the prices of renewables hardware for everyone else, meaning it has \"become very difficult to make any other choice in some places,\" according to Heymi Bahar, senior analyst at the International Energy Agency. [...]\n \nMore broadly, the US's focus on fossil fuels and pullback of support for clean energy further cedes influence over the future global energy system to China. The US is trying to tie its trading partners into fossil fuels, pressing the EU to buy $750 billion of American oil, natural gas, and nuclear technologies during his presidency as part of a trade deal, scuppering an initiative to begin decarbonizing world shipping and pressuring others to reduce their reliance on Chinese technology. But the collapsing cost of solar panels in particular has spoken for itself in many parts of the world. Experts caution that the US's attacks on renewables could cause lasting damage to its competitiveness against China, even if an administration more favorable to renewables were to follow Trump's.","contentLength":2298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SpaceX Set To Win $2 Billion Pentagon Satellite Deal","url":"https://tech.slashdot.org/story/25/10/31/2347207/spacex-set-to-win-2-billion-pentagon-satellite-deal?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761980400,"author":"BeauHD","guid":246,"unread":true,"content":"According to the Wall Street Journal, SpaceX is reportedly poised to secure a $2 billion Pentagon contract to develop hundreds of missile-tracking satellites for President Trump's ambitious Golden Dome defense system. The Independent reports: The planned \"air moving target indicator\" system in question could ultimately feature as many as 600 satellites once it is fully operational, The Wall Street Journal reports. Musk's company has also been linked to two more satellite ventures, which are concerned with relaying sensitive communications and tracing vehicles, respectively.\n \nGolden Dome, inspired by Israel's \"Iron Dome,\" was announced by Trump and Secretary of War Pete Hegseth at the White House in May and will amount to a complex system of satellites and weaponry capable of destroying incoming missiles before they hit American targets. The president promised it would be \"fully operational\" before he leaves office in January 2029, capable of intercepting rockets, \"even if they are launched from space,\" with an overall price tag of $175 billion.","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: From Cloud to Desk: 3 Signs the AI Revolution is Going Local (11/1/2025)","url":"https://hackernoon.com/11-1-2025-techbeat?source=rss","date":1761977453,"author":"Techbeat","guid":293,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/hacker-Antho\">@hacker-Antho</a> [ 4 Min read ] \n New research shatters AI security assumptions, showing that poisoning large models is easier than believed and requires a very small number of documents. <a href=\"https://hackernoon.com/the-illusion-of-scale-why-llms-are-vulnerable-to-data-poisoning-regardless-of-size\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/socialdiscoverygroup\">@socialdiscoverygroup</a> [ 6 Min read ] \n Discover how React 19's new hooks—useActionState, useFormStatus, and useOptimistic—simplify form handling with less boilerplate and cleaner code.  <a href=\"https://hackernoon.com/react-19-new-tools-to-work-with-forms\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mayukhsuri\">@mayukhsuri</a> [ 3 Min read ] \n AWS outage on Oct 20, 2025, disrupted major apps worldwide. Learn what caused it, how it spread, and key lessons to build stronger cloud systems. <a href=\"https://hackernoon.com/aws-outage-2025-what-really-happened-on-october-20-and-what-it-teaches-us-about-the-cloud\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/filestack\">@filestack</a> [ 6 Min read ] \n Stop babysitting profile pictures. Learn how Filestack Workflows turn image uploads into scalable, async, and lightning-fast experiences. <a href=\"https://hackernoon.com/how-to-fix-profile-image-upload-headaches-with-filestack-workflows\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/nownodes\">@nownodes</a> [ 4 Min read ] \n Blast API ends operations in Oct 2025. Explore the best developer alternatives like NOWNodes and Alchemy for secure, scalable RPC migration. <a href=\"https://hackernoon.com/blast-api-shutdown-the-best-alternatives-for-developers\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mend\">@mend</a> [ 4 Min read ] \n Traditional testing breaks with AI. Learn how red teaming and AI-powered fuzzing uncover hidden weaknesses in large language models. <a href=\"https://hackernoon.com/why-traditional-testing-breaks-down-with-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/knightbat2040\">@knightbat2040</a> [ 5 Min read ] \n What started as a simple script evolved into a full-fledged data engineering and NLP pipeline that can process a decade's worth of legal decisions in minutes. <a href=\"https://hackernoon.com/python-script-to-read-and-judge-1500-legal-cases\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackmarketing\">@hackmarketing</a> [ 7 Min read ] \n Learn how Web3 projects can grow sustainably through education, trust, and human-centered marketing that builds real users and community. <a href=\"https://hackernoon.com/the-future-of-web3-marketing-education-trust-and-sustainability\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/botbeat\">@botbeat</a> [ 8 Min read ] \n A deep dive into the 30 companies that burned over one trillion OpenAI tokens—featuring Duolingo, OpenRouter, and Indeed as top power users of GPT tech. <a href=\"https://hackernoon.com/whos-used-one-trillion-plus-openai-tokens-salesforce-shopify-canva-hubspot-and-26-more-companies\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/melvin-manni\">@melvin-manni</a> [ 5 Min read ] \n Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  <a href=\"https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/giovannicoletta\">@giovannicoletta</a> [ 11 Min read ] \n An interrogation of how physics concepts like black holes, entropy, and quantum theory mirror the rise and limits of artificial intelligence. <a href=\"https://hackernoon.com/the-physics-of-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ichebykin\">@ichebykin</a> [ 5 Min read ] \n Context engineering for coding agents is the best way to improve the model performance for code generation.  <a href=\"https://hackernoon.com/context-engineering-for-coding-agents\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mcsee\">@mcsee</a> [ 3 Min read ] \n Avoid Boolean variables, they lead to conditional logic and force you to write Ifs. Create polymorphic states instead <a href=\"https://hackernoon.com/code-smell-07-avoid-boolean-variables\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/sanjaybarot\">@sanjaybarot</a> [ 23 Min read ] \n Ransomware has gone cloud-native: no payloads, just API abuse. Learn the tactics—IAM takeovers, KMS locks, backup sabotage—and how to build resilience. <a href=\"https://hackernoon.com/ransomware-goes-cloud-native\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ainativedev\">@ainativedev</a> [ 4 Min read ] \n GitHub Copilot evolves: cloud-based agents now handle PRs, iterate from feedback, and fit seamlessly into dev workflows. <a href=\"https://hackernoon.com/githubs-copilot-adds-cloud-agent-to-draft-pull-requests-autonomously\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/aifundingtracker\">@aifundingtracker</a> [ 13 Min read ] \n AI startups raised over $3.6 billion this week across infrastructure, wearable AI, enterprise automation, and fintech innovation. <a href=\"https://hackernoon.com/weekly-ai-startup-funding-october-20-25-2025\">Read More.</a></p>","contentLength":2886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Numbers Show Xbox's Current Plan Isn't Working","url":"https://games.slashdot.org/story/25/10/31/2332211/the-numbers-show-xboxs-current-plan-isnt-working?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761967800,"author":"BeauHD","guid":245,"unread":true,"content":"An anonymous reader quotes a report from Gizmodo: It's time for Xbox to eat some humble pie and perform some real soul-searching. Microsoft released its latest quarterly earnings report and proved the worst of our fears about its gaming brand. Not only are Xbox hardware sales down significantly, but the brand itself is barely treading water. Gamers are voicing their displeasure with their wallets, but Microsoft's top brass is still only thinking about the margins. Microsoft was more keen to promote the scale of its cloud and AI services revenue -- which was up 28% year over year -- than talk about its beleaguered gaming brand. The company's overall gaming revenue fell by 2% compared to the same time last year. This was precipitated by a \"decline in Xbox hardware,\" which was down by 22% following a steady decline quarter after quarter. Its first-party games and its Game Pass subscription were doing better, though the overall growth was only up by 1%, and even that was driven by the \"better-than-expected performance\" of third-party games. You can give credit to titles like Clair Obscur: Expedition 33 for why Xbox isn't in an even deeper hole than it is now.\n \nThe tech giant has no expectation that its Xbox brand will start making more money anytime soon. In its earnings call with investors, Microsoft Chief Financial Officer Amy Hood said the company expects Xbox will continue to decline \"in the low to mid-single digits\" for the following quarter. That's mostly due to the lack of landmark first-party titles. Just this month, Xbox released Ninja Gaiden 4, The Outer Worlds 2, and Double Fine's The Keeper. Xbox also made a huge marketing push for its first handheld, made in partnership with Asus, the ROG Xbox Ally and Ally X. In any other year, this would be a big month for any gaming company. The dour outlook comes after months of bad news. After two subsequent price hikes, Xbox Series S and Series X consoles now cost between $100 to $150 more than they did at launch five years ago. Microsoft also pushed prices of its Game Pass Ultimate subscription tier from $20 to $30 per month. A full-year's subscription would now demand $360. In a separate article, Gizmodo reviews Microsoft's new ROG Xbox Ally X handheld, which \"offers a better experience overall\" than the \"other small-scale Windows PC gaming devices released this year.\" However, \"it's still nowhere close to what you truly want from a console.\"","contentLength":2436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI Launches Aardvark To Detect and Patch Hidden Bugs In Code","url":"https://it.slashdot.org/story/25/10/31/2314223/openai-launches-aardvark-to-detect-and-patch-hidden-bugs-in-code?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761963000,"author":"BeauHD","guid":244,"unread":true,"content":"OpenAI has introduced Aardvark, a GPT-5-powered autonomous agent that scans, reasons about, and patches code like a human security researcher. \"By embedding itself directly into the development pipeline, Aardvark aims to turn security from a post-development concern into a continuous safeguard that evolves with the software itself,\" reports InfoWorld. From the report: What makes Aardvark unique, OpenAI noted, is its combination of reasoning, automation, and verification. Rather than simply highlighting potential vulnerabilities, the agent promises multi-stage analysis -- starting by mapping an entire repository and building a contextual threat model around it. From there, it continuously monitors new commits, checking whether each change introduces risk or violates existing security patterns.\n \nAdditionally, upon identifying a potential issue, Aardvark attempts to validate the exploitability of the finding in a sandboxed environment before flagging it. This validation step could prove transformative. Traditional static analysis tools often overwhelm developers with false alarms -- issues that may look risky but aren't truly exploitable. \"The biggest advantage is that it will reduce false positives significantly,\" noted Jain. \"It's helpful in open source codes and as part of the development pipeline.\"\n \nOnce a vulnerability is confirmed, Aardvark integrates with Codex to propose a patch, then re-analyzes the fix to ensure it doesn't introduce new problems. OpenAI claims that in benchmark tests, the system identified 92 percent of known and synthetically introduced vulnerabilities across test repositories, a promising indication that AI may soon shoulder part of the burden of modern code auditing.","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FCC To Rescind Ruling That Said ISPs Are Required To Secure Their Networks","url":"https://it.slashdot.org/story/25/10/31/237241/fcc-to-rescind-ruling-that-said-isps-are-required-to-secure-their-networks?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761960600,"author":"BeauHD","guid":243,"unread":true,"content":"The FCC plans to repeal a Biden-era ruling that required ISPs to secure their networks under the Communications Assistance for Law Enforcement Act, instead relying on voluntary cybersecurity commitments from telecom providers. FCC Chairman Brendan Carr said the ruling \"exceeded the agency's authority and did not present an effective or agile response to the relevant cybersecurity threats.\" Carr said the vote scheduled for November 20 comes after \"extensive FCC engagement with carriers\" who have taken \"substantial steps... to strengthen their cybersecurity defenses.\" Ars Technica reports: The FCC's January 2025 declaratory ruling came in response to attacks by China, including the Salt Typhoon infiltration of major telecom providers such as Verizon and AT&amp;T. The Biden-era FCC found that the Communications Assistance for Law Enforcement Act (CALEA), a 1994 law, \"affirmatively requires telecommunications carriers to secure their networks from unlawful access or interception of communications.\"\n \n\"The Commission has previously found that section 105 of CALEA creates an affirmative obligation for a telecommunications carrier to avoid the risk that suppliers of untrusted equipment will \"illegally activate interceptions or other forms of surveillance within the carrier's switching premises without its knowledge,'\" the January order said. \"With this Declaratory Ruling, we clarify that telecommunications carriers' duties under section 105 of CALEA extend not only to the equipment they choose to use in their networks, but also to how they manage their networks.\" A draft of the order that will be voted on in November can be found here (PDF).","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is Bending Spoons? Everything to know about AOL’s acquirer","url":"https://techcrunch.com/2025/10/31/what-is-bending-spoons-everything-to-know-about-aols-acquirer/","date":1761959494,"author":"Anna Heim","guid":180,"unread":true,"content":"<article>Bending Spoons remains largely unknown, even as its portfolio of products has served more than a billion people. </article>","contentLength":113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wine 10.18 Released With More WoW64 Mode Improvements","url":"https://www.phoronix.com/news/Wine-10.18-Released","date":1761957308,"author":"Michael Larabel","guid":694,"unread":true,"content":"<article>Wine 10.18 is now available for capping off the month of October and working toward the code freeze for Wine 11.0 beginning in early December...</article>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How L.A. Scores “Vulnerability” of Unhoused People Is Changing: What You Need to Know","url":"https://hackernoon.com/how-la-scores-vulnerability-of-unhoused-people-is-changing-what-you-need-to-know?source=rss","date":1761956616,"author":"The Markup","guid":292,"unread":true,"content":"<p><em>Welcome to The Markup, where we use investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up for</em><em><a href=\"https://themarkup.org/newsletter/klaxon\">Klaxon</a>, a newsletter that delivers our stories and tools directly to your inbox.</em></p><p>\\\nOne year after <a href=\"https://themarkup.org/investigation/2023/02/28/l-a-s-scoring-system-for-subsidized-housing-gives-black-and-latino-people-experiencing-homelessness-lower-priority-scores\">a Markup investigation</a> revealed racial bias in Los Angeles’s housing intake system for people experiencing homelessness, local politicians have pressed for reforms and the agency responsible for housing is taking steps to make its approach more equitable and effective.</p><p>\\\nShortly after our original investigation published, Los Angeles City Council Member Nithya Raman, who chairs the Housing and Homelessness committee, introduced <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_misc_03-10-23.pdf\">a motion</a> citing the article and <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_rpt_hh_03-15-23.pdf\">calling on</a> the Los Angeles Homeless Services Authority (LAHSA) to come up with a plan to reform its intake system. The legislation, approved <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_CAF_3-24-23.pdf\">unanimously</a>, called specifically for greater fairness in the “vulnerability” scoring system that The Markup analyzed. Used by Los Angeles for the past decade, the system <a href=\"https://themarkup.org/investigation/2023/02/28/l-a-s-scoring-system-for-subsidized-housing-gives-black-and-latino-people-experiencing-homelessness-lower-priority-scores\">rated Black people as significantly less vulnerable</a> than White people year after year, making them less likely to obtain subsidized permanent housing.</p><p>\\\nBlack people are hugely overrepresented among unhoused people in L.A., making up about 9 percent of Los Angeles County’s population but about 30 percent of the county’s people experiencing homelessness.</p><p>\\\n“To see that the tool that we’re using to put people in line for housing was not actually housing unhoused Black Angelenos as quickly as we could was really surprising to me,” said Raman, who read the article in the Los Angeles Times, where it was <a href=\"https://www.latimes.com/california/story/2023-02-28/black-latino-homeless-people-housing-priority-list-los-angeles\">co-published</a>. Raman, who is currently running for re-election in District Four, in central LA, said the investigation “absolutely” spurred the council to act.</p><p>\\\nLAHSA, given a deadline of April 2023 in the legislation, still has not provided a reform plan. A spokesperson for the agency didn’t directly respond to a request for comment about the plan.</p><p>\\\nRaman said LAHSA has taken some steps in the past year to improve how it allocates housing. Among other changes, she said, the agency has started to prioritize some groups, including those already involved in housing programs and those who already have the documents required to move into a building, like an ID and social security number.</p><p>\\\nMeanwhile, the agency <a href=\"https://www.lahsa.org/news?article=936-changes-to-ces-psh-prioritization-and-matching\">also de-emphasized</a> the score’s importance in placing people for permanent housing. People applying for housing are scored on a 17-point scale. Previously, the people with the highest scores were given the highest priority, but now any person who scores an eight or above can be prioritized, depending on the other factors being considered.</p><p>\\\nStill, equity in the housing system remains a known problem. In November, researchers from the University of Southern California and the University of California Los Angeles, working in partnership with LAHSA, released <a href=\"https://cais.usc.edu/wp-content/uploads/2023/11/CESTTRR-Final-Report-2023.pdf\">a long-awaited study</a> on racial bias in the system and ways to reform the scoring system, known as the VI-SPDAT.</p><p>\\\nThe study, which analyzed scores across race and ethnicity, tracked with The Markup’s findings from earlier in the year, concluding that the scoring tool is biased toward White people and that it’s ineffective overall. The study, in some respects, went even further. Using data on who ultimately faced an “adverse” event, like jail or death, the researchers concluded that tool was “not much more accurate than a random guess at predicting vulnerability.”</p><p>\\\nThe study suggested several ways the scoring system could become more accurate and equitable, some of which matched The Markup’s reporting. The scoring system asks intensely personal questions about a person’s life, including around issues like violence and substance abuse, and the report <a href=\"https://cais.usc.edu/wp-content/uploads/2023/11/CESTTRR-Final-Report-2023.pdf#page=89\">recommends rewording and reframing</a> questions to make the survey less complex and more sensitive. A revised version of the system with new questions and scoring could substantially reduce bias, the researchers conclude.</p><p>\\\nFor example, the study suggests that the existing question about whether anyone has “forced you or tricked you to do things that you do not want to do” should be amended to stress that answering yes “will not result in punishment or any negative consequences.” Another question currently asks, “Are there any medications like painkillers that you don’t take the way the doctor prescribed or where you sell the medication?” The study suggested softening it to, “Do you have medication that you choose to sell instead of taking to help support yourself financially? Answering yes to this question will not result in punishment or negative consequences for you.” Several questions were suggested for removal entirely.</p><p>\\\nIn a written statement, LAHSA spokesperson Christopher Yee acknowledged that it’s long been clear that “the VI-SPDAT has shortcomings related to equity,” adding that the survey is “long, cumbersome, and not trauma-informed in the content of the questions or administration process.”</p><p>\\\nYee highlighted the study on recommended changes to the system and said the agency is “working with key partners and stakeholders to create a plan to implement and refine” a new iteration of the scoring system while it continues to use the old version.</p><p>\\\nThe agency, he noted, has already dropped a requirement to score people for interim housing entry or time-limited subsidy programs, but will still require scoring for permanent housing. LAHSA must use some sort of prioritization system to access certain federal housing funds under <a href=\"https://www.hud.gov/sites/documents/17-01CPDN.PDF\">rules</a> established by the U.S. Department of Housing and Urban development.</p><p>\\\nThe planned changes to the scoring system will first apply to screening for adults, and later the agency plans to explore changes to related tools for young people and families with children. The Markup found that racial disparities were even more stark for a variation of the VI-SPDAT used in Los Angeles for people under the age of 25.</p><p>\\\nYee’s statement did not provide a timeline for the revised tool’s launch, but in <a href=\"https://www.lahsa.org/documents?id=7737-cesttrr-report-faq.pdf\">a FAQ</a> released alongside the study LAHSA said service providers could expect more information early this year on changes to the intake process, known as the Coordinated Entry System.</p><p>\\\nRaman, for her part, said she’s withholding judgment until data can show how those changes affect who is housed. But, she said, “there’s no question in my mind that CES needs reform.”</p>","contentLength":6475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Most Anticipated BNB Launch of 2025: $BALZ Brings The Meme Migration Home","url":"https://hackernoon.com/the-most-anticipated-bnb-launch-of-2025-$balz-brings-the-meme-migration-home?source=rss","date":1761951173,"author":"Chainwire","guid":291,"unread":true,"content":"<p>Singapore, Singapore, October 31st, 2025/Chainwire/--The Binance Smart Chain (BNB) network has seen renewed activity, and BALZ has emerged as one of its notable community movements, with over 40,000 active members before launch on X (@). Observers regard it as one of the more anticipated community-driven launches of the year, comparable to projects such as Aster and Four.meme.</p><p>Raising over $2 million within days of opening, BALZ has positioned itself as a significant project developing on BNB, despite its informal branding and memetic culture. </p><p>With more than 40,000 members prior to its anticipated token presale, the project has adopted an unconventional approach to community growth through guerrilla marketing and its \"rug pull recovery protocol.\"</p><p>Instead of allocating capital to influencer campaigns, the team integrated communities from Solana and Base, migrating them to BNB through its protocol. At the time of writing, more than 10,000 verified holders are in the process of migration.</p><h3>The Token Presale: Closing Tonight, October 31st at 23:59 PDT</h3><p>At the center of BALZ is the Fair-As-F* Launch (FAF), a limited-time token presale closing on October 31 at 23:59 PDT. Within days of opening, BALZ raised over $2 million, drawing parallels to earlier community-led launches such as Shiba Inu and Floki in 2020.</p><p>FAF is structured with a fixed price and specific time frame, allowing equal participation without insider advantages or automated trading. In a market that has frequently favored early access and automation, BALZ seeks to show that fairness can be built into its design.</p><h3>BNB Market Conditions and Timing</h3><p>The timing aligns with a significant shift in the cryptocurrency market. On October 10, 2025, the sector experienced its largest liquidation event to date, with $19 billion eliminated within 48 hours as Bitcoin declined from $126,000 to $105,000. This event represented market deleveraging rather than capitulation.</p><ul><li>Open interest decreased from $48.7 billion to $45.1 billion</li><li>Funding rates fell by 51 percent</li><li>Overleveraged positions were cleared</li></ul><p>The result is a market now characterized by conviction-based participants and institutional capital seeking new deployment opportunities.</p><p>Market structure mirrors 2020-2021 exactly:</p><ul><li>Bitcoin ETFs pulled in $2.71 billion during October 6-10, BlackRock's IBIT holding $65.26 billion</li><li>85% of institutional firms now allocate to digital assets</li><li>Fed rate cuts hit 93% probability for next quarter</li></ul><p>BNB Smart Chain Shows Continued Growth</p><ul><li>3.62 million daily active addresses in October 2025</li><li>Total Value Locked surged 217% to $17.1 billion</li><li>70% of BNB meme traders are currently profitable</li></ul><p>CZ is back. He changed his X profile from \"ex-@binance\" to \"@binance\" in September 2025. BNB hit an all-time high of $1,311. Real infrastructure that actually supports growth. BNB is where smart money is rotating.</p><p>BALZ is capturing this momentum at the exact moment Solana and Base communities are looking for an exit. Market observers note the project is one CZ tweet away from a billion-dollar market cap, similar to previous meme token cycles where single endorsements rapidly accelerated valuations into nine-figure territory.</p><p>The presale window closes October 31st at 23:59 PDT.</p><p>Follow: X: @ | Telegram: t.me/BALZ_Official</p><p> is a meme coin launching on Binance Smart Chain with a mission: to build the safest, fastest trading platform and no-code launchpad in crypto. Led by a doxxed team and powered by 40,000+ active members.</p><p>:::tip\n<em>This story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":3623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aster’s Rocket Launch Surpasses $1B in Trading Volume, as Nubila Joins with Over 6 Million $NB","url":"https://hackernoon.com/asters-rocket-launch-surpasses-$1b-in-trading-volume-as-nubila-joins-with-over-6-million-$nb?source=rss","date":1761950749,"author":"Chainwire","guid":290,"unread":true,"content":"<p>George Town, British Virgin Islands, October 31st, 2025/Chainwire/--, the decentralized trading platform, has generated strong momentum with its innovative product .</p><p>In the first six days following the debut of Rocket Launch, Aster recorded approximately $122 million in spot trading volume and $933 million in perpetual trading volume. Within five days after APRO’s $AT token TGE, Aster captured over 90% of the market share in $AT perpetual trading, underscoring Rocket Launch’s significant contribution to overall market activity.</p><p>Since its debut on October 24, Rocket Launch has meaningfully increased both user activity and engagement on the platform. On October 29, Aster announced a 500,000 $AT Loyalty Bonus distributed to early participants who traded within the first four days of the campaign.</p><p>The platform also disclosed that the spot trading competition features a reward pool of no less than 1.5 million $AT, followed by a perpetual trading campaign with at least 1.5 million $AT in additional rewards, marking a continuation of strong user engagement across both markets.</p><p>The first Rocket Launch event not only accelerated new user acquisition but also reactivated existing traders and token holders, significantly enhancing overall liquidity and engagement across the Aster ecosystem. This milestone demonstrates Rocket Launch’s strong driving force and long-term potential in shaping the growth of the Aster DeFi landscape.</p><p>Next Rocket Launch: Nubila Debuts, Powering the Physical Oracle Layer for AI and Prediction Markets</p><p>Aster announced that the next Rocket Launch will begin on October 31, 2025, at 12:00 UTC, featuring , a decentralized oracle network for AI and prediction markets. The seven-day campaign will include both spot and perpetual trading campaigns for Nubila ($NB).</p><p>The event adopts a dual reward structure. The Spot campaign offers a $200,000 $ASTER prize pool alongside over 3 million $NB in rewards, while the Perpetual campaign features an exclusive pool exceeding 3 million $NB, aimed at fostering broader participation and sustained market activity.</p><p>Continuing its long-term vision, Aster is redefining the evolution of token launches through Rocket Launch, transforming what used to be a single market event into a continuous, growth-oriented journey.</p><p>Each Rocket Launch campaign is structured to create a self-reinforcing value loop. The reward pool combines $ASTER and the project’s native tokens. Project teams contribute both capital and tokens, while Aster allocates those funds to buy back $ASTER from the open market.</p><p>The repurchased $ASTER, together with the project tokens, are then distributed as rewards to participants, ensuring that users benefit directly from both trading activity and ecosystem growth.</p><blockquote><p>“Aster’s Rocket Launch is more than a trading campaign; it’s an engine for on-chain innovation,” said Leonard, CEO of Aster. “Every participant becomes part of the ecosystem, contributing to the process of value creation for emerging projects.”</p></blockquote><p> is building the physical oracle layer for AI and prediction markets. Its decentralized sensor network captures real-world data and transforms it into verifiable intelligence for AI systems and smart contracts.</p><p>Backed by BCG, Block Space Force, Quantum Holdings, VeChain, and IoTeX, Nubila has deployed 21,000+ devices across 122 countries and 16,000+ validator nodes, powering the next wave of AI agents and decentralized applications with real, trustworthy physical data.</p><p> is a next-generation decentralized exchange offering both Perpetual and Spot trading, designed as a one-stop onchain venue for global crypto traders. It features MEV-free, one-click execution in 1001x Mode. Perpetual Mode adds 24/7 stock Perpetuals, Hidden Orders, and grid trading, available across BNB Chain, Ethereum, Solana, and Arbitrum.</p><p>Its unique edge lies in the ability to use liquid-staking tokens (asBNB) or yield-generating stablecoins (USDF) as collateral, unlocking unparalleled capital efficiency. Backed by YZi Labs, Aster is building the future of DeFi: fast, flexible, and community-first.</p><p>:::tip\n<em>This story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":4256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GNOME Gains A New macOS-Inspired Quick Menu Option","url":"https://www.phoronix.com/news/GNOME-Kiwi-macOS-Quick-Menu","date":1761946882,"author":"Michael Larabel","guid":693,"unread":true,"content":"<article>For GNOME desktop users desiring a more macOS-like experience, a new GNOME extension provides a macOS-inspired quick menu option...</article>","contentLength":131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two Windows vulnerabilities, one a 0-day, are under active exploitation","url":"https://arstechnica.com/security/2025/10/two-windows-vulnerabilities-one-a-0-day-are-under-active-exploitation/","date":1761944636,"author":"Dan Goodin","guid":329,"unread":true,"content":"<p>Two Windows vulnerabilities—one a zero-day that has been known to attackers since 2017 and the other a critical flaw that Microsoft initially tried and failed to patch recently—are under active exploitation in widespread attacks targeting a swath of the Internet, researchers say.</p><p>The zero-day&nbsp;went undiscovered until <a href=\"https://www.trendmicro.com/en_us/research/25/c/windows-shortcut-zero-day-exploit.html\">March</a>, when security firm Trend Micro said it had been under active exploitation since 2017, by as many as 11 separate advanced persistent threats (APTs). These APT groups, often with ties to nation-states, relentlessly attack specific individuals or groups of interest. Trend Micro went on to say that the groups were exploiting the vulnerability, then tracked as ZDI-CAN-25373, to install various known post-exploitation payloads on infrastructure located in nearly 60 countries, with the US, Canada, Russia, and Korea being the most common.</p><h2>A large-scale, coordinated operation</h2><p>Seven months later, Microsoft still hasn’t patched the vulnerability, which stems from a bug in the <a href=\"https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-shllink/16cb4ca1-9339-4d0c-a68d-bf1d6cc0f943\">Windows Shortcut</a> binary format. The Windows component makes opening apps or accessing files easier and faster by allowing a single binary file to invoke them without having to navigate to their locations. In recent months, the ZDI-CAN-25373 tracking designation has been changed to CVE-2025-9491.</p>","contentLength":1299,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2022/10/windows-malware-1024x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"Bluesky hits 40 million users, introduces ‘dislikes’ beta","url":"https://techcrunch.com/2025/10/31/bluesky-hits-40-million-users-introduces-dislikes-beta/","date":1761941646,"author":"Sarah Perez","guid":179,"unread":true,"content":"<article>As users \"dislike\" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.</article>","contentLength":191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta bought 1 GW of solar this week","url":"https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/","date":1761938790,"author":"Tim De Chant","guid":178,"unread":true,"content":"<article>The social media company inked three deals in the U.S. to power its data centers and offset its carbon footprint.</article>","contentLength":113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI mania tanks CoreWeave’s Core Scientific acquisition — it buys Python notebook Marimo","url":"https://techcrunch.com/2025/10/31/ai-mania-tanks-coreweaves-core-scientific-acquisition-it-buys-python-notebook-marimo/","date":1761936828,"author":"Julie Bort","guid":177,"unread":true,"content":"<article>CoreWeave's failed buy of Core Scientific is another sign of an AI bubble. But it's still shopping.</article>","contentLength":99,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holography in Cuprates: Critical Review of Quantitative Claims","url":"https://hackernoon.com/holography-in-cuprates-critical-review-of-quantitative-claims?source=rss","date":1761932719,"author":"The Tech Reckoning is Upon Us!","guid":289,"unread":true,"content":"<p>The theories of both, finite- and zero-density, spinons have been extensively discussed in the context of the ’strange metal’ phase in the underdoped cuprates and other (arguably, even stranger) heavy-fermion compounds long before the advent of holography [3]. Once there, the applied holography quickly joined the quest into the properties of this phase that had long evaded a consistent and satisfactory explanation.</p><p>\\\nInstead of going after the NFL fermion propagator, however, many of the holographic proposals focused on reproducing the experimental data in the cuprates - and often times even claimed achieving a quantitative agreement.</p><p>\\\nIn light of its intrinsically unsettled status one would have thought that it might be rather detrimental for any speculative approach to seek out not a mere qualitative but an actual quantitative, down to the number, agreement between its specific predictions and some preselected sets of experimental data. In fact, if such a quantitative agreement were indeed achieved one would have even more explaining to do (first and foremost, as to why an apriori approximate approach appears to be so unexpectedly accurate?).</p><p>\\\nThe earlier discussion of some of the popular evidence in support of condensed matter holography as well as the debunking of a number of its specific predictions [26] can be found in [34]. However, the admirable persistence with which those predictions continued to be regularly cited in the subsequent holographic literature [35] suggests that the comments of [34] might have had been (most regretfully) overlooked.</p><p>\\\nIn fact, there is more than a single reason for which semiclassical holography (or its improvement at the level of accounting for the matter back-reaction in the HartreeFock approximation) - thus far, the only practical way of performing the holographic calculations [26–29] - would not have been expected to provide any quantitatively accurate results in the first place. There are, of course, such obvious differences from the string-theoretical holographic constructions as a low physical value of N (which, in practice, often amounts to ’spin up/down’) and the lack of Lorentz, translational, and/or rotational (as well as any super-)symmetries.</p><p>\\\nArguably, though, the most important is the fact that much of the condensed matter physics operates in the intermediate - as opposed to ultra-strong - interaction regime, while it is only the latter that is supposed to have a weakly coupled gravity as its bulk dual [26]. Indeed, most solids form under the condition that its potential (interaction) and kinetic energies on average balance each other out. This suggests that the ’bona fide’ strong-coupling regime could only become attainable in some sort of a ’flat band’ scenario where kinetic energy is completely quenched or, at least, significantly diminished.</p><p>\\\nIn light of that, it is unsurprising that much of the recent effort towards implementing such mechanism has been centered on the SYK model and its variants [31] whose ’flat band’ nature facilitates the existence of a holographic dual. A viable candidate to this role was proposed in the form of the Jackiw-Teitelboim (JT) dilatonenhanced 1 + 1-dimensional gravity [31].</p><p>\\\nIt is worth pointing out, though, that at the practical level all the holographic matching between the SYK and JT theories has been, so far, established within their low-energy sectors that are both controlled by a single soft Schwarzian mode (’boundary graviton’). So as far as the low-energy properties of the two models are concerned, they both allow for the same (effectively 0 + 1- dimensional) description in terms of either a fluctuating 1d boundary or Liouvillian-type large-N matrix quantum mechanics [31, 36]. This is not surprising given the intrinsically non-dynamical nature of 2d (and 3d) pure gravity. Such a caveat notwithstanding, the low-energy SYK-JT equivalence has been repeatedly and staunchly referred to as a genuine example of holographic correspondence between the 1+1-dimensional bulk and 0+1-dimensional boundary theories [31].</p><p>\\\nAs to the general HV models (22) and corresponding vacuum metrics (26), the standard list of observables to be matched includes temperature-dependent specific heat</p><p>\\\nand frequency-dependent optical conductivity</p><p>\\\ndetermined by the bare scaling dimensions.</p><p>\\\nIncidentally, this value of the HV parameter was previously singled out on the basis of analyzing entanglement entropy [28]. Besides, it suggests the interpretation of d − θ as an effective number of dimensions orthogonal to the FS.</p><p>\\\nThe other frequently invoked relation [26, 28, 29] is</p><p>\\\nin which case the first inequality in (27) is marginally satisfied as equality. Notably, in 2d it would only be consistent with (40) for z = 3/2.</p><p>\\\nAlso, from the beginning of the cuprates saga an even greater fixation has always been on the linear-T dependence of resistivity, also observed in a variety of other materials [35]. Of course, the conductivity scaling with frequency (39) does not readily translate into its temperature dependence, as it would be determined by a specific mechanism of momentum relaxation (i.e., Umklapp, phonons, and/or disorder).</p><p>\\\nTo this end, the use of the memory matrix technique yielded a proper conductivity scaling [26, 35] in both limits of strong,</p><p>\\\nmomentum-non-conserving scattering where ∆ is the dimension of the leading translation invariance-breaking 8 operator. The formulas (42) and (43) agree for ∆ = z + (d − θ)/2 which condition coincides with that of marginal fulfillment of the Harris criterion for the disorder scattering to become a relevant perturbation.</p><p>\\\nAn alternate interpretation of the linear-T resistivity, σ(T ) ∼ 1/T , proposed in [26, 35] relates it to the FL-like entropy, S(T ) ∼ C(T ) ∼ T . This school of thought introduces the notion of inelastic ’Planckian’ scattering rate as a potentially single most important scale for thermalization/equilibration/information scrambling (albeit not a rate of momentum relaxation) in strongly interacting systems</p><p>\\\nInterestingly, it is the (admittedly, unphysical) model of [38] that so far has managed to reproduce a longer list of the power-law dependencies found in the cuprates, as compared to the competing schemes [39]. Unfortunately, such a serendipitous success does not offer any immediate insight into the underlying mechanism of the NFL behavior in the cuprates.</p><p>\\\nFurthermore, contrasting the large-r and -τ asymptotics (31) of the HV holographic propagators against their eikonal/bosonization counterparts in search of some agreement suggests finite positive values of θ, contrary to the ’Planckian’ scenario. This observation might further reduce the chances of constructing a consistent HV holographic model of the strange metal phase in the cuprates.</p><p>\\\nIn part, the deficiencies of the HV-based approach have been circumvented by the arrival of the ’second SYK wave’ [40] which utilizes the Hamiltonian obtained from the conventional combination of a kinetic (quadratic) and interaction (quartic) terms by randomizing the amplitudes of either one or both of these terms a la SY K. Making such randomization spatially non-uniform one opens a channel for non-conservation of momentum which then gives rise to the linear-T ’Planckian’ rate (on top of a constant).</p><p>\\\nOf course, the very existence of different explanations (cf., for example, [35, 39] and [40]) for certain scaling laws observed in the cuprates may suggest that their ultimate interpretation is yet to be found. It would be, therefore, imperative to strive to extend the list of matching properties, akin to [38, 39] as the means of discriminating between the competing schemes.</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":7853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT: Everything you need to know about the AI-powered chatbot","url":"https://techcrunch.com/2025/10/31/chatgpt-everything-to-know-about-the-ai-chatbot/","date":1761932180,"author":"Kyle Wiggers, Cody Corrall, Alyssa Stringer, Kate Park","guid":176,"unread":true,"content":"<article>A timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year.</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tattd gave four TechCrunch writers tattoos at Startup Battlefield","url":"https://techcrunch.com/2025/10/31/tattd-gave-four-techcrunch-writers-tattoos-at-startup-battlefield/","date":1761931929,"author":"Amanda Silberling","guid":175,"unread":true,"content":"<article>Tattd, a marketplace for tattoo-seekers and artists, set up a mini tattoo parlor in the Expo Hall at TechCrunch Disrupt 2025.</article>","contentLength":125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hackers threaten to leak data after breaching University of Pennsylvania to send mass emails","url":"https://techcrunch.com/2025/10/31/hackers-threaten-to-leak-data-after-breaching-university-of-pennsylvania-to-send-mass-emails/","date":1761931802,"author":"Amanda Silberling","guid":174,"unread":true,"content":"<article>As the hackers plainly stated in their message (\"Please stop giving us money\"), this breach appears motivated to suppress alumni donations. </article>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS exceeds Wall Street’s expectations as demand for cloud infra remains high","url":"https://techcrunch.com/2025/10/31/aws-exceeds-wall-streets-expectations-as-demand-for-cloud-infra-remains-high/","date":1761929958,"author":"Rebecca Szkutak","guid":173,"unread":true,"content":"<article>AWS continues to see strong demand as companies gobble up its cloud infrastructure services in the age of AI. </article>","contentLength":110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Government hackers breached telecom giant Ribbon for months before getting caught","url":"https://techcrunch.com/2025/10/31/government-hackers-breached-telecom-giant-ribbon-for-months-before-getting-caught/","date":1761929138,"author":"Zack Whittaker","guid":172,"unread":true,"content":"<article>Ribbon, which provides software and technology to phone and internet giants, said nation-state hackers were in its systems since at least December 2024.</article>","contentLength":152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holographic Propagators: Geodesics and Local Criticality","url":"https://hackernoon.com/holographic-propagators-geodesics-and-local-criticality?source=rss","date":1761927304,"author":"The Tech Reckoning is Upon Us!","guid":288,"unread":true,"content":"<p>The early holographic studies of fermion propagators [28] produced a number of intriguing results, including multiple Fermi surfaces (which merge into one critical ’Fermi ball’ in some extreme limits), dispersionless poles, and oscillatory frequency dependence (which was later shown not to arise in more systematic ’top down’ constructions [26]), etc. A physical interpretation of those results is impeded by the fact that much of this work is numerical.</p><p>\\\nA simple and amenable to analytical treatment semiclassical calculation can be performed in the regime mL ≫ 1 where m is a mass of the conjectured dual bulk fermion [28, 29]. In this regime, the fermion’s paths contributing to various quantum-mechanical amplitudes follow closely the classical boundary-to-boundary trajectories (geodesics) derived from the (imaginary-time) action</p><p>\\\nby varying over τ(u) and r(u).</p><p>\\\nEvaluating this action on its geodesic one obtains</p><p>\\\nWhile an explicit analytic computation of (29) can only be performed in some special cases, the one-parameter space/time dependencies can be readily found for a broad variety of metrics. Specifically, for the HV metric (26) one obtains [29, 30]</p><p>\\\nNotably, in the absence of hyperscaling violation (θ = 0) both these asymptotics become either constant (less likely) or logarithmic (more likely, see below). Thus, if the classical EMD Lagrangian (22) were to represent a valid bulk dual of a boundary theory with the gauge-like interaction (1) the asymptotics (31) would not be readily reconcilable with the eikonal/bosonization results (11,21) which depend primarily on z (via η) rather than θ.</p><p>\\\nand is composed of the two independent solutions which read</p><p>\\\nImposing the proper boundary conditions and following the holographic dictionary [26] one then defines the propagator as a reflection coefficient for the wave incident at the boundary</p><p>\\\nA different behavior (unattainable in the case of a HV metric (26) with finite z and θ) occurs for α = β + 1 in which case the integral in (33) diverges at u → 0. This peculiar NFL regime, dubbed ’local criticality’, is characterized by the propagator</p><p>\\\nwhere a(k), b(k), and ν(k) ∼ k are non-singular functions of momentum that can, in general, produce multiple poles identified as the distinct (’fractionalized’) FS [28].</p><p>\\\nFourier transforming (36) is complicated by the fact that G(ω, k) is not analytically known across the entire range of its arguments. However, the fast (and/or furious) Fourier transformation via a saddle point suggests the following form of this function in the spacetime domain</p><p>\\\nAdding to the intrigue, there are some recent Monte Carlo results on the 2d Hubbard and t − J models that have long been thought to represent the prototypical NFL normal state in the cuprates. These results do not readily conform to a momentum-independent, yet strongly energy-dependent, self-energy function, showing less of energy/temperature dependence than any of the above expressions [33]. It remains to be seen as to what this might imply for the general applicability of the theories of fermions (’spinons’) governed by the interactions (1) to the analysis of those microscopic models.</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":3313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KosmicKrisp Now Vulkan 1.3 Compliant For Apple Devices","url":"https://www.phoronix.com/news/KosmicKrisp-Vulkan-1.3","date":1761927280,"author":"Michael Larabel","guid":692,"unread":true,"content":"<article>Over the summer months LunarG announced KosmicKrisp as a new Vulkan-on-Metal implementation for Apple devices and built around Mesa. That alternative to MoltenVK was upstreamed for next quarter's Mesa 26.0 release and now it's also celebrating being an officially Vulkan 1.3 conformant implementation...</article>","contentLength":303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: The Road to Hell is Paved with Good DRY Intentions (10/31/2025)","url":"https://hackernoon.com/10-31-2025-newsletter?source=rss","date":1761926592,"author":"Noonification","guid":287,"unread":true,"content":"<p>🪐 What’s happening in tech today, October 31, 2025?</p><p>By <a href=\"https://hackernoon.com/u/melvin-manni\">@melvin-manni</a> [ 5 Min read ] Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  <a href=\"https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/salkimmich\">@salkimmich</a> [ 15 Min read ] The evolution of workload identity: Kerberos to X.509 to SPIFFE to TWI. Why credentials should expire faster than your containers run. <a href=\"https://hackernoon.com/workload-identity-what-history-teaches-us-about-the-future-of-machine-identity\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity strikes multi-year licensing deal with Getty Images","url":"https://techcrunch.com/2025/10/31/perplexity-strikes-multi-year-licensing-deal-with-getty-images/","date":1761925574,"author":"Rebecca Bellan","guid":171,"unread":true,"content":"<article>Perplexity’s agreement with Getty appears to legitimize some of the startup’s previous use of Getty’s stock photos. Perplexity came under fire last year for a series of plagiarism accusations from several news organizations. </article>","contentLength":231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video Friday: Happy Robot Halloween!","url":"https://spectrum.ieee.org/video-friday-robot-halloween-2674252642","date":1761924603,"author":"Evan Ackerman","guid":146,"unread":true,"content":"<p>Your weekly selection of awesome robot videos</p>","contentLength":45,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk5MzIzNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwMjU4MDA0M30.nBNXvmaVwqodlAcXGpck-A2yCbWsmFT6eVePaIA2W5Y/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Tim Cook says Apple is open to M&A on the AI front","url":"https://techcrunch.com/2025/10/31/tim-cook-says-apple-is-open-to-ma-on-the-ai-front/","date":1761923832,"author":"Sarah Perez","guid":170,"unread":true,"content":"<article>Apple CEO Tim Cook noted in the company's Q4 2025 earnings call that Apple was preparing to announce more AI partnerships like the one it has with OpenAI to integrate ChatGPT into Siri and Apple Intelligence.</article>","contentLength":208,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Luminar is cutting jobs, losing its CFO, and warning of a cash shortage","url":"https://techcrunch.com/2025/10/31/luminar-is-cutting-jobs-losing-its-cfo-and-warning-of-a-cash-shortage/","date":1761922849,"author":"Sean O'Kane","guid":169,"unread":true,"content":"<article>The new turmoil comes as founder Austin Russell is trying to buy the company just a few months after being replaced as CEO.</article>","contentLength":123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Ubiquitous NFL Problem: Comparing Bosonization, Eikonal, and Holographic Techniques","url":"https://hackernoon.com/the-ubiquitous-nfl-problem-comparing-bosonization-eikonal-and-holographic-techniques?source=rss","date":1761922824,"author":"The Tech Reckoning is Upon Us!","guid":286,"unread":true,"content":"<p>Compared to what it has been just recently [26], the seemingly endless flurry of holographic publications in JHEP, PRD, and other traditional ’condensed matter oriented’ venues has been steadily coming to a mere trickle. Those few holographic exercises that do occasionally pop out still tend to begin with the mantra ’holography is well known to be an established method for studying strongly correlated systems’. However, this optimistic reassurance often appears to be in a rather stark contrast with the typical summary that sounds more like ’as no unambiguous agreement with experiment was found, the problem is left to future work’.</p><p>\\\nAlso, much of the original thrust towards boldly treating an arbitrary condensed matter system of interest as yet another application of some opportunistically chosen weakly-coupled semiclassical gravity has retreated into a ’safer-haven’ topic of hydrodynamics (which, while highlighted and revitalized by holography, can be - and of course had long been - successfully discussed without ever mentioning the latter).</p><p>\\\nOn the outside, it may seem as though the heuristic ’holo-hacking’ (a.k.a. ’bottom up’ or ’non-AdS/nonCFT’) approach tends to pick out its favorite gravity-like bulk theory on the basis of such physically compelling reasons as an existence of the previously found classical solutions and normal modes’ spectra, availability of the numerical simulation software, or mere need to engage students with the tangible computational tasks.</p><p>\\\nHowever, apart from having become a massive and customary practice, there hasn’t been much effort made towards any serious justification of neither the overall holographic scheme, nor its specific ’dictionary’ which was copy-pasted from the original string-theoretical framework. In that regard, it might be worth keeping in mind that just because everyone else on a highway may be driving above the posted speed limit does not by itself make it legal.</p><p>\\\nIn light of the above, comparing holographic propagators to the predictions of other techniques could provide an additional testing ground for, both, the alternate methods as well as the holographic approach itself. the bulk metric, gauge, and scalar (dilaton) fields [26]</p><p>\\\nAmong all the classical solutions of the theory (22) there is a special class of Lifshitz metrics (θ = 0) which were discovered in the semiclassical (ThomasFermi) analysis of matter back-reaction on the metric, as well as in the ’electron star’ scenarios, etc. [27].</p><p>\\\nMore generally, any viable solutions of (22) must obey certain stability (’null energy’) conditions [26]</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":2751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YC alum Adam raises $4.1M to turn viral text-to-3D tool into AI copilot","url":"https://techcrunch.com/2025/10/31/yc-alum-adam-raises-4-1m-to-turn-viral-text-to-3d-tool-into-ai-copilot/","date":1761920436,"author":"Anna Heim","guid":168,"unread":true,"content":"<article>After generating over 10 million social media impressions with the launch of its text-to-3D model app, Adam has raised a $4.1 million seed round to power its next steps.</article>","contentLength":169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Saving 20+ Hours a Week: How Jamie I.F. Built AffiliateFinder.ai to Automate Affiliate Recruitment","url":"https://hackernoon.com/saving-20-hours-a-week-how-jamie-if-built-affiliatefinderai-to-automate-affiliate-recruitment?source=rss","date":1761920220,"author":"NewsByte.Tech","guid":285,"unread":true,"content":"<p>Affiliate and influencer programs is one of the MOST profitable revenue channels for SaaS businesses - but everyone complains about how hard it is to find good affiliates and influencers to promote them. It takes a LOT of manual work.</p><p>So, we save you 20+ hours per week by finding every affiliate currently promoting your competitors so you can recruit them, as well as all the top influencers and creators in your niche to build partnerships with.</p><h2>3. What do you love about your team, and why are you the ones to solve this problem?</h2><p>We’re a team of people who all have a LOT of experience in affiliate marketing - either as affiliate marketers ourselves, or as affiliate managers. We get the industry.</p><p>We know how time-consuming it is to search for new affiliates all the time - and we literally built <a href=\"http://AffiliateFinder.ai\">AffiliateFinder.ai</a> to solve our own problem while running our affiliate management agency! We know what we want in a software like this, so we feel we know exactly how to build this to solve other people having similar problems.</p><h2>4. If you weren’t building your startup, what would you be doing?</h2><p>Probably building a different startup! I really enjoy the game, and I wouldn’t trade it for anything.</p><h2>5. At the moment, how do you measure success? What are your metrics?</h2><p>Product feedback, customer growth, and retention. If we don’t have a good product, people will not complete their 7-day free trial, and we won’t grow customers or revenue. Everything we’re focused on now is around retention, and not just building something they’ll use once to get a list of affiliates, but how to make this incredibly helpful as an ongoing companion for all your partner management work.</p><h2>6. In a few sentences, what do you offer to whom?</h2><p><a href=\"http://AffiliateFinder.ai\">AffiliateFinder.ai</a> helps affiliate, influencer, and partnerships teams 3x their affiliate recruitment by finding all the best-fit potential affiliates for them - and ordering them by priority so they can start with the absolute top partners.</p><p>We save you 20+ hours per week of boring, manual research - freeing you up to focus on building those relationships with affiliates so they can send you sales while you sleep.</p><p>It works great, no matter what type of business you are: B2B SaaS, AI, DTC/ecom, travel, iGaming, fintech and trading - we have many customers across all types of businesses.</p><h2>7. What’s most exciting about your traction to date?</h2><p>We’re used by several of the largest companies in the world. It’s always validating when multi-billion dollar companies use your tool and find it valuable enough to use and pay for!</p><p>And, surprisingly, some of our first customers were forward-thinking managers at these huge companies - they saw our product and instantly understood how it could help them scale their affiliate revenue and outcompete their competition.</p><h2>8. Where do you think your growth will be next year?</h2><p>We’d like to reach 2,000 paying customers by the end of next year, and we’re building to hit that right now.</p><p>If we can help those 2,000 brands reach their goals by recruiting more partners - then we’ll be extremely happy. We have a lot of great products and features we want to build out, and we’re very excited to build these for the world.</p><h2>9. Tell us about your first paying customer and revenue expectations over the next year.</h2><p>It took us a while to get our first paying customer - a good few months from launch.</p><p>We originally launched with a freemium option where you’d get your first 15 affiliates for free, and then you had to upgrade yourself to get the full version. But nobody was upgrading!</p><p>Once we switched to a 7-day free trial, the customers started rolling in.</p><h2>10. What’s your biggest threat?</h2><p>We’re an AI-powered tool, and we use AI to filter out bad-fit affiliates - and recommend the good fits. But, naturally, if there is an extremely advanced AI that can eventually do this all, then this is a huge threat to us. But, we’re working on custom data to make us the most useful tool in affiliate marketing. That hopefully keeps our competitive advantage as AI improves.</p>","contentLength":4037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Construct Koin Plans to Bridge a $300 Trillion Market Gap in Real Estate Financing","url":"https://hackernoon.com/how-construct-koin-plans-to-bridge-a-$300-trillion-market-gap-in-real-estate-financing?source=rss","date":1761919399,"author":"Ishan Pandey","guid":284,"unread":true,"content":"<blockquote><h2>Can blockchain technology solve century-old problems in real estate financing, or does it represent another attempt to apply a solution in search of a problem?</h2></blockquote><p>The question becomes more pressing as <a href=\"https://www.constructkoin.com/\">Construct Koin</a> launches a presale that aims to raise $100 million by offering tokens that start at $0.10 and scale to $1 across 10 phases. The project positions itself as a Real Estate Financing (ReFi) protocol that will modernize how capital flows into property development, but the proof will depend on whether it can deliver where others have failed.</p><p>\\\nThe timing appears calculated. The <a href=\"https://coinpedia.org/research-report/top-real-world-asset-rwa-projects/\">tokenized real-world asset market crossed $30 billion in 2025</a>, a figure that reflects roughly a 10-fold increase from 2022 levels. Private credit accounts for approximately $17 billion of this total, while U.S. Treasuries make up $7.3 billion. The momentum suggests that institutions are finding utility in blockchain infrastructure for specific use cases, particularly those involving yield-bearing assets with standardized documentation.</p><h2>Understanding ReFi and What Construct Koin Actually Does</h2><p>Real Estate Financing through blockchain differs from the property tokenization projects that dominated headlines in previous cycles. Instead of selling fractional ownership in buildings, ReFi protocols focus on the financing process itself. Think of it as digitizing the loan origination and management workflow rather than the asset title.</p><p>\\\nConstruct Koin operates by connecting property developers who need capital with investors who provide it through token purchases. The platform claims to use artificial intelligence integrated with Building Information Management (BIM) software to analyze development proposals and make lending decisions in hours rather than the weeks or months typical in traditional finance. According to the project website, loans are secured by legal charges registered on title deeds with HM Land Registry in the UK, providing a layer of protection through real property collateral.</p><p>\\\nThe mechanics work through a loan book model. When developers borrow funds, they pay interest and profit shares back to the protocol. CTK token holders who stake their tokens receive 8 to 12% annual percentage rates paid in USDT stablecoin. The protocol claims it currently has <a href=\"https://www.constructkoin.com/\">£15 million of assets already secured on-chain</a>, though independent verification of this figure through public blockchain explorers remains limited in available documentation.</p><h2>Breaking Down the $100 Million Presale Structure</h2><p>The fundraising approach spans 10 phases, each with a price increment. The first phase opened at $0.10 per token, and the final phase will close at $1.00 per token at the Token Generation Event (TGE). The model resembles how venture capital rounds work, with later participants paying higher prices than earlier ones. Out of the 1 billion total token supply, 400 million tokens have been allocated to the presale.</p><p>\\\nThis represents 40% of the total supply going to public participants. Another 15% has been earmarked for staking rewards, 20% for ecosystem growth, 15% for team and advisors, and 10% for liquidity and reserves. The vesting schedules for team tokens matter here, though specific timeframes were not detailed in the publicly available documentation. Projects that allow insiders to sell immediately after launch have historically faced selling pressure that can depress token prices.</p><p>\\\nThe presale accepts payments through both fiat channels (credit cards, bank transfers) and six major cryptocurrency networks including Ethereum, Bitcoin, Solana, Polygon, and Binance Smart Chain. Minimum purchase amounts vary by network due to transaction fee structures. Ethereum requires a $100 minimum due to higher gas fees, while networks like Polygon and Solana allow $10 minimum purchases. The tokens will remain locked until TGE, which the project estimates will occur 12 to 24 months from launch.</p><h2>The Technology Stack and AI Claims</h2><p>The project emphasizes its use of AI for underwriting decisions. Traditional property development loans can take weeks or months to process as lenders manually review business plans, financial projections, and construction documents. Construct Koin claims its system achieves a 95% speed improvement by automating this analysis through machine learning models that assess risk based on multiple data points.</p><p>\\\nThe integration with BIM systems provides the AI with access to architectural plans, material specifications, and construction schedules. In theory, this allows the algorithm to evaluate whether a project is feasible, properly costed, and likely to complete on time. The platform processes applications and provides offers in hours rather than weeks, according to marketing materials. However, the details about which specific AI models are being used, what training data they rely on, and how they handle edge cases remain undisclosed.</p><p>\\\nThe technical infrastructure runs on Ethereum as an ERC-20 token. The smart contracts are described as audited, though the names of the audit firms and links to audit reports were not prominently featured in the reviewed materials. The choice of Ethereum provides compatibility with existing DeFi infrastructure and wallet solutions, but it also means users will contend with network congestion and variable transaction fees unless they utilize Layer 2 solutions.</p><h2>Market Context and the RWA Surge</h2><p>Understanding where Construct Koin fits requires context about the broader <a href=\"https://www.coindesk.com/business/2025/06/26/real-world-asset-tokenization-market-has-grown-almost-fivefold-in-3-years\">real-world asset tokenization movement</a>. The sector experienced 380% growth over three years, reaching <a href=\"https://www.coindesk.com/business/2025/06/26/real-world-asset-tokenization-market-has-grown-almost-fivefold-in-3-years\">$24 billion by mid-2025</a> according to a report by RedStone, Gauntlet, and RWA.xyz. This represents a shift from experimental pilots to scaled institutional adoption, particularly in fixed income and private credit categories.</p><p>\\\nMajor financial institutions have entered the space. BlackRock launched a $2.9 billion tokenized fund (BUIDL), while Franklin Templeton's tokenized money market fund represents $420 million in assets. Goldman Sachs partnered with BNY Mellon to tokenize money market funds, supported by regulatory frameworks that aim to streamline settlement and reduce costs. The institutional involvement provides validation that blockchain infrastructure can solve real operational problems in capital markets.</p><h2>Who is Building This and Corporate Structure</h2><p>Chris Baldrey-Chouro serves as CEO and founder of Construct Koin. According to a <a href=\"https://podcasts.apple.com/be/podcast/the-crypto-podcast/id1578175723\">podcast interview on The Crypto Podcast</a>, Baldrey-Chouro describes the project as executing \"one of the most innovative fundraising strategies in crypto history.\" His background includes work in recruitment and staffing solutions for commerce, based on corporate registry information.</p><p>\\\nThis multi-jurisdictional setup is common among crypto projects seeking to optimize for regulatory environments while maintaining operational flexibility. The UK entity provides legitimacy through Companies House registration and operates under UK corporate law. The BVI structure offers advantages for token operations, while the Dubai presence targets the Middle East market, which represents over $2 trillion in real estate value according to project materials.</p><p>The project positions itself as compliance-first, a pitch aimed at institutional allocators who need legal clarity before committing capital. The protocol includes KYC and AML requirements for all investors, with enhanced due diligence for purchases exceeding $10,000. This approach contrasts with many DeFi protocols that operate pseudonymously or with minimal identity verification.</p><p>\\\nThe emphasis on milestone-driven disbursements and oracle-verified events addresses one of the key concerns institutional investors have about blockchain-based financing. Traditional tranche financing releases funds only after borrowers meet specific milestones such as completing foundation work or reaching specific construction stages. Construct Koin claims its smart contracts replicate this structure by releasing capital only after verification of progress, reducing the risk of fund misuse.</p><p>\\\nThe security model relies on conservative loan-to-value ratios of 60 to 70%. This means if a developer defaults, the property securing the loan should be worth significantly more than the outstanding debt, allowing the protocol to recover funds through foreclosure and sale. The protocol also maintains an insurance vault funded by a portion of fees to cover defaults beyond normal parameters. Whether these mechanisms will perform as designed during an actual default scenario remains untested.</p><h2>The Presale Risk Landscape in 2025</h2><p>\\\n<a href=\"https://cryptsy.com/red-flags-to-watch-for-in-crypto-presales/\">Common red flags in presales include</a> anonymous teams, unrealistic return promises, poorly written whitepapers, unclear tokenomics, and lack of transparency about milestones. Legitimate projects typically disclose team identities, provide detailed technical documentation, show clear roadmaps, and communicate regularly about progress. Projects that lack these elements often disappear after raising funds, leaving investors with worthless tokens.</p><p>\\\nThe regulatory environment adds another layer of complexity. Crypto projects face scrutiny about whether their tokens constitute securities under various jurisdictions. The classification determines which regulations apply and what disclosures are required. Projects that ignore legal frameworks or operate without proper licensing expose themselves and their investors to enforcement actions, frozen assets, and potential criminal charges.</p><h2>Real Estate Tokenization Track Record</h2><p>The history of real estate tokenization projects provides lessons. Multiple ventures have attempted to bring property onto blockchain with mixed results. Early projects focused on fractional ownership, allowing investors to buy shares in specific buildings. These faced challenges with liquidity, regulatory compliance, and the complexity of managing physical assets through digital interfaces.</p><p>\\\nMore recent projects have shifted toward the financing layer rather than ownership tokenization. This approach encounters fewer regulatory hurdles since it deals with loan products rather than securities representing property ownership. However, the business model still requires borrowers, which means projects must build relationships with developers and prove they can provide capital at competitive rates.</p><p>\\\nThe question of whether blockchain adds genuine value or merely adds complexity remains contentious. Supporters argue that on-chain transparency, programmable terms, and global capital access justify the technology overhead. Critics point out that traditional finance already has efficient systems for real estate lending and that blockchain's benefits are often overstated relative to implementation costs.</p><p>Several execution risks warrant examination. First, the loan book model requires a steady pipeline of creditworthy borrowers. If developers can obtain financing through traditional channels at lower costs, they have little incentive to use a new platform that charges fees. The project must either offer better terms than banks or target developers who cannot access traditional financing, which introduces credit risk.</p><p>\\\nSecond, the AI underwriting system remains largely unproven at scale. While automation can speed processes, it also concentrates risk if the algorithms make systematic errors. A series of bad loans could deplete the insurance fund and leave token holders with losses. The lack of detailed information about the AI's training data, error rates, and decision-making process makes it difficult to assess this risk.</p><p>\\\nThird, regulatory changes could impact operations. Governments continue to develop frameworks for crypto assets and tokenized securities. A regulatory crackdown in key markets could force the project to halt operations, delist from exchanges, or face enforcement actions. The multi-jurisdictional structure provides some flexibility but also creates compliance complexity across multiple legal systems.</p><p>\\\nFourth, the token economics depend on sustained borrower demand and investor interest. If loan volume does not grow as projected, staking rewards may decline, reducing demand for the token. If token prices fall significantly below the purchase price, early investors may become discouraged and sell, creating additional downward pressure. The long lock-up period until TGE means investors cannot exit if circumstances change.</p><p>Looking at established players in the space provides benchmarks. <a href=\"https://coinpedia.org/research-report/top-real-world-asset-rwa-projects/\">Centrifuge has achieved $1 billion in Total Value Locked</a>, making it the third RWA protocol to reach this milestone. The platform tokenizes invoices, receivables, and trade finance instruments, pushing them into DeFi markets as collateral. Centrifuge completed a V3 migration in 2025, delivering multichain infrastructure across six EVM chains.</p><p>\\\nOndo Finance focuses on institutional-grade tokenized securities and has built infrastructure for bringing fixed-income products on-chain. The platform emphasizes compliance and works within regulatory frameworks rather than attempting to circumvent them. This approach has allowed Ondo to partner with traditional financial institutions and build sustainable business models.</p><p>\\\nThe difference between these established protocols and a new entrant like Construct Koin lies in track record. Centrifuge and Ondo have processed real transactions, demonstrated their technology works, and built reputations over multiple years. They have also secured institutional backing and navigated regulatory processes. Construct Koin must still prove it can execute its vision and deliver returns to token holders.</p><h2>What The Numbers Actually Show</h2><p>The project claims £15 million in assets already secured on-chain. Converting to dollars at current exchange rates gives approximately $19 million in collateral backing the protocol before the presale completes. If the presale reaches its $100 million target, the ratio of raised capital to existing collateral will be roughly 5 to 1. This means the project would need to deploy the raised funds into new loans relatively quickly to maintain proportional backing.</p><p>\\\nThe staking rewards of 8 to 12% APR paid in USDT require generating sufficient revenue from loan interest and fees. If the protocol charges borrowers 7 to 15% annual interest, as stated in marketing materials, the math works if the majority of loans perform and default rates remain low. However, a 10% default rate combined with recovery costs could quickly consume the margin between what borrowers pay and what stakers receive.</p><p>\\\nThe tokenomics allocate 15% of supply for staking rewards. With 1 billion total tokens, this equals 150 million tokens reserved for rewards. If tokens reach $1 at TGE as the presale structure suggests, that represents $150 million in value designated for staking. Paying 12% APR on a pool of staked tokens would require substantial protocol revenue, meaning the loan book must grow significantly to sustain these yields.</p><h2>The Bigger Picture: Does ReFi Have Product-Market Fit?</h2><p>The core question is whether blockchain-based real estate financing solves problems that matter to enough participants to create a sustainable market. Developers need capital, and investors want returns. Traditional systems provide both, albeit with friction from intermediaries, paperwork, and slow processes.</p><p>\\\nBlockchain's value proposition centers on disintermediation, transparency, and global access. By removing middlemen, protocols can theoretically offer borrowers lower rates and investors higher yields. By recording transactions on-chain, all parties can audit the state of loans in real-time. By operating globally, capital can flow from anywhere to anywhere, removing geographical barriers.</p><p>\\\nThe counterargument is that the friction in traditional finance exists for reasons. Paperwork and slow processes often serve as risk management mechanisms that prevent bad deals from proceeding. Intermediaries like banks provide expertise in underwriting, legal structuring, and recovery that algorithmic systems may struggle to replicate. Global capital flows sound attractive until investors face losses in foreign jurisdictions where recovering assets is difficult or impossible.</p><p>After examining the available information about Construct Koin, the project represents both the promise and peril of real-world asset tokenization in its current phase. The promise lies in the genuine growth of the RWA sector, which has demonstrated that certain use cases have found product-market fit. The peril comes from execution risk, regulatory uncertainty, and the long history of crypto projects that fail to deliver on ambitious visions.</p><p>\\\nThe data points to consider are straightforward. The RWA market is real and growing, reaching $30 billion with institutional participation from major financial players. Real estate represents the largest asset class globally at over $300 trillion, meaning even a tiny percentage of tokenization would create enormous value. The technology for tokenizing loans and managing them through smart contracts exists and has been deployed by other projects successfully.</p><p>\\\nAgainst this, the presale model concentrates risk on early participants who must wait 12 to 24 months for tokens to unlock while hoping the project executes. The team, while public, does not appear to have prior experience building DeFi protocols or managing large-scale lending operations. The AI claims lack substantiation through independent testing or published benchmarks. The regulatory landscape remains fluid, with governments still determining how to classify and regulate tokenized assets.</p><p>\\\nThe honest assessment is that Construct Koin is attempting something difficult that, if successful, could generate returns for early participants. It is also attempting something that could fail for multiple reasons including poor execution, regulatory intervention, lack of borrower adoption, or macroeconomic changes that reduce demand for real estate development financing. Potential participants should view this as a high-risk investment where capital loss is a realistic outcome, not merely a theoretical possibility disclosed in legal disclaimers.</p><p>\\\nThe project would benefit from greater transparency about its AI technology, more detailed disclosure of its existing loan book, and clearer communication about partnerships with developers who will actually use the platform. Without these elements, investors are essentially betting on a vision backed by marketing materials rather than proven operational metrics.</p><p>\\\nIn a market where over half of projects fail, the bar for success is high. Whether Construct Koin clears that bar will depend on execution over the coming years, not on the quality of its presale marketing or the size of its fundraising target.</p><p>\\\nDon’t forget to like and share the story!</p><p>:::tip\n<em>This author is an independent contributor publishing via our&nbsp;. HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO</em></p>","contentLength":18959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubuntu 25.10 amd64v3 Benchmarks: Some Minor & Rare Performance Advantages For Desktop Workloads","url":"https://www.phoronix.com/review/ubuntu-2510-amd64v3","date":1761917760,"author":"Michael Larabel","guid":691,"unread":true,"content":"<article>Yesterday Canonical announced architecture variants for Ubuntu Linux with Ubuntu 25.10 seeing the introduction of \"amd64v3\" packages that are built for the x86_64-v3 micro-architecture feature level to assume AVX/AVX2 and other newer CPU ISA features found since Intel Haswell and AMD Excavator processors. Eager to run some initial tests, here is a first look at the Ubuntu 25.10 amd64v3 performance for desktop workloads.</article>","contentLength":423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reddit CEO says chatbots are not a traffic driver","url":"https://techcrunch.com/2025/10/31/reddit-ceo-says-chatbots-are-not-a-traffic-driver/","date":1761917643,"author":"Ivan Mehta","guid":167,"unread":true,"content":"<article>During Reddit's Q3 2025 call, CEO Steve Huffman noted that Google search and direct access continue to be its top traffic drivers.</article>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nvidia expands AI ties with Hyundai, Samsung, SK, Naver","url":"https://techcrunch.com/2025/10/31/nvidia-expands-ai-ties-with-hyundai-samsung-sk-naver/","date":1761917384,"author":"Kate Park","guid":166,"unread":true,"content":"<article>Nvidia CEO Jensen Huang is visiting South Korea to strengthen partnerships with Samsung, Hyundai, SK, and Naver, unveiling plans for AI-powered networks and next-generation intelligent systems.</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In 1953, the Ford X-100 Concept Car Had It All","url":"https://spectrum.ieee.org/ford-x-100-concept-car","date":1761915604,"author":"Allison Marsh","guid":145,"unread":true,"content":"<p>Heated seats, a radio phone, even an electric shaver in the glove box</p>","contentLength":69,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1MTgxNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMTUyNjcyMn0.mXOXoQHDNVmqLsPQO9kBLfdSxCqStA7LmZrn3W5Xvu8/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"AMD Windows Driver Changes For RX 5000/6000 Series Won't Impact Linux Users","url":"https://www.phoronix.com/news/AMD-Windows-RX-5000-6000-Game","date":1761907869,"author":"Michael Larabel","guid":690,"unread":true,"content":"<article>Over the past day there have been many reports of AMD planning to no longer provide game optimizations for the Radeon RX 5000 and RX 6000 series graphics cards for their Microsoft Windows driver. Surprisingly many in the Linux community still seem to think it will impact the Linux drivers, but long story short, there is no real concern for Linux users/gamers...</article>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.18-rc4 Fixes Another Performance Regression In The Power Management Code","url":"https://www.phoronix.com/news/Linux-6.18-rc4-PM-Perf-Fix","date":1761906960,"author":"Michael Larabel","guid":689,"unread":true,"content":"<article>Last week there was a fix for a \"serious performance regression\" in the Linux kernel's power management code that affected some Intel-powered Chromebooks. This week the power management fixes ahead of Linux 6.18-rc4 is addressing another performance regression...</article>","contentLength":263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AerynOS 2025.10 ISOs Released - GNOME 49, Switches Back To GNU libstdc++","url":"https://www.phoronix.com/news/AerynOS-2025.10-ISOs","date":1761905568,"author":"Michael Larabel","guid":688,"unread":true,"content":"<article>AerynOS 2025.10 ISOs were released today for closing out the month of October. AerynOS as a reminder is the Linux distribution that was started by Ikey Doherty and originally known as Serpent OS that has since evolved into an open-source team effort...</article>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Krita Lands Basic HDR Support On Wayland","url":"https://www.phoronix.com/news/Krita-HDR-Wayland-Support","date":1761905043,"author":"Michael Larabel","guid":687,"unread":true,"content":"<article>The KDE/Qt-aligned Krita digital painting application is the latest creative app now supporting high dynamic range (HDR) on Linux when using Wayland...</article>","contentLength":151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulkan 1.4.331 Brings Two New Extensions","url":"https://www.phoronix.com/news/Vulkan-1.4.331-Released","date":1761904127,"author":"Michael Larabel","guid":686,"unread":true,"content":"<article>Just one week after Vulkan 1.4.330 brought five new extensions, Vulkan 1.4.331 is now available with another two new extensions for this high performance graphics and compute API...</article>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aembit Introduces Identity And Access Management For Agentic AI","url":"https://hackernoon.com/aembit-introduces-identity-and-access-management-for-agentic-ai?source=rss","date":1761901555,"author":"CyberNewswire","guid":283,"unread":true,"content":"<p>Silver Spring, USA/ Maryland, October 30th, 2025/CyberNewsWire/-- today announced the launch of Aembit Identity and Access Management (IAM) for Agentic AI, a set of capabilities that help organizations safely provide and enforce access policies for AI agents as they move into production. </p><p>The release introduces Blended Identity, which defines how AI agents act on behalf of verified users, and the MCP Identity Gateway, which ensures secure access to enterprise resources based on identity, access policy, and runtime attributes.</p><p>The new offering extends the Aembit Workload IAM Platform to address one of the most pressing operational questions in artificial intelligence and modern IT: how to control what autonomous and user-driven AI agents can access, under what conditions, and with what accountability.</p><p>AI agents are rapidly becoming a key part of enterprise operations. Nearly half of technology executives say they are already adopting or fully deploying agentic AI, and about the same share expect most of their AI deployments to be autonomous within two years, according to an . These agents retrieve sensitive data, open tickets, and execute code across cloud, on-premises, and SaaS environments.</p><p>Yet most access models were built for people, not self-directed software. Many still rely on static secrets and shared credentials, creating risk and obscuring accountability. </p><p>Worse yet, agents’ actions are often hidden behind the identity of a human, making it almost impossible to audit the actions each actor has taken. The result is a widening gap between the pace of AI adoption and the ability of organizations to secure it with confidence.</p><p> assigns each agent a cryptographically verified identity, issues ephemeral credentials, and enforces policy at runtime. The system records every access decision and maintains attribution across both human-driven and autonomous agent activity. </p><p>By bringing agent activity under the same centralized policy control plane that governs other workloads, Aembit enables enterprises to deploy AI at scale while maintaining control, auditability, and compliance.</p><blockquote><p>“Enterprises want to say yes to agentic AI, and they’re asking Aembit for ways to securely grant agents access to data and applications,” said David Goldschlag, co-founder and CEO of Aembit. </p></blockquote><blockquote><p>“Aembit IAM for Agentic AI gives enterprises the same level of control and audit over agent access that IAM systems have long provided for employees. Our approach enables organizations to advance their AI initiatives without expanding their threat and risk surface.”</p></blockquote><p>The release introduces two core capabilities to the Aembit Workload IAM Platform:</p><ul><li>Blended Identity, which gives every AI agent its own verified identity and, when needed, binds it to the human it represents. This establishes a single, traceable identity for each agent action and allows Aembit to issue a secure credential that reflects that combined context.</li><li>MCP Identity Gateway, which receives that identity credential and controls how agents connect to tools through the Model Context Protocol (MCP). The gateway authenticates the agent, enforces policy, and performs token exchange to securely retrieve the necessary access permissions for each connected resource – without ever exposing them to the agent runtime.</li></ul><p>Together, this functionality allows enterprises to apply least-privilege access, revoke permissions immediately when needed, and ensure that every AI action is attributable and auditable. </p><p>They operate on Aembit’s established Workload IAM foundation, which enforces policy dynamically at runtime, issues ephemeral credentials just in time, and records structured events for full traceability.</p><p>Aembit developed IAM for Agentic AI through collaboration with large businesses, government organizations, and innovative agentic AI startups deploying AI for operational and security workloads. Those efforts helped shape an approach that combines enterprise enforcement with the adaptability AI projects demand.</p><blockquote><p>“AI agents don’t live inside one stack or trust domain,” said Kevin Sapp, co-founder and CTO of Aembit. “They move between hybrid environments in seconds. With Aembit, every agent carries a verified identity that our gateway can authenticate and control in real time. It’s how enterprises can give agents the access they need to work, while never losing sight of who they are or what they touch.”</p></blockquote><p>Aembit IAM for Agentic AI is now available to customers using its Workload IAM Platform. Organizations can learn more, request a demo, or get started today at .</p><p> is the identity and access management platform for agentic AI and workloads. It enforces access based on identity, context, and centrally managed policies, giving organizations a singular place to control access risk from AI agents, automate credential management, and accelerate AI adoption. </p><p>With Aembit, enterprises can confidently control access to sensitive resources across all the workloads that power their business.</p><p>:::tip\n<em>This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":5160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Can Governments Pay Open Source Maintainers?","url":"https://hackernoon.com/how-can-governments-pay-open-source-maintainers?source=rss","date":1761894488,"author":"Terence Eden","guid":282,"unread":true,"content":"<p>When I worked for the UK Government I was once asked if we could find a way to pay for all the Open Source Software we were using. It is a surprisingly hard problem and I want to talk about some of the issues we faced.</p><p>What about the Open Source that UK Government ?</p><p>Open Source is facing a crisis. The code that the world relies on is often developed by underpaid engineers on the brink of burn-out.  While I don't think anyone wants Open Source to have a paywall, it seems obvious that large organisation should pay their way and not rely solely on volunteer labour.</p><p>Here are some of the problems I faced when trying to get the UK Government to pay for OSS and how  as a maintainer can help make it easier for large organisations to pay you.</p><p>Firstly, lots of OSS doesn't have a well defined owner; so who gets the money?</p><p>I'm not saying that every little library you create needs to be published by a registered company, nor am I suggesting that you should remove your anonymity. But Governments and other organisations need to know  they are funding and  the money is going. The danger of accidentally funnelling money to a sanctioned state or person is just too big a risk for most organisations.</p><p>If you want to receive funding - make it  clear who you are.</p><p>Even when there is an owner, there often isn't an easy mechanism for paying people. Donation sites like GitHub Sponsors, Ko-Fi, and Patreon are great for individuals who want to throw a small amount of money to creators but they can be problematic for larger organisations.  Many OSS projects get around this by offering support contracts. It makes it much easier for an organisation to justify their spend because they're no longer donating to something which can be obtained for free; they're paying for a service.</p><p>This doesn't have to be a contract offering a 24/7 response and guaranteed SLA. It can be as simple as offering best-effort email support.</p><p>The important thing is to offer an  way for a larger organisation to buy your services. Many organisations have corporate credit cards for lower-cost discretionary spending which doesn't require a full business-case.  How easily could a manager buy a £500 support contact from your site?</p><p>Maintainers don't only have to offer support contracts. Many choose to offer training packages which are a good way to raise money  get more people using your product. Some project maintainers will speak at your conference for a suitable fee.</p><p>Again, the aim here is for maintainers to offer a  reason for a payment to be made.</p><p>Open Source has a brilliant culture of allowing multiple (often anonymous) contributors. That's fine when there's no money involved, but how does a moderately sized project decide who receives what share of the funding? Services like <a href=\"https://opencollective.com/\">OpenCollective</a> can make it easier to show  the money is going but it is better to discuss in advance with all contributors what they expect as a share.</p><p>If people think they're being taken advantage of, or that a project maintainer is unjustly enriching themselves, it can cause arguments.  Be very clear to contributors what the funding is for and whether they're entitled to any of it.</p><p>Finally, we faced the issue that some OSS projects didn't  to take money from the \"big bad state\". They were worried that if people saw \"Sponsored by the Government\" they would assume that there were backdoors for spies, or that the developer might give in to pressure to add unwanted features.  This (usually) isn't the case but it is easy to see why having a single large organisation as the main donor could give the impression of impropriety.</p><p>The best defence against this is to have  of paying sponsors! Having the state as one of many partners makes it clear that a project isn't beholden to any one customer.</p><p>It isn't impossible to get Governments to spend on Open Source. But state spending is heavily scrutinised and, bluntly, they aren't set up to pay  amounts to non-suppliers, who aren't charging money.  While large projects often have the resources to apply for Government grants and contracts, smaller projects rarely have the time or expertise. It is critical that maintainers remove the barriers which make it too hard for organisations to pay them.</p><ul><li><p>Make it easy for Governments and other large organisations to pay you.</p></li><li><p>Be as obvious as possible that you are able to accept payments from them.</p></li><li><p>Don't be afraid to put a large price on your talents.</p></li><li><p>Offer multiple paid-for options like speaker fees, support, and feature development funding.</p></li><li><p>Talk with your contributors to let them know how any funding will be shared.</p></li></ul>","contentLength":4568,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empowering Flink CDC: Schema Evolution Support Lands in Apache SeaTunnel","url":"https://hackernoon.com/empowering-flink-cdc-schema-evolution-support-lands-in-apache-seatunnel?source=rss","date":1761894481,"author":"William Guo","guid":281,"unread":true,"content":"<article>From classroom to codebase! Meet Dong Jiaxin, a student from USTB who brought CDC Schema Evolution to Apache SeaTunnel on Flink during the OSPP. </article>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scenes from TechCrunch Disrupt 2025","url":"https://techcrunch.com/2025/10/30/scenes-from-techcrunch-disrupt/","date":1761883353,"author":"Connie Loizos","guid":165,"unread":true,"content":"<article>Thanks to everyone who made this year's San Francisco event what it was -- and to the 10,000 of you who filled the halls, made the connections, and left with more than you came with. Couldn't make it? These images tell part of the story. </article>","contentLength":238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Genode-Powered Sculpt OS 25.10 Brings Performance Improvements & Better Drivers","url":"https://www.phoronix.com/news/Sculpt-OS-25.10-Released","date":1761870165,"author":"Michael Larabel","guid":685,"unread":true,"content":"<article>The Genode operating system framework continues innovating over a decade and a half later on this original open-source OS creation and with that Sculpt OS as its general purpose OS. Out today is Sculpt OS 25.10 to incorporate the latest enhancements to the platform...</article>","contentLength":268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}