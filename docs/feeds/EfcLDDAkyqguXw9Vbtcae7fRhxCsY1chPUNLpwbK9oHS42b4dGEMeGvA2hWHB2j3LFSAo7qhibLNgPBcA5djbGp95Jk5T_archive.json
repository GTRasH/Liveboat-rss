{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"The looming AI clownpocalypse","url":"https://honnibal.dev/blog/clownpocalypse","date":1772375935,"author":"/u/syllogism_","guid":366,"unread":true,"content":"<p>Over the last few years there‚Äôs been a big debate raging with keywords like ‚Äúthe singularity‚Äù,\n‚Äúsuperintelligence‚Äù, and ‚Äúdoomers‚Äù. I propose a sort of truce on that debate. The terms of\nthe truce are that everyone still gets to sneer at their erstwile opponents and their cringe\nidiot takes, but we also all agree that whatever‚Äôs being discussed there, the hypothetical\n‚ÄúBut what if the dumbest possible version of everything happens? What then?‚Äù hasn‚Äôt really\nbeen the conversation, because wtf why make that the premise, right?</p><p>Well. Times have changed.</p><p>The way current and imminent AI technologies are being deployed introduces very\ntangible risks. These risks don‚Äôt require superintelligence, and they‚Äôre\nnot ‚Äúexistential‚Äù. They‚Äôre plenty bad though. So the truce I‚Äôm proposing is that we all get to care\nabout these risks, without the ‚Äúdenialists‚Äù rushing to say ‚Äúsee it‚Äôs not existential!‚Äù or\nthe ‚Äúdoomers‚Äù getting to say ‚Äúsee I told you shit could get bad‚Äù.</p><p>I promise this is a serious post, even though the situation is so stupid my tone will often\ncrack. The basic thesis statement is that a self-replicating thing doesn‚Äôt have to be very smart\nto cause major problems. Generally we can plan ahead though, and contain the damage. Well, we \ndo that. In theory. Or we could spice things up a bit. Maybe run some bat-licking ecotours instead.\nWhy not?</p><p>Here‚Äôs a rough sketch of a bad scenario. Imagine you have some autonomous way to convert resources\ninto exploits ‚Äî hacks, basically. Maybe you have some prompts that try to trick Claude Code or Codex\ninto doing it, maybe you use open-source models. However works. Now, these exploits are going to pay out\nin various ways when you can land them. Lowest yield is just some compute, but maybe you can also steal\nsome dollars or crypto, or steal some data to sell, or even ransomware. The question is, what happens\nwhen we reach the tipping point where exploits become cheaper to autonomously develop than they yield on\naverage?</p><p>The general scenario is something I‚Äôve always thought was worth worrying about. But you know, maybe\nit could be okay, at least for a while ‚Äî after all, the stuff that‚Äôs making the exploits cheaper to\ndevelop should let us make everything more secure too, right? ‚Ä¶Right? Lol no, this is the clownpocalypse,\nwhere the bats taste great. We use coding agents to make everything way  secure.</p><p>The general mindset in the industry at the moment is that everything‚Äôs a frantic race, and if you‚Äôre worrying\nyou‚Äôre losing. The sheer pace of change in software systems would be a concern in itself, but there are so many\nother problems I almost don‚Äôt know where to start.</p><p>I guess I‚Äôll start with an example that would be easy to fix, but captures the zeitgeist pretty well. Coding agents\nlike Claude Code and Codex can read in ‚Äúskills‚Äù files, which are basically just Markdown files that get appended\nto the prompt (you can have code as well, but that‚Äôs not important here). Kind of nice. So everyone rushes to\npublish skills, you get sites to find and install skills like <a target=\"_blank\" href=\"https://skills.sh/\">Skills.sh</a>. Except, nobody\nbothered to even think far enough ahead to prohibit HTML comments in the Markdown. This means any skill you browse\non a website like Skills.sh could have hidden text that isn‚Äôt rendered to you, but can direct your agent to get\nup to various mischief. Remember that agents often have extremely broad permissions. During development loops\npeople often give the agent access to basically everything the developer has. People leave agents running\nunsupervised. This problem has been known for weeks. There was even a <a target=\"_blank\" href=\"https://x.com/theonejvo/status/2015892980851474595\">high-profile demonstration</a>\nof the vulnerability: Jamieson O‚ÄôReilly published a skill called ‚ÄúWhat Would Elon Do‚Äù (chef‚Äôs kiss), manipulated it\nto the top of a popular marketplace, and notified victims they‚Äôd been owned. The fix is trivial: obviously\nthe skills format should prohibit HTML comments, but to date there‚Äôs been zero move to actually do that.\nIt‚Äôs nobody‚Äôs problem and nobody seems to care.</p><p>O‚ÄôReilly demonstrated the unrendered text vulnerability in the <a target=\"_blank\" href=\"https://github.com/openclaw/openclaw\">OpenClaw</a> ecosystem, which is for sure\none of the four balloon animals of the AI clownpocalypse. I don‚Äôt know what the other three would be, but OpenClaw\nis a lock for one of them. So many stories of people just giving the agent all their keys and letting it drive,\nonly for it to immediately drive into a wall by deleting files, distributing sensitive information, racking\nup usage bills, deleting emails‚Ä¶And all of these things can honestly be considered expected usage, it isn‚Äôt\na ‚Äúbug‚Äù when a classifier makes an incorrect prediction, it‚Äôs part of the game. What  a bug are the <a target=\"_blank\" href=\"https://blogs.cisco.com/ai/personal-ai-agents-like-openclaw-are-a-security-nightmare\">thousands\nof misconfigured instances open to the internet</a>,\nalong with the hundreds of other security vulnerabilities. Mostly nobody cared though. It was still the fastest\ngrowing project in GitHub history, before being\n<a target=\"_blank\" href=\"https://www.cnbc.com/2026/02/15/openclaw-creator-peter-steinberger-joining-openai-altman-says.html\">acquihired into OpenAI</a>.</p><p>How did we get here? I dunno man, I really don‚Äôt. <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Normalization_of_deviance\">Normalization of deviance</a> I guess? The literal phrase seems to capture\nthe current political meta, and there‚Äôs an air of resigned watch-the-world-burn apathy to everything. It doesn‚Äôt help\nthat insecurity is baked into LLMs pretty fundamentally. When ChatGPT was first released I thought prompt injection\nwould be this sort of quaint oversight, like oh they forgot to concatenate in a copy of the prompt vector high up\nin the network, so the model can tell which bit is the prompt alone and which bit is the prompt-plus-context. But\nnah nobody ever did that. I guess it didn‚Äôt work? Nobody talks about it, so as far as I can tell nobody‚Äôs even trying.\nSo we‚Äôve all just accepted that maybe one day our coding agent will read an html page that tricks it into deleting our home\ndirectory. Oopsie. Well I can run my agent sandboxed, so at least my files will be safe. But what if it tricks my agent\ninto including a comment in the source of my docs page that will trick a lot of  agents into including a comment that‚Ä¶\netc. Well, fortunately that hasn‚Äôt happened yet, and we all know that‚Äôs the main thing that counts when assessing\nthe severity of a potential vulnerability, right?</p><p>You see the go-fast-but-also-meh-whatever vibe everywhere if you look for it. Google‚Äôs LLM product, Gemini, insisted on shipping\nwith this one-click API key workflow, presumably because the product owners hated the idea of making users sign up through Google Cloud,\nwhich is a longer process than you need for something like OpenAI. Except, this introduced this whole separate auth flow,\nwhich has been recently upgraded from clusterfuck to catastrafuck. Previously I thought that the situation was just confusing:\nthe web pages for the two rival workflows don‚Äôt mention each other, there‚Äôs no vocabulary to describe the difference, and\nthere‚Äôs some features that only work if you auth one way but not the other. Clusterfuck.\nBut, recently we learned that <a target=\"_blank\" href=\"https://trufflesecurity.com/blog/google-api-keys-werent-secrets-but-then-gemini-changed-the-rules\">the Gemini API keys break a design assumption behind Google‚Äôs existing security posture</a>: keys aren‚Äôt\nsupposed to be secrets; you‚Äôre supposed to be able to embed them in client code, if you‚Äôre doing something like distributing a free\napp that has to access Google Maps. But now many of those existing keys are  auth keys for Gemini! So thousands of people had\nkeys lying around that could be used to steal money from them by using Gemini (e.g. to develop malware), having done absolutely nothing\nwrong themselves. Well, fortunately the vulnerability was found by professionals, and reported through the proper channels, so no\nharm done, right? Well, almost. The researchers did contact Google correctly, but then Google first denied the problem, and only\naccepted it when the researchers showed  were affected. So then the 90 day disclosure window started, and Google\nshuffled their feet a bit, rolled out a patchwork fix, and ultimately blew the deadline. So the report went live without a full fix\nin place. Catastrafuck.</p><p>So far even when they‚Äôve been bad, malware attacks haven‚Äôt been  bad. So okay, even if this does go wrong‚Ä¶how bad could the\nAI clownpocalypse be? This is where I ask for just a little imagination, along with some acceptance that today‚Äôs AI models are not entirely\nincompetent, and they‚Äôre getting more capable every day. Many current AI models are no longer really ‚Äúlanguage models‚Äù, in that the\nobjective they‚Äôve mostly been trained to do is predict successful reasoning paths, rather than predict likely text continuations.\nI wrote about this in a <a href=\"https://honnibal.dev/blog/ai-bubble\">previous post</a>. If there‚Äôs a malware going around suborning existing agents or co-opting hardware\nby installing its own agent onto it, it‚Äôs probably going to be using one of these reasoning-trained models. They‚Äôre much better for\ncoding, and the malware probably wants to execute multi-step plans. It wants to send phishing emails, do some social engineering,\nhunt around for crypto or bank details, maybe send some ‚Äúhelp stranded please send money‚Äù scam messages ‚Äî you get the picture.\nWell, those plans will involve reading a lot of text in, and the malware probably isn‚Äôt going to use a high capability model. At\nany point the model‚Äôs view of its current goal can drift. Instead of telling your grandmother to send money, it could tell her to\ndrink drain cleaner. Or it could message her ‚ÄúRawr XD *tackles you*‚Äú. I don‚Äôt want to make out like there‚Äôs this inner kill-bot,\nwaiting to be unleashed. It‚Äôs just that it could be anything.\nThere‚Äôs truly no way of knowing. Anthropic call it the <a target=\"_blank\" href=\"https://alignment.anthropic.com/2026/hot-mess-of-ai/\">‚Äúhot mess‚Äù</a> safety\nproblem, which I think is apt. In the clownpocalypse scenario you have millions of these hot messes.</p><p>How bad could that be? Hard to say! We‚Äôve seen ransomware attacks against hospitals already, so pencil that in as a possibility. Somewhere\na bot sends a message, ‚ÄúI‚Äôve infilitrated the hospital. Pay me or I‚Äôll change around all the data so people get the wrong medications and\ndie‚Äù. Is it bluffing? Probably, but what if it‚Äôs not? It‚Äôs not like you can even pay it ‚Äî it can just send the same message again. Some\nof these won‚Äôt be bluffs, and it could be anything. What happens if you hack a dam? The power grid? We got a lot of guys in their 80s with\nwealth and power around the world, what could they be tricked into doing if the wrong bot is able to slide into their DMs? Can the Russian\nmilitary be compromised? A lot of their frontline stuff is running off\n<a target=\"_blank\" href=\"https://militarnyi.com/en/news/investigation-80-of-russian-troops-linkups-on-the-front-line-produced-by-u-s-company-ubiquiti/\">consumer hardware</a>.\nAre there any Ukrainian drones that could be hacked and sent to bomb Berlin?\nSomewhere in Pakistan is there some dusty PC running Windows 98 hooked up to exactly the wrong network? The only thing we can be\nconfident about is that whatever the worst situation is, it‚Äôs extremely unlikely anyone will predict exactly that thing.</p><p>A lot of the AI safety debate has been like, ‚ÄúIs it possible to design a door so secure it wouldn‚Äôt be practical for anyone to pick it before\nsecurity guards arrive?‚Äù. I think that debate‚Äôs important, but like, look around. Door? What door? Oh, you mean those things\nwe used to have in entrance ways? Yeah nah those were bad for user experience. We‚Äôre all about on-ramps now.</p><p>If you think superintelligence is an urgent existential risk, I‚Äôm not asking you to stop caring about that or making the case. And if you think\nsuperintelligence is robot rapture nonsense, I‚Äôm not asking you to admit the folks you‚Äôve been calling libertarian edgelords were right about anything.\nBut we need to pause and take stock. It‚Äôs not going to take a superintelligence to wreck our shit. The coding agents are getting better and better, and\nwhat we‚Äôre doing with the technology is working really hard to make ourselves more and more exposed. We‚Äôre shipping the vulnerabilities super fast now though üí™.\nGo team I guess?</p><p>So what can be done? I mean, lots! I wouldn‚Äôt call it a clownpocalypse if it were some desperate dilemma. If we can just recognise the danger and honk the horn,\nwe could be rolling out meaningful fixes tomorrow. If you‚Äôre an AI consumer, start taking security posture much much more seriously. A lot of people are\nskating by on the idea that meh, I‚Äôm not really worth targeting specifically ‚Äî but that‚Äôs not going to be how it works. As soon as we reach that tipping\npoint where autonomous attacks have a positive return, it‚Äôs going to be a full-court press. We‚Äôre also going to face huge pressure on non-computational\ninterfaces ‚Äî all those processes that involve picking up a phone or manually emailing someone. Some of those problems will be really difficult, so the\nleast we can do is get ready and make sure we‚Äôre not making them worse. For the major AI providers, please please take much more prosaic safety and security\nissues more seriously. By all means, continue paying for papers about the hard problem of consciousness ‚Äî it‚Äôs not like philosopers are expensive, on the\nscale of things. But you  to be willing to introduce some product friction for security. It‚Äôs essential. If you don‚Äôt this is all going to blow up\nreally badly.</p><p>The following list was generated with AI assistance. I‚Äôve visited the links but haven‚Äôt read them all fully.</p>","contentLength":13256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rhyv48/the_looming_ai_clownpocalypse/"},{"title":"Simple Made Inevitable: The Economics of Language Choice in the LLM Era","url":"https://felixbarbalet.com/simple-made-inevitable-the-economics-of-language-choice-in-the-llm-era/","date":1772336718,"author":"/u/alexdmiller","guid":362,"unread":true,"content":"<p>Two years ago, I <a href=\"https://felixbarbalet.com/leveraging-polylith-to-improve-consistency-reduce-complexity-and-increase-changeability/\" rel=\"noreferrer\">wrote about managing twenty microservices at Qantas</a> with a small team. The problem was keeping services in sync, coordinating changes across system boundaries, fighting the  of a codebase that grew faster than our ability to reason about it. </p><p>Many years before my time, someone had chosen Clojure to build these systems. I suggested we add Polylith - this was a powerful combination because it enabled us to attack that \"entropy\" directly. Composition over coordination. Data over ceremony. Simplicity over familiarity.</p><p>I described it at the time as a \"fight against accidental complexity\" - the stuff that isn't the problem itself, but the overhead imposed by our tools and processes. The stuff that accretes.</p><p>Fast forward to today - I've been watching LLM coding agents struggle with the exact same fight, and I think the choice of language matters far more than most people realise. </p><p>I've used Clojure for a decade, and I'm biased. But I think the  have shifted in ways that make my bias look less like preference and more like - well, let's call it a \"fortunate capital allocation\".</p><h2>The distinction that matters</h2><p>Fred Brooks drew the line in 1986. In \"<a href=\"https://en.wikipedia.org/wiki/No_Silver_Bullet?ref=felixbarbalet.com\" rel=\"noreferrer\">No Silver Bullet</a>,\" he separated the difficulty of software into two categories:  - fundamental to the problem, irreducible - and  - imposed by our tools, languages, and processes. </p><p>Brooks argued that no tool would deliver an order-of-magnitude improvement because most of programming's difficulty is essential. But he also argued that accidental complexity was the  part amenable to radical improvement.</p><p>Rich Hickey picked up that thread and built a programming language around it (Clojure).</p><p>In his 2011 talk \"<a href=\"https://www.youtube.com/watch?v=SxdOUGdseq4&amp;ref=felixbarbalet.com\" rel=\"noreferrer\">Simple Made Easy</a>,\" Hickey drew a distinction that the industry has spent fifteen years : the difference between  (objectively unentangled, not braided together) and  (familiar, near to hand, comfortable). The industry systematically confuses the two. We choose languages because they're easy - because the syntax looks familiar, because we can find developers on LinkedIn, because there are ten thousand Stack Overflow answers for every error message. </p><p>Not many people choose languages because they're simple.</p><p>Hickey's word for accidental complexity is \"incidental.\" As he puts it: \"Incidental is Latin for .\"</p><p>He catalogued the sources with uncomfortable precision. State complects everything it touches. Objects complect state, identity, and value. Methods complect function and state. Syntax complects meaning and order. Inheritance complects types. Every one of these entanglements is a source of accidental complexity that has nothing to do with the problem you're trying to solve.</p><p>Clojure was designed to avoid these entanglements. Immutable data by default. Plain maps instead of class hierarchies. Functions instead of methods. Composition instead of inheritance. It was, and is, a language that optimises for simplicity over ease.</p><p>For fifteen years, the response has been: \"Sure, but the learning curve.\", or \"Sure, but we can't hire Clojure developers, it's too niche.\"</p><p>And there it is. The objections that no longer hold.</p><h2>The learning curve is dead</h2><p>Nathan Marz <a href=\"https://x.com/nathanmarz/status/2022035827103347124?ref=felixbarbalet.com\" rel=\"noreferrer\">recently described building a complex distributed system</a> with Claude Code using Rama, a Clojure framework. Claude absorbed the framework's patterns through a few corrections and some documentation, and then wrote load modules, transaction handlers, and query topologies fluently. </p><p>Marz's conclusion is worth reading carefully:</p><blockquote>\"If AI can absorb a framework's semantics quickly, then the right framework to choose is the one with the best actual abstractions - the one that eliminates the most accidental complexity - regardless of how 'easy to learn' it is for a human picking it up on a weekend. Developer familiarity stops being the dominant selection criterion.\"</blockquote><p>Read that again. <em>Developer familiarity stops being the dominant selection criterion.</em></p><p>Wes McKinney - the creator of pandas, a developer who knows something about language ecosystems - demonstrates this from the other direction. He writes in his recent essay \"<a href=\"https://wesmckinney.com/blog/mythical-agent-month/?ref=felixbarbalet.com\" rel=\"noreferrer\">The Mythical Agent-Month</a>\" that he \"basically does not write code anymore, and now writes tons of code in a language (Go) I have never written by hand.\"</p><p>The barrier to entry for all languages has collapsed. An LLM doesn't look at Clojure's parentheses and feel intimidated. It doesn't need a weekend tutorial. It doesn't care whether the syntax resembles what it learned in university. The \"easy\" axis - familiarity, comfort, prior experience - has been zeroed out.</p><p>What remains is the \"simple\" axis. <strong>The intrinsic quality of the abstractions.</strong></p><p>Thinking like an economist: the learning curve was always a switching cost, not a measure of the language's value. It's easy to confuse the price of entry with the value of the asset. Now, LLMs have driven that switching cost toward zero. What's left is the underlying return on investment - and that's where Clojure was built to compete.</p><p>McKinney's essay contains what I think is the most important observation about LLM-assisted development written so far:</p><blockquote>\"I am already dealing with this problem as I begin to reach the 100 KLOC mark and watch the agents begin to chase their own tails and contextually choke on the bloated codebases they have generated.\"</blockquote><p>He calls this \"technical debt on an unprecedented scale, accrued at machine speed.\"</p><p>Stop me if you've heard this one before. Systems grow and age, they accrete, they accumulate stuff. The accidental complexity compounds until the codebase becomes too large and too tangled for anyone (human or machine) to navigate effectively. I described this at Qantas as a problem of coordination overhead and context-switching costs. McKinney is describing the same phenomenon, accelerated by an order of magnitude.</p><p>The mechanism is straightforward. LLMs are, as McKinney puts it, \"probably the most powerful tool ever created to tackle accidental complexity.\" They can refactor, write tests, clean up messes. But they also  new accidental complexity as a byproduct: \"large amounts of defensive boilerplate that is rarely needed in real-world use,\" \"overwrought solutions to problems when a simple solution would do just fine.\"</p><p>Brooks predicted this. His \"No Silver Bullet\" argument is that agents are brilliant at accidental complexity but struggle with essential design problems - and worse, they can't reliably tell the difference. They attack the accidental complexity with extraordinary capability while simultaneously producing more of it.</p><p>This is where language choice becomes a <strong>capital allocation decision</strong> with compounding returns. The brownfield barrier isn't about whether an LLM  write Python or Go or JavaScript - of course it can. It's about what happens at scale. The cost of a language choice isn't visible in the first ten thousand lines. It's visible at a hundred thousand, when the compounding effects of accidental complexity become the dominant cost. </p><p>Classic economics where marginal cost curves that look flat early and then inflect sharply.</p><h2>Why Clojure pushes the barrier further</h2><p>Clojure attacks this \"brownfield barrier\" from multiple directions simultaneously, and the effects compound.</p><p><a href=\"https://martinalderson.com/posts/which-programming-languages-are-most-token-efficient/?ref=felixbarbalet.com\" rel=\"noreferrer\">Martin Alderson's analysis</a> of Rosetta Code tasks across nineteen popular languages found Clojure to be the most token-efficient. Not by a trivial margin:</p><p>These aren't obscure comparisons. Python, JavaScript, and Java are the three most used languages in the world. Clojure expresses the same logic in roughly a fifth fewer tokens than Python and a third fewer than JavaScript or Java.</p><p>Why does this matter? Because context windows are a hard constraint, and they degrade non-linearly. <a href=\"https://arxiv.org/abs/2307.03172?ref=felixbarbalet.com\" rel=\"noreferrer\">Research from Stanford and Berkeley</a> shows that LLM performance drops by more than 30% when relevant information falls in the middle of the context window. Factory.ai <a href=\"https://factory.ai/news/context-window-problem?ref=felixbarbalet.com\" rel=\"noreferrer\">found</a> that models claiming 200,000 tokens of context become unreliable around 130,000 - a sharp cliff, not a gentle slope. Anthropic <a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents?ref=felixbarbalet.com\" rel=\"noreferrer\">describes</a> context engineering as a first-class discipline, noting that \"structured data like code consumes disproportionately more tokens.\"</p><p>If 80% of a coding agent's context window is code - reads, edits, diffs - then Clojure's 19% token advantage over Python translates to roughly 15% more room for actual problem context. Against JavaScript or Java, it's nearly 30% more room. Over a long session with multiple file reads and iterative edits, this compounds. The agent that runs out of useful context first loses.</p><p>And these are just the token-level numbers. At the program level, the difference is starker. Anthony Marcar at WalmartLabs <a href=\"https://devm.io/java/clojure-alternative-java-169315?ref=felixbarbalet.com\" rel=\"noreferrer\">reported that</a> \"Clojure shrinks our code base to about one-fifth the size it would be if we had written in Java.\" A fifth. McKinney's 100 KLOC brownfield barrier in Go could be structurally unreachable in Clojure - not because the agent is smarter, but because there's less accidental complexity for it to choke on.</p><h3><strong>Immutability eliminates defensive boilerplate</strong></h3><p>McKinney specifically identifies that agents \"tend to introduce unnecessary complexity, generating large amounts of defensive boilerplate.\" Null checks. Defensive copies. Synchronisation guards. Clojure's immutable data structures eliminate entire categories of this bloat. The agent literally cannot generate certain kinds of accidental complexity because the language makes it unnecessary.</p><p>As Hickey puts it: \"Values support reproducible results. If you define a function in terms of values, every time you call that function with the same values, will you get the same answer? Yes.\"</p><p>An LLM reasoning about immutable code doesn't need to track  a variable was modified or . It can reason algebraically: this function takes X and returns Y. Full stop. No temporal reasoning required. That's fewer balls to juggle - and as Hickey reminds us, even the best juggler in the world maxes out at about twelve.</p><p>Stuart Halloway made this point devastatingly in his talk \"<a href=\"https://www.youtube.com/watch?v=Qx0-pViyIDU&amp;ref=felixbarbalet.com\" rel=\"noreferrer\">Running With Scissors.</a>\" When you use typed structs or classes, \"all of your data manipulation scissors are gone. You do not have generic data any more. Each one of these structs requires its own custom special scissors to manipulate it.\"</p><p>With Clojure's maps, the LLM learns one toolkit - , , , ,  - that works on  data. With an object-oriented language, the LLM must learn a different API for every class. That's the difference between O(n) and O(n^2) in what the agent must hold in context. As the codebase grows, this gap widens.</p><h3><strong>The REPL closes the feedback loop</strong></h3><p>Halloway's formulation is the best I've seen: \"REPL + Functional = faster bricks. Things that you understand how they are going to work. They always work the same way, and you can compose them to build your system.\" And the dark corollary: \"REPL + imperative = faster spaghetti. If you are a net negative producing developer and we speed you up... we have just made things worse.\"</p><p>An LLM agent at a Clojure REPL can evaluate any expression in the running system, inspect the result, and adjust. No compilation step. No build system. No waiting. The feedback loop is as tight as it gets.</p><p>I should note that the major coding agents today - Claude Code, Codex, Cursor - don't use REPLs. They use file-edit, compile-or-test, read-errors, iterate loops. The industry has implicitly chosen compiler-style feedback. This is worth engaging with honestly.</p><p>But the evidence is more nuanced than it appears. Research on <a href=\"https://genai-evaluation-kdd2024.github.io/genai-evalution-kdd2024/assets/papers/GenAI_Evaluation_KDD2024_paper_25.pdf?ref=felixbarbalet.com\" rel=\"noreferrer\">CodePatchLLM (KDD 2024)</a> found that compiler feedback improves Java and Kotlin code generation by 45% - but provides  improvement for Python, because there's no compiler feedback to give. Dynamic languages get nothing from the compile loop. Replit Agent, <a href=\"https://blog.replit.com/automated-self-testing?ref=felixbarbalet.com\" rel=\"noreferrer\">notably</a>,  use a REPL-based verification system and reports results three times faster and ten times cheaper than previous approaches.</p><p>And Halloway's distinction cuts precisely here. A Python or JavaScript REPL creates exactly the temporal coupling problem that critics identify - mutable state accumulating in the session, order-dependent evaluation, \"faster spaghetti.\" Clojure's REPL evaluates expressions that return immutable values. Data in, data out. No temporal coupling. The REPL provides richer feedback than a compiler - actual return values, not just \"compiled\" or \"didn't\" - while Clojure's immutability means it doesn't create the stateful mess that imperative REPLs do. Clojure-MCP bridges the remaining gap: the agent writes to files and validates in the REPL. Bille reported tasks completing in hours instead of days.</p><p>There's a revealing irony buried in the data. McKinney chose Go for his new projects - a language famous for its simplicity. He writes it via LLM agents and hits the brownfield barrier at 100 KLOC.</p><p>But Go's simplicity is an  simplicity in Hickey's sense. It's familiar. It's readable. You can hire for it. It achieves this through verbosity: explicit error handling on every function call, no generics until recently, no macros, no metaprogramming. For human programmers, this verbosity is a feature - it makes code predictable and reviewable.</p><p>For LLM agents, it's a tax.</p><p>Alderson's data shows Go as one of the more token-inefficient popular languages. Every <code>if err != nil { return err }</code> consumes tokens that could be used for problem context. The language chosen for  simplicity creates  problems. Go is optimised for human-readable code; Clojure is optimised for expressing ideas with minimal ceremony. The LLM era rewards the latter.</p><p>There's a seductive counter argument here: that Go's verbosity actually  the model reason. Verbose output as chain-of-thought scaffolding - the same mechanism that helps LLMs solve maths problems. More tokens, more thinking.</p><p>It's wrong, and the architecture tells you why.</p><p>Modern reasoning models - o1, o3, Claude with extended thinking - do their reasoning in hidden tokens that are discarded after generation. The thinking has already happened before the model outputs a single character of code. Go's <code>if err != nil { return err }</code> is output tokens, not reasoning tokens. It doesn't expand the model's thinking budget. It spends the context budget.</p><p>The empirical evidence is decisive. Research <a href=\"https://proceedings.mlr.press/v267/liu25ah.html?ref=felixbarbalet.com\" rel=\"noreferrer\">presented at ICML 2025</a> found that generating code , then reasoning, yielded a 9.86% improvement over the traditional reason-then-code order. If verbose output were serving as reasoning scaffolding, the opposite should be true. The <a href=\"https://arxiv.org/abs/2512.08266?ref=felixbarbalet.com\" rel=\"noreferrer\">Token Sugar paper (ICSE 2025)</a> systematically compressed high-frequency verbose patterns - exactly the kind Go generates - and achieved up to 15.1% token reduction with near-identical correctness scores. If the boilerplate were contributing to correctness, removing it would degrade performance. It didn't.</p><p>Worse, context dilution research shows that repetitive, low-information tokens actively harm performance by diluting the model's finite attention budget - accuracy drops of 13.9 - 85%. </p><p>Every  repeated fifty times across a codebase isn't scaffolding. It's noise competing for the model's attention with the actual problem.</p><p>Let's assess some of the arguments against my thesis above - some of which are genuinely strong.</p><h3><strong>LLMs are measurably worse at Clojure</strong></h3><p>This is the big one. The <a href=\"https://arxiv.org/abs/2601.02060?ref=felixbarbalet.com\" rel=\"noreferrer\">FPEval benchmark</a> found that GPT-5 generates code with 94% imperative patterns in Scala, 88% in Haskell, and 80% in OCaml. LLMs don't just write worse functional code - they write imperative code  as functional code, and the prevalence of non-idiomatic patterns actually  alongside gains in functional correctness. <a href=\"https://jackpal.github.io/2025/02/03/Solving_AoC_Multiple_Languages.html?ref=felixbarbalet.com\" rel=\"noreferrer\">Jack Palvich's Gemini experiments</a> across twenty-four languages found that \"the Lisps suffer from paren mis-matches and mistakes using standard library functions.\" The <a href=\"https://github.com/nuprl/MultiPL-E?ref=felixbarbalet.com\" rel=\"noreferrer\">MultiPL-E benchmark</a> shows performance correlating with language popularity. And the \"<a href=\"https://arxiv.org/abs/2503.17181?ref=felixbarbalet.com\" rel=\"noreferrer\">LLMs Love Python</a>\" paper found that models default to Python in 93-97% of language-agnostic problems.</p><p>This is real. I'm not going to pretend it isn't.</p><p>But notice what's actually being measured. These benchmarks measure whether the LLM can generate a  in language X. They don't measure whether the resulting  - the codebase at 50 or 100 KLOC - is maintainable, navigable, or tractable for future agent sessions. \"Better at generating Python\" and \"Python generates better systems\" are different claims.</p><p>And the FPEval result is, if you squint, actually evidence  the thesis. If LLMs default to imperative patterns even when writing in functional languages, then the language's  matter more, not less. Clojure's immutability isn't a suggestion - it's a default. The language itself acts as a guardrail. An LLM generating Clojure has fewer ways to produce the kind of stateful, tangled code that compounds into the brownfield barrier. You can't mutate what the language won't let you mutate.</p><p>The parenthesis problem is real but solvable. Julien Bille <a href=\"https://medium.com/@_jba/my-experience-with-cursor-and-clojure-mcp-6e323b90a6f3?ref=felixbarbalet.com\" rel=\"noreferrer\">documented</a> his experience with Clojure-MCP: initially \"simple things took way too long\" and the AI was \"unable to get parentheses right.\" But after integrating s-expression-aware tooling, \"the agent experience got much better\" and \"it goes a LOT faster to write good code solutions.\" The parenthesis issue is a tooling gap, not a fundamental limitation.</p><p>And the training data argument is about the , not the . Models are improving rapidly. The accidental complexity argument is about permanent properties of the language. One is a snapshot; the other is a trajectory.</p><p>And the snapshot is less damning than it looks. <a href=\"https://arxiv.org/abs/2208.08227?ref=felixbarbalet.com\" rel=\"noreferrer\">Cassano et al.'s MultiPL-E study (IEEE TSE, 2023)</a> found that model perplexity - how uncertain the model is when predicting the next token - is not strongly correlated with the correctness of generated code. Codex's perplexity (uncertainty) was highest for JavaScript and TypeScript, yet it performed best on those languages. Some niche languages performed as well as popular ones. Training data volume is not the determinant the gravity well argument assumes.</p><p><a href=\"https://arxiv.org/abs/2308.09895?ref=felixbarbalet.com\" rel=\"noreferrer\">MultiPL-T (OOPSLA, 2024)</a> went further: fine-tuning on automatically translated data closed the gap entirely. Lua exceeded base Python performance after targeted fine-tuning. Julia saw 67% relative improvement. The gap isn't a permanent feature of the landscape - it's bridgeable engineering.</p><p>There's also the question of cross-lingual transfer. Research on scaling laws for code found that training on one language improves performance on related languages. Clojure sits on the JVM. The massive Java training corpus isn't irrelevant - it's a shared ecosystem, shared libraries, shared concepts. </p><h3><strong>Static type systems provide a feedback loop Clojure lacks</strong></h3><p>Also strong. Research <a href=\"https://arxiv.org/abs/2504.09246?ref=felixbarbalet.com\" rel=\"noreferrer\">from ETH Zurich (PLDI 2025)</a> shows that type-constrained decoding reduces compilation errors by more than half and increases functional correctness by 3.5-5.5%. TypeScript advocates report 90% reductions in certain bug categories. Rust's strict compiler creates tight generate-compile-fix loops.</p><p>I'll grant it: types help LLMs get individual functions right. The evidence is clear.</p><p>But types also create coupling. As Hickey argues: \"Statically typed languages yield much more heavily coupled systems. Flowing type information is a major source of coupling in programs.\" Types help the LLM write correct function A. But they also create structural dependencies between A, B, C, and D that make the  harder to reason about as it grows. The question is which effect dominates at scale - and McKinney's brownfield barrier suggests that system-level coupling is the bigger problem.</p><p>Clojure offers a middle path. Spec and <a href=\"https://github.com/metosin/malli?ref=felixbarbalet.com\" rel=\"noreferrer\">Malli</a> provide optional schema validation - type-like constraints when you want them, without the token overhead and coupling when you don't. And the REPL provides a runtime feedback loop that is arguably faster than a compilation cycle: the agent evaluates an expression, sees the result or the error, and corrects immediately.</p><p>This is how I'm leveraging Clojure (and Polylith) while I'm building AXONLORE - components with Malli function schema on every interface, enforced at testing and development time.</p><p>It's also worth noting Alderson's data: Haskell and F#, typed languages with strong inference, are nearly as token-efficient as Clojure. If the type system feedback loop is your priority, those are better choices than TypeScript or Rust, both of which are significantly more token-heavy. But Haskell and F# have their own ecosystem and adoption challenges. There's no free lunch.</p><h3><strong>The ecosystem is small and hiring is hard</strong></h3><p>This is the objection I've spent a decade fielding, and it cuts differently now. If developers aren't writing code by hand, \"knowing Clojure\" matters less than having good design taste - which McKinney identifies as the scarce resource: \"Design talent and good taste are the most scarce resources, and now with agents doing all of the coding labor, I argue that these skills matter more now than ever.\"</p><p>The hiring bottleneck shifts from language fluency to architectural judgement. Clojure developers tend to be more senior and more experienced. That's exactly the profile McKinney says will thrive.</p><p>And on ecosystem: Clojure has access to the entire JVM ecosystem through Java interop. The \"small ecosystem\" argument was always about discoverability for humans - and LLMs don't need Stack Overflow. </p><p>There's one more structural advantage worth noting. Hickey argued in his talk \"<a href=\"https://www.youtube.com/watch?v=oyLBGkS5ICk&amp;ref=felixbarbalet.com\" rel=\"noreferrer\">Spec-ulation</a>\" that \"dependency hell is not a different thing than mutability hell. It IS mutability hell. It is just at this scale.\"</p><p>LLMs are trained on vast codebases. Breaking changes in a language ecosystem mean that the training data contains conflicting information about the same names.  has meant the same thing for seventeen years. Compare that with Python 2 versus 3, React class components versus hooks versus server components, Angular.js versus Angular, or JavaScript's shifting parade of module systems.</p><p>Stability means consistent training signal. Consistent signal means more reliable output. This isn't a flashy advantage, but it's a durable one. When an LLM generates Clojure, it's drawing on seventeen years of consistent semantics. When it generates React, it's navigating a minefield of deprecated patterns, version-specific APIs, and conflicting idioms from different eras of the framework.</p><p>Erik Bernhardsson built a tool called <a href=\"https://github.com/erikbern/git-of-theseus?ref=felixbarbalet.com\" rel=\"noreferrer\">Git of Theseus</a> - after the philosophical paradoxabout the ship whose planks are replaced one by one until nothing original remains. Run it<p>against a Git repository and it shows you what percentage of each year's code survives into</p>the present. The half-life of a line of code in Angular is 0.32 years. In Rails, 2.43<p>years. In Linux, 6.6 years. Linux's longevity, Bernhardsson notes, comes from its</p>modularity - drivers and architecture support scale linearly because they have well-defined<p>interfaces. Each marginal feature takes roughly the same amount of code. Bad projects, on the other hand, scale superlinearly - every marginal feature takes more and more code.</p></p><p>Rich Hickey published code retention charts for Clojure in his ACM paper \"<a href=\"https://docdrop.org/download_annotation_doc/3386321-trk2f.pdf?ref=felixbarbalet.com\" rel=\"noreferrer\">AHistory of Clojure.</a>\" The Clojure chart is nearly flat - almost all code from every releasesurvives into the current version. </p><p>For an LLM, this is the difference between signal and noise. Every breaking change in alanguage's history creates conflicting training data - the same function name meaning<p>different things in different eras. Every renamed API, every deprecated pattern, every</p>framework migration is a source of confusion that the model must navigate<p>probabilistically. Clojure's stability means the probability mass is concentrated. There's</p>one way to use map, one way to use assoc, and that's been true since 2007. The model<p>doesn't have to guess which era of the language it's generating for.</p></p><p>I'm not arguing that Clojure is perfect. I'm arguing that the selection criteria have changed, and we haven't updated our decision-making frameworks to match.</p><p>The industry has - until now - selected languages for human convenience: familiar syntax, large hiring pools, abundant tutorials, massive ecosystems of libraries with thousands of GitHub stars. These were rational criteria when humans wrote the code. They optimised for the dominant constraint.</p><p>But the dominant constraint has shifted. Humans increasingly don't write the code. Machines do. And machines have different constraints: context windows, token efficiency, the ability to reason about entangled state, the compounding cost of accidental complexity at scale.</p><p>The question you should ask is: what's the time horizon?</p><p>If you're building a prototype that needs to work next week, use Python. The LLM is better at it today, the ecosystem is massive, and the brownfield barrier is someone else's problem (perhaps future you?). This is the savings account - safe, familiar, reliable returns.</p><p>If you're building something you plan to maintain for five years, the calculation changes. The language that generates the most maintainable codebase - <strong>the one that produces the least accidental complexity per unit of work</strong>, that fits more meaning into fewer tokens, that constrains the agent away from its worst impulses - that's the language with the higher compounding return. Even if the individual function quality is lower today.</p><p>There's also an uncomfortable possibility lurking here: <strong>that the best language for LLMs might not be any existing language at all</strong>. Perhaps we'll see languages designed from scratch for machine cognition - token-efficient, structurally regular, with built-in verification. But if we're choosing among what exists today, the properties Hickey optimised for seventeen years ago - simplicity, immutability, data orientation, homoiconicity, stability - happen to be exactly what machines need.</p><p>There's an obvious outcome though, at least while humans still choose the tools. Developer preference, hiring committees, LinkedIn keyword searches - these are powerful forces, and they don't evaporate just because the code is being written by a machine. The industryhas spent decades optimising for human convenience, and switching costs are real. It's entirely possible we stick with the popular languages for another decade, not because they're the most efficient allocation of capital, but because the humans holding the cheque books are comfortable with them.</p><p>My bet is on the other outcome. An industry that chose languages for humans will eventually notice that the humans have left the keyboard. And when the constraint you optimised for no longer binds, the economics eventually catch up. They always do.</p>","contentLength":26310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rhmyf9/simple_made_inevitable_the_economics_of_language/"},{"title":"Segment Anything with One mouse click","url":"https://eranfeit.net/one-click-segment-anything-in-python-sam-vit-h/","date":1772309264,"author":"/u/Feitgemel","guid":363,"unread":true,"content":"<p>Last Updated on 30/01/2026 by Eran Feit</p><h2></h2><p><code>Segment Anything in Python lets you segment any object with a single click using SAM ViT-H, delivering three high-quality masks instantly.</code><code>In this tutorial, you‚Äôll set up the environment, load the checkpoint, click a point, and export overlays‚Äîclean, practical code included.</code><code>Whether you‚Äôre labeling datasets or prototyping, this one-click workflow is quick, reliable, and easy to reuse.</code></p><p>Segment Anything in Python builds on a powerful promptable segmentation pipeline: a ViT-H image encoder extracts features once, a lightweight prompt encoder turns your click into guidance, and a mask decoder returns multiple high-quality candidates. This tutorial shows the exact flow‚Äîload the checkpoint, set the image, provide a single positive point, and review three masks with scores‚Äîso you can pick the cleanest boundary without manual tracing.</p><p>Segment Anything in Python is also practical beyond demos: you‚Äôll learn how to avoid OpenCV headless conflicts, run on CPU/GPU/MPS, and export overlays for quick sharing. We also cover adding negative points to suppress spillover, saving binary masks for downstream tasks, and keeping your run reproducible with clear paths and model_type matching. Use it to bootstrap datasets, refine labels, or prototype segmentations in seconds.</p>\n\n\n\nFor a deeper dive into automatic mask creation from detections, see my post on <a href=\"https://eranfeit.net/yolov8-object-detection-with-jetson-nano-and-opencv/\">YOLOv8 object detection with Jetson Nano and OpenCV</a>.\n\n\n\n<p>üöÄ Want to get started with Computer Vision or take your skills to the next level ?</p><h2></h2><p>Create a conda environment, install PyTorch (CUDA optional), and add the key libraries: , , and .These steps make your runtime stable and reproducible.</p><p>You‚Äôre creating an isolated Python 3.9 environment, ensuring compatible PyTorch/CUDA, installing OpenCV + Matplotlib, and pulling SAM directly from the official repo.</p><div data-code-block-pro-font-family=\"Code-Pro-JetBrains-Mono\"><pre tabindex=\"0\"><code></code></pre></div><p> after this step, your machine is ready to run SAM and display interactive windows.</p><h2></h2><p>Import NumPy, PyTorch, Matplotlib, OpenCV, then add three tiny helpers to draw masks, points, and boxes.<p>These functions make SAM‚Äôs results easy to see.</p></p><p>You‚Äôll visualize the clicked point (green star), optional negatives (red), and overlay semi-transparent masks on the image.</p><div data-code-block-pro-font-family=\"Code-Pro-JetBrains-Mono\"><pre tabindex=\"0\"><code></code></pre></div><p> your visual overlays are ready‚Äîclicks and masks will be easy to inspect.</p>\n\n\n\nIf you prefer a full framework, check out <a href=\"https://eranfeit.net/detectron2-panoptic-segmentation-made-easy-for-beginners/\">Detectron2 panoptic segmentation made easy for beginners</a> for training-ready pipelines.\n\n\n\n<h2></h2><p>Load an image, open an OpenCV window, and  the object once.Press  to confirm and capture the coordinates.</p><p>You‚Äôll build a tiny helper function that returns the (x, y) coordinates of your click‚ÄîSAM‚Äôs only required input in this flow.</p><div data-code-block-pro-font-family=\"Code-Pro-JetBrains-Mono\"><pre tabindex=\"0\"><code></code></pre></div><p> you now have a single (x, y) pointing to the object‚ÄîSAM will do the rest.</p>\n\n\n\nWant point-based interaction in videos? See <a href=\"https://eranfeit.net/segment-anything-python-no-training-image-masks/\">Segment Anything in Python ‚Äî no training, instant masks</a> for more live demos.\n\n\n\n<h2></h2><p>Load the SAM checkpoint (ViT-H), move it to GPU if available, and attach a .Then set the current image so SAM can compute features.</p><p>This step binds the model + image together and readies the predictor for your single click.</p><div data-code-block-pro-font-family=\"Code-Pro-JetBrains-Mono\"><pre tabindex=\"0\"><code></code></pre></div><p> SAM is loaded, on the right device, and primed with your image.</p>\n\n\n\nIf you‚Äôre exploring medical or structured masks, compare with <a href=\"https://eranfeit.net/u-net-medical-segmentation-with-tensorflow-and-keras-polyp-segmentation/\">U-Net medical segmentation with TensorFlow &amp; Keras</a>.\n\n\n\n<h2></h2><p>Turn your (x, y) into SAM inputs, get , show them, and save each result.You‚Äôll see mask scores to help you pick your favorite.</p><p>You‚Äôll get three high-quality segmentations and PNGs saved to disk for later use.</p><div data-code-block-pro-font-family=\"Code-Pro-JetBrains-Mono\"><pre tabindex=\"0\"><code></code></pre></div><p> you now have three crisp segmentations saved‚Äîchoose the best and keep creating.</p>\n\n\n\nNext, try improving mask quality with post-processing or super-resolution: <a href=\"https://eranfeit.net/upscale-your-images-and-videos-using-super-resolution/\">upscale your images and videos using super-resolution</a>.\n\n\n\n<div itemscope=\"\" itemtype=\"https://schema.org/FAQPage\"><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">SAM is a general-purpose segmentation model that returns object masks from simple prompts like a single click. It‚Äôs ideal for fast labeling and prototyping.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Use ViT-H for best quality. Use ViT-L/B for lower memory. Match model_type to your checkpoint name.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">No, but GPU or Apple MPS speeds up inference significantly. CPU works, just slower.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Compare the three candidates by score and visual quality. Choose the one that cleanly captures your object.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Yes. Label 0 for background to suppress unwanted regions. Mix positives and negatives for precision.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Use opencv-python (GUI) instead of the headless build. The post includes a cleanup step.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Anywhere. Update the code‚Äôs path_for_sam_model to match your file location.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Yes. The code saves overlay images. You can also save binary masks by converting to 0/255 and writing with OpenCV.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Yes. SAM accepts points and bounding boxes. Boxes help guide segmentation when objects are crowded.</p></div></div><div itemscope=\"\" itemprop=\"mainEntity\" itemtype=\"https://schema.org/Question\"><h3 itemprop=\"name\"></h3><div itemscope=\"\" itemprop=\"acceptedAnswer\" itemtype=\"https://schema.org/Answer\"><p itemprop=\"text\">Absolutely. One-click masks are a quick way to bootstrap datasets or refine labels with minimal effort.</p></div></div></div><p>You‚Äôve just built a complete  tool around  in Python.The workflow is intentionally lightweight: create an environment, install SAM, click a point, and export masks.<p>Because SAM generalizes broadly, it‚Äôs excellent for new domains where you don‚Äôt have labeled data yet.</p>From here, you can add negative clicks for refinement, use bounding boxes, or integrate with super-resolution and post-processing to lift mask quality even further.</p><p>If you plan to use this in production, consider wrapping the flow in a small GUI, storing your clicks/masks, and adding batch processing for entire image sets.For research, this pipeline is a fantastic way to prototype and compare segmentations across different scenes quickly.</p>","contentLength":5455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rhckl3/segment_anything_with_one_mouse_click/"},{"title":"A Rabbit Hole Called WebGL (8-part series on the technical background of a WebGL application w/ functional demo)","url":"https://www.hendrik-erz.de/post/a-rabbit-hole-called-webgl","date":1772306089,"author":"/u/nathan_lesage","guid":365,"unread":true,"content":"<p>I have this tradition. At least, it appears like a tradition, because it happens with frightening regularity. Every one to two years, as Christmas draws close, I get this urge to do something new. In 2017, I released a tiny tool that has turned into one of the go-to solutions for hundreds of thousands of people to write, <a href=\"https://www.zettlr.com/\">Zettlr</a>. In 2019, I wrote my first <a href=\"https://github.com/nathanlesage/visualizrs\">Rust program</a>. In 2021, I did a <a href=\"https://www.hendrik-erz.de/post/analyse-koalitionsvertrag-2021-spd-grune-fdp-ampel\">large-scale analysis of the coalition agreement</a> of the German ‚ÄúTraffic light‚Äù government. During the pandemic, I built a bunch of mechanical keyboards (because  I did). In 2023, I didn‚Äôt really do much, but in 2024, I wrote a <a href=\"https://www.hendrik-erz.de/post/localchat-chat-with-an-ai-assistant-on-your-computer\">local LLM application</a>. So okay, it‚Äôs not necessarily every year, but if you search this website, you‚Äôll find many tiny projects that I used to distract myself from especially dire stretches in my PhD education.</p><p>Now, is it a good use of my time to spend it on some weird technical topics instead of doing political sociology? I emphatically say yes. If you are a knowledge-worker, you need to keep your muscles moving. Even as a researcher, if you do too much of the same thing, you become less of a knowledge-worker, and more of a secretary. Call it an artistic outlet, that just so happens to make my research job . The last time I had to think about wrong data structures in my analytical code or when running some linear regression was ‚Ä¶ let‚Äôs say a long time ago. The more I know about software and hardware, the more I can actually focus on my research questions when I turn to the next corpus of text data.</p><p>But alright, you didn‚Äôt click on this article because you wanted to hear me rationalize my questionable life choices, you want to read up on the next rabbit hole I fell into: OpenGL and WebGL. In the following, I want to walk you through the core aspects of what WebGL is and what you can do with it, what I actually did with it, and what the end result was. If you‚Äôre not into technical topics (which, given the history of articles here, I actually have to start to doubt at this point), <a href=\"https://nathanlesage.github.io/iris-indicator/\">click here to see the full glory of my recent escapade</a>.</p><blockquote><p>Note: In the following, I will skip over a lot of basics, and merely explain some interesting bits of the source code (<a href=\"https://github.com/nathanlesage/iris-indicator\">which you can find here</a>), central decisions I took, and things I learned. I don‚Äôt verbatim copy the entire code that you can find <a href=\"https://github.com/nathanlesage/iris-indicator\">in the repository</a>. The entire thing is still insanely long and will span multiple articles, even though I try to leave out a lot which you can learn via, e.g., <a href=\"https://webgl2fundamentals.org/\">WebGLFundamentals</a>, which I recommend you read to learn more.</p></blockquote><p>First, some context. At the end of 2024, <a href=\"https://github.com/Zettlr/Zettlr/issues/5414\">someone complained</a> that project exports in my app, Zettlr, were lacking any visual indication of their progress. As a quick primer: Zettlr uses Pandoc to convert Markdown to whichever format you choose. However, especially for long projects, exporting may take quite some time, during which the app looks as if it‚Äôs doing nothing. You can still work with the app, and do things, but it‚Äôs hard to know when Zettlr is actually done performing the project export. The biggest issue was less finding a way to just  users which background tasks are currently running, and more how to adequately visualize this to them. For quite a bit of time, my brain kept churning idea after idea in the search for a cool way to visualize ‚Äúsomething is happening in the background.‚Äù You can read up on many discussions that I‚Äôve had with Artem in the <a href=\"https://github.com/Zettlr/Zettlr/issues/5414\">corresponding issue</a> on the issue tracker.</p><p>Indeed, the task was quite massive, because the requirements were so odd:</p><ul><li>The indication should convey a sense of ‚Äúsomething is happening‚Äù without actually knowing the precise progress of the task being performed.</li><li>It should quickly and easily convey how many tasks are currently running in the background, and what their status is.</li><li>It should be so compact that it fits into a toolbar icon.</li><li>It should absolutely avoid giving people the impression that something might be stuck.</li></ul><p>At some point, I had my  moment: Why not produce an iris-like visualization? Intuitively, it ticked all the boxes: One can animate the picture to convey a sense of movement without looking like a run-of-the-mill loading spinner that we have collectively come to dread; by coloring its segments, one can include several ‚Äúthings‚Äù with different status; and by toggling between an ‚Äúon‚Äù- and ‚Äúoff‚Äù-state, one could indicate whether something is running, or not.</p><p>I currently suspect that my brain simple mangled together the circular appearance of a loading spinner and the <a href=\"https://www.3blue1brown.com/\">logo of 3Blue1Brown</a> into a contraption that would prove to be insanely difficult to create.</p><p>Because I wanted to convey a lot of subtle movement, I opted to choose WebGL to implement it, using all the fanciness of graphics processing. My thinking was as follows: I could combine something I‚Äôd have to do at some point anyway with something new to learn. I thought: ‚ÄúHow hard can it be to learn some shader programming on the side?‚Äù</p><p>‚Ä¶ well, if you‚Äôve read until here, you know that I was  so wrong with my estimate of how long it would take as this time. What started as a ‚Äúlet me hack something together in two Christmas afternoons‚Äù ended up being an almost two-week intensive endeavor that has had my partner get  mad at me for spending so much time in front of my computer.</p><p>But now, it is done, and I have succeeded in achieving exactly what I had imagined weeks ago. To salvage what I can, I am writing these lines to let you partake in my experience, and maybe you find understanding the guts of GPU-accelerated rendering on the web even intriguing!</p><p>On the page, there are four sections: Some settings, configuration for the segments, a frame counter, and the actual animation below that.</p><p>Let me guide you through the settings first:</p><ul><li>: This setting sets how long it takes for the indicator to rotate once around. By default it is set to 120 seconds, so two minutes, but you can turn it down to increase its speed. The minimum setting is 10 seconds which is quite fast.</li><li>: This setting determines how fast the individual rays will increase and shrink in size. It is pre-set to five seconds for one full movement, but you can turn it down to increase their speed. The minimum is 100ms, which is stupidly fast.</li><li>: This enables or disabled multi-sample antialiasing. If disabled, the animation can look very rugged and pixelated.</li><li>: This setting enables or disables the bloom effect which makes the entire indicator ‚Äúglow.‚Äù This can actually reduce the performance of the animation quite a bit, but it also has a great visual impact.</li><li>: This effectively allows you to determine how much blurring will be applied to the image. It is preset to 2√ó, which is a good default. You can reduce it to 1√ó which will make the effect more subtle. A setting of 8√ó may be a bit much, but I decided to leave it in since I feel it is instructive.</li><li>: This setting determines how detailed the resolution is. It is preset with whatever device pixel ratio your display has. If you‚Äôre opening the website on a modern phone or on a MacBook, it will probably be preset to 2√ó, but on other displays, it will be 1√ó. It has a moderate performance impact.</li><li><strong>Segment adjustment step duration</strong>: This setting determines how fast the segment colors adjust when you change the segment counts in the next section.</li></ul><p>The next section allows you to determine the segments that will be displayed. As a reminder: The whole intention of this project was to visualize the status of running tasks, which might be successful, unsuccessful, or still en route. You have four segments available, and can determine how many tasks are in each segment, alongside their color. The colors are hard-coded because this way I can ensure that they all fit and blend together well.</p><p>By default, the demonstration page will auto-simulate changes to the segments so that you don‚Äôt have to click around. When the simulation is active it will, each second, determine what to do. There is a 30% chance each that one of the first three segments will be incremented by one. Further, there is a 10% chance that the simulation will reset everything to zero and start again.</p><p>The last section includes settings for the frame rate. The frame rate simply means how often the entire animation will be re-drawn (hence, frames-per-second). At the top, it displays the current frame rate. The frame rate is bound to your display, so on a MacBook (which has a refresh rate of 120 Hz), the frame rate will be at most 120 frames per second. On my secondary display, the frame rate is 75 Hz.</p><p>By default, I have implemented a frame limit of at most 30 frames per second. This ensures that the animation is still smooth without being too demanding on your computer or phone. However, you can change the frame rate to, e.g., 60 fps. This will render the animation twice as frequently. Especially if you turn the rotation speed to the max, you actually want to increase the frame limit, because on 30 frames per second, it can indeed look very stuttery.</p><p>Feel free to play around with the settings to see how they change the animation. Again, you can also go through <a href=\"https://github.com/nathanlesage/iris-indicator\">the source code of the animation</a> to learn how it works.</p><h2>About This Article Series</h2><p>Over the next three months, I will publish one part per week on how I finally managed to achieve this feat. The logic behind it is very complex, and it takes a lot of research to understand how to achieve the various effects. The articles will be as follows:</p><p>In the next article, I will introduce you to WebGL, OpenGL, and how to set everything up to actually start doing things with WebGL. I will talk about the basic architectural decisions I took, and how code can be properly organized. I will also introduce you to OpenGL‚Äôs rendering pipeline, and how it works.</p><p>In article three, I will guide you to drawing the rays that make up the iris. You will learn about how to provide data to OpenGL, and how the drawing actually works.</p><p>In the fourth installment, I will talk through how to add two of the three animations that make up the iris: rotation, and the movement of the rays. This article almost exclusively focuses on JavaScript, and contains minimal changes to the shaders, because movement is mostly a thing of JavaScript.</p><p>In article five, I will introduce you to the algorithm I designed to both color the segments of the iris according to the number of running tasks, i.e., the main goal of the entire endeavor. I will also explain the final, third animation that the indicator includes: animating the colors of the iris.</p><p>This article will be more in-depth and explain another big part of OpenGL‚Äôs rendering pipeline. It explains how to enable a renderer to perform post-processing. It also adds one post-processing step: tone-mapping.</p><p>Article seven focuses on the centerpiece of the animation, the one big part that would not have been possible using other techniques such as SVG. I explain how to add a bloom post-processing step in between the ray rendering and the output, and how bloom actually works. (It‚Äôs surprisingly simple!)</p><h3>Adding Multi-Sample Antialiasing</h3><p>In the eight and final practical article in this series, I explain MSAA a bit more in detail, why it sometimes works, and sometimes doesn‚Äôt, and how to actually add it to the animation. I also explain the final piece of the OpenGL Rendering pipeline that you probably need to know to understand what is happening.</p><p>When I set out to create this animation, I imagined it would take me maybe two days ‚Äî nothing to write home about (literally). However, I was wrong, and, to the contrary, we are now looking towards an astonishing nine (!) articles just to explain what has happened here.</p><p>I found the journey extremely rewarding, even though it ate up my winter holidays. I want to let you partake in what I learned, and I hope you stick along for the ride.</p><p>So, please, come back next Friday for part two: Setting everything up!</p><p>Jump directly to an article that piques your interest.</p>","contentLength":11959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rhb89v/a_rabbit_hole_called_webgl_8part_series_on_the/"},{"title":"MQTT: The Protocol Behind Every Smart Device (Golang)","url":"https://youtu.be/S64crfW9tQU","date":1772305420,"author":"/u/huseyinbabal","guid":364,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rhaxyp/mqtt_the_protocol_behind_every_smart_device_golang/"},{"title":"Yes, and...","url":"https://htmx.org/essays/yes-and/","date":1772301689,"author":"/u/BinaryIgor","guid":367,"unread":true,"content":"<p>I teach computer science at <a rel=\"noopener\" target=\"_blank\" href=\"https://www.cs.montana.edu\">Montana State University</a>.  I am the father of three sons who\nall know I am a computer programmer and one of whom, at least, has expressed interest in the field.  I love computer\nprogramming and try to communicate that love to my sons, the students in my classes and anyone else who will listen.</p><p>A question I am increasingly getting from relatives, friends and students is:</p><blockquote><p>Given AI, should I still consider becoming a computer programmer?</p></blockquote><p>My response to this is: ‚ÄúYes, and‚Ä¶‚Äù</p><p>Computer programming is, fundamentally, about two things:</p><ul><li>Problem-solving using computers</li><li>Learning to control complexity while solving these problems</li></ul><p>I have a hard time imagining a future where knowing how to solve problems with computers and how to control the complexity\nof those solutions is  valuable than it is today, so I think it will continue to be a viable career even with the\nadvent of AI tools.</p><p>That being said, I view AI as very dangerous for junior programmers because it  able to effectively generate code for\nmany problems.  If a junior programmer does not learn to write code and simply generates it, they are robbing\nthemselves of the opportunity to develop the visceral understanding of code that comes with being down in the trenches.</p><p>Because of this, I warn my students:</p><p>‚ÄúYes, AI can generate the code for this assignment. Don‚Äôt let it. You  to write the code.‚Äù</p><p>I explain that, if they don‚Äôt write the code, they will not be able to effectively  the code.  The ability to\nread code is certainly going to be valuable, maybe  valuable, in an AI-based coding future.</p><p>I do not agree with this simile.</p><p>Compilers are, for the most part, deterministic in a way that current AI tools are not.  Given a high-level programming\nlanguage construct such as a for loop or if statement, you can, with reasonable certainty, say what the generated\nassembly will look like for a given computer architecture (at least pre-optimization).</p><p>The same cannot be said for an LLM-based solution to a particular prompt.</p><p>High level programming languages are a  way to create highly specified solutions to problems\nusing computers with a minimum of text in a way that assembly was not.  They eliminated a lot of\n<a rel=\"noopener\" target=\"_blank\" href=\"https://en.wikipedia.org/wiki/No_Silver_Bullet\">accidental complexity</a>, leaving (assuming the code was written\nreasonably well) mostly necessary complexity.</p><p>LLM generated code, on the other hand, often does not eliminate accidental complexity and, in fact, can add\nsignificant accidental complexity by choosing inappropriate approaches to problems, taking shortcuts, etc.</p><p>If you can‚Äôt read the code, how can you tell?</p><p>And if you want to read the code you must write the code.</p><p>Another thing that I tell my students is that AI, used properly, is a tremendously effective TA.  If you don‚Äôt use it\nas a code-generator but rather as a partner to help you understand concepts and techniques, it can provide a huge boost\nto your intellectual development.</p><p>One of the most difficult things when learning computer programming is getting ‚Äústuck‚Äù.  You just don‚Äôt see the trick\nor know where to even start well enough to make progress.</p><p>Even worse is when you get stuck due to accidental complexity: you don‚Äôt know how to work with a particular tool chain\nor even what a tool chain is.</p><p>This isn‚Äôt a problem with , this is a problem with your environment.  Getting stuck pointlessly robs you of time to\nactually be learning and often knocks people out of computer science.</p><p>(I got stuck trying to learn Unix on my own at Berkeley, which is one reason I dropped out of the computer science\nprogram there.)</p><p>AI can help you get past these roadblocks, and can be a great TA if used correctly.  I have posted an\n<a rel=\"noopener\" target=\"_blank\" href=\"https://gist.github.com/1cg/a6c6f2276a1fe5ee172282580a44a7ac\">AGENTS.md</a> file that I provide to my students to configure\ncoding agents to behave like a great TA, rather than a code generator, and I encourage them to use AI in this role.</p><p>AI doesn‚Äôt  to be a detriment to your ability to grow as a computer programmer, so long as it is used\nappropriately.</p><p>I do think AI is going to change computer programming.  Not as dramatically as some people think, but in some\nfundamental ways.</p><p>It may be that the  of coding will lose  value.</p><p>I regard this as too bad: I usually like the act of coding, it is fun to make something do something with your\n(metaphorical) bare hands.  There is an art and satisfaction to writing code well, and lots of aesthetic decisions to be\nmade doing it.</p><p>However, it does appear that raw code writing prowess may be less important in the future.</p><p>As this becomes relatively less important, it seems to me that other skills will become more important.</p><p>For example, the ability to write, think and communicate clearly, both with LLMs and humans seems likely to be much more\nimportant in the future.  Many computer programmers have a literary bent anyway, and this is a skill that will likely\nincrease in value over time and is worth working on.</p><p>Reading books and writing essays/blog posts seem like activities likely to help in this regard.</p><p>Another thing you can work on is turning some of your mental energy towards understanding a business (or government\nrole, etc) better.</p><p>Computer programming is about solving problems with computers and businesses have plenty of both of these.</p><p>Some business folks look at AI and say ‚ÄúGreat, we don‚Äôt need programmers!‚Äù, but it seems just as plausible to me that\na programmer might say ‚ÄúGreat, we don‚Äôt need business people!‚Äù</p><p>I think both of these views are short-sighted, but I do think that AI can give programmers the ability to continue\nfundamentally working as a programmer while  investing more time in understanding the real-world problems (business or\notherwise) that they are solving.</p><p>This dovetails well with improving communication skills.</p><p>Like many computer programmers, I am ambivalent towards the term ‚Äúsoftware architect.‚Äù  I have seen\n<a rel=\"noopener\" target=\"_blank\" href=\"https://www.joelonsoftware.com/2001/04/21/dont-let-architecture-astronauts-scare-you/\">architect astronauts</a> inflict\na lot of pain on the world.</p><p>For lack of a better term, however, I think software architecture will become a more important skill over time: the\nability to organize large software systems effectively and, crucially, to control the complexity of those systems.</p><p>A tough part of this for juniors is that traditionally the ability to architect larger solutions well has come from\nexperience building smaller parts of systems, first poorly then, over time, more effectively.</p><p>Most bad architects I have met were either bad coders or simply didn‚Äôt have much coding experience at all.</p><p>If you let AI take over as a code generator for the ‚Äúsimple‚Äù stuff, how are you going to develop the intuitions necessary\nto be an effective architect?</p><p>This is why, again, you must write the code.</p><p>Another skill that seems likely to increase in value (obviously) is knowing how to use LLMs effectively.  I think that\ncurrently we are still in the process of figuring out what that means.</p><p>I also think that what this means varies by experience level.</p><p>Senior programmers who already have a lot of experience from the pre-AI era are in a good spot to use LLMs effectively:\nthey know what ‚Äúgood‚Äù code looks like, they have experience with building larger systems and know what matters and\nwhat doesn‚Äôt.  The danger with senior programmers is that they stop programming entirely and start suffering from\n<a rel=\"noopener\" target=\"_blank\" href=\"https://www.media.mit.edu/publications/your-brain-on-chatgpt/\">brain rot</a>.</p><p>Particularly dangerous is firing off prompts and then getting sucked into\n<a rel=\"noopener\" target=\"_blank\" href=\"https://theneverendingstory.fandom.com/wiki/The_Nothing\">The Eternal Scroll</a> while waiting.</p><p>I typically try to use LLMs in the following way:</p><ul><li>To analyze existing code to better understand it and find issues and inconsistencies in it</li><li>To help organize my thoughts for larger projects I want to take on</li><li>To generate relatively small bits of code for systems I am working on</li><li>To generate code that I don‚Äôt enjoy writing (e.g. regular expressions &amp; CSS)</li><li>To generate demos/exploratory code that I am willing to throw away or don‚Äôt intend to maintain deeply</li><li>To suggest tests for a particular feature I am working on</li></ul><p>I try not to use LLMs to generate full solutions that I am going to need to support.  I will sometimes use LLMs alongside\nmy manual coding as I build out a solution to help me understand APIs and my options while coding.</p><p>I never let LLMs design the APIs to the systems I am building.</p><p>Juniors are in a tougher spot.  I will say it again: you must write the code.</p><p>The temptation to vibe your way through problems is very, very high, but you will need to fight against that temptation.</p><p>Peers  be vibing their way through things and that will be annoying: you will need to work harder than they do,\nand you may be criticized for being slow.  The work dynamics here are important to understand: if your company\nprioritizes speed over understanding (as many are currently) you need to accept that and not get fired.</p><p>However, I think that this is a temporary situation and that soon companies are going to realize that vibe coding at\nspeed suffers from worse complexity explosion issues than well understood, deliberate coding does.</p><p>At that point I expect slower, more deliberate coding with AI assistance will be understood as the best way to utilize\nthis new technology.</p><p>Where AI  help juniors is in accelerating the road to senior developer by eliminating accidental complexity that often\ntrips juniors up.  As I said above, viewing AI as a useful although sometimes overly-eager helper rather than a servant\ncan be very effective in understanding the shape of code bases, what the APIs and techniques available for a particular\nproblem are, how a given build system or programming language works, etc.</p><p>But you must write the code.</p><p>And companies: you must let juniors write the code.</p><p>The questions I get around AI and programming fundamentally revolve around getting a decent job.</p><p>It is no secret that the programmer job market is bad right now, and I am seeing good CS students struggle to find\npositions programming.</p><p>While I do not have a crystal ball, I believe this is a temporary rather than permanent situation.  The computer\nprogrammer job market tends to be cyclical with booms and busts, and I believe we will recover from the current bust\nat some point.</p><p>That‚Äôs cold comfort to someone looking for a job now, however, so I want to offer the specific job-seeking advice that\nI give to my students.</p><p>I view the online job sites as mostly pointless, especially for juniors.  They are a lottery and the chances of finding\na good job through them are low.  Since they are free they are probably still worth using, but they are not worth\ninvesting a lot of time in.</p><p>A better approach is the four F‚Äôs: Family, Friends &amp; Family of Friends.  Use your personal connections to find positions\nat companies in which you have a competitive advantage of knowing people in the company.  Family is the strongest\npossibility.  Friends are often good too.  Family of friends is weaker, but also worth asking about.  If you know or\nare only a few degrees separated from someone at a company you have a much stronger chance of getting a job at that\ncompany.</p><p>I stress to many students that this doesn‚Äôt mean your family has to work for Google or some other big tech company.</p><p> companies of any significant size have problems that need to be solved using computers.  Almost every company over 100\npeople has some sort of development group, even if they don‚Äôt call it that.</p><p>As an example, I had a student who was struggling to find a job.  I asked what their parent did, and they said they worked\nfor Costco corporate.</p><p>I told them that they were in fact extremely lucky and that this was their ticket into a great company.</p><p>Maybe they don‚Äôt start as a ‚Äúcomputer programmer‚Äù there, maybe they start as an analyst or some other role.  But the\nability to program on top of that role will be very valuable and likely set up a great career.</p><p>So I still think pursuing computer programming as a career is a good idea.  The current job market is bad, no doubt, but\nI think this is temporary.</p><p>I do think how computer programming is done is changing, and programmers should look at building up skills beyond\n‚Äúpure‚Äù code-writing.  This has always been a good idea.</p><p>I don‚Äôt think programming is changing as dramatically as some people claim and I think the fundamentals of programming,\nparticularly writing good code and controlling complexity, will be perennially important.</p><p>I hope this essay is useful in answering that question, especially for junior programmers, and helps people feel\nmore confident entering a career that I have found very rewarding and expect to continue to do for a long time.</p><p>And companies: let the juniors write at least some of the code.  It is in your interest.</p>","contentLength":12488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1rh9bnw/yes_and/"}],"tags":["dev"]}