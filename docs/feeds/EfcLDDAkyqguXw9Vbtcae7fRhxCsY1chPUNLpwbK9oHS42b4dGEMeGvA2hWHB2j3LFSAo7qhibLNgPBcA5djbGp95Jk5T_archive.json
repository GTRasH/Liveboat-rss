{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Understanding Multi-Platform Docker Builds with QEMU","url":"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/","date":1762024493,"author":"/u/Helpful_Geologist430","guid":617,"unread":true,"content":"<p>One intriguing feature of containers and images is their multi-platform support. Docker uses Buildx, which is based on BuildKit, to enable multi-platform builds.</p><p>The old way (build for the current host’s platform, if you’re on an ARM CPU, you build an ARM image that won’t run on x86, and vice versa):</p><p>The new way: without a single care, build a multi-platform image that supports both x86 and ARM (and others):</p><div><div><code><table><tbody><tr><td><pre>docker buildx build  linux/amd64,linux/arm64 </pre></td></tr></tbody></table></code></div></div><p>How is this sorcery possible? Let’s take a look.</p><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#but-first-what-are-containers\"></a></h2><p>Containers, under the hood, are simply processes isolated thanks to <a href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\">Linux’s namespaces</a>. The executables and files of these processes, packaged in layers, are compiled for a specific architecture.</p><p>Put differently, a container is a bundled runtime. This is what the <a href=\"https://specs.opencontainers.org/runtime-spec/runtime/?v=v1.0.2\">OCI runtime bundle</a> defines:</p><div><div><code><table><tbody><tr><td><pre>coolcontainer/\n├── config.json\n└── rootfs/\n    ├── bin/\n    ├── lib/\n    └── ...\n</pre></td></tr></tbody></table></code></div></div><p>An OCI runtime bundle (used to start a container) is obtained from an OCI image (Docker images are OCI-compliant).</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n</pre></td><td><pre>apt  umoci skopeo runc\n\n\nskopeo copy docker://alpine:latest oci:alpine:latest\n\nalpine/\n\numoci unpack  alpine:latest alpine-runtime-bundle\nalpine-runtime-bundle/\n\nrunc run  alpine-runtime-bundle mycoco\n    yo  /home/greeting\n    alpine-runtime-bundle/rootfs/home/greeting\n</pre></td></tr></tbody></table></code></div></div><p>This spec defines what’s needed to run a container. All OCI-compliant container solutions adhere to it (Docker, Podman, etc.). These files are then used to create a container process. The isolation is achieved through Linux namespaces. To the container, it feels like it’s running on its own filesystem, network, PID space, and so on, but in reality, it’s just a process, albeit a well-isolated one.</p><p>The reference implementation that takes an OCI runtime bundle and starts a container is  (Docker uses it under the hood). It takes all the information and layers in the bundle and creates the container process. Mounts, environment variables, and all kinds of options that can be specified when running a container are handled by .</p><p>This means that the executables and binaries are destined for a specific OS and architecture:</p><div><div><code><table><tbody><tr><td><pre>file  alpine-runtime-bundle/rootfs/bin/ls\nalpine-runtime-bundle/rootfs/bin/ls: ELF 64-bit LSB executable, ARM aarch64\n</pre></td></tr></tbody></table></code></div></div><p>So the  command inside the container layers is simply a regular executable built for ARM64.</p><p>You can’t just run an image built for an x86 CPU on an ARM CPU (out of the box). That’s where multi-platform images come into the picture.</p><p>An image that supports multiple architectures? Say what?</p><div><div><code><table><tbody><tr><td><pre>skopeo inspect  docker://docker.io/library/ubuntu:latest | jq | \n        ...\n</pre></td></tr></tbody></table></code></div></div><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n</pre></td><td><pre>\nskopeo copy  amd64  linux \n  docker://docker.io/library/nginx:latest \n  oci:nginx-amd64:latest\n\numoci unpack  nginx-amd64:latest nginx-amd64-runtime-bundle\n\n\nfile  nginx-amd64-runtime-bundle/rootfs/bin/ls\n</pre></td></tr></tbody></table></code></div></div><p>J’accuse! Intruder! An x86-64 binary on an ARM machine!</p><div><div><code><table><tbody><tr><td><pre>./alpine-runtime-bundle/rootfs/bin/ls\n\n./nginx-amd64-runtime-bundle/rootfs/bin/ls\n\nrunc run  nginx-amd64-runtime-bundle my-nginx-container\n</pre></td></tr></tbody></table></code></div></div><p>And that’s the heart of the problem when it comes to running containers across different platforms. What to do?</p><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#qemu-and-binfmt-misc-to-the-rescue\"></a></h2><p>QEMU (Quick EMUlator) is quite the remarkable piece of software. It’s both an emulator and a virtualizer, and it also provides user-level emulation. Ehhh, what? Well, that’s what you get when you look up QEMU. Let’s put it in simpler terms:</p><ul><li><p>Emulator: It emulates hardware. It simulates entire systems (CPU, memory, disk, network, etc.) in software, meaning it exposes an interface to a guest program similar to actual hardware. Think about it: for an OS, all it sees is a bunch of CPU machine code that interacts with hardware and registers. If those registers and hardware behaviors are simulated in software, the OS is none the wiser and that’s exactly what QEMU does. You can simulate different CPUs (ARM, x86, RISC-V, etc.), run machine code instructions, and update state (registers, flags, program counter, etc.) as if you were running on real hardware, it’s just slower. By emulating CPUs in software, QEMU can run an OS built for the same or a different CPU architecture.</p></li><li><p>Virtualizer: Some CPUs offer hardware-assisted virtualization, basically, the CPU can differentiate between a guest and a host OS. This is a lot faster than using an emulator, but since you’re using the same CPU, you can only run a guest OS built for that CPU (for example, an x86 Linux guest on an x86 Linux host). This is supported in Linux through KVM. QEMU can make use of KVM, so when available, it’s better to use it for faster guest execution.</p></li><li><p>User-space emulation: This allows us to run a binary built for an architecture different from our machine’s by translating machine code and system calls on the fly. For instance,  works on an  CPU as if it were native. It’s truly magical, QEMU decodes ARM instructions and translates them into x86-64 ones, roughly:</p></li></ul><div><div><code><table><tbody><tr><td><pre>ARM code:          ADD R0, R1, R2\nQEMU intermediate: tcg_gen_add_i32(result, R1, R2)\nx86-64 host code:  mov eax,[R1]; add eax,[R2]; mov [R0],eax\n</pre></td></tr></tbody></table></code></div></div><p>So QEMU user-space emulation is the first piece of the cross-platform image puzzle.</p><p>The second piece is . It stands for <em>Binary Format Miscellaneous</em> (quite the name). The basic idea is that your Linux kernel knows how to run executables built for its own architecture. If you’re on an x86-64 CPU, your kernel can run x86-64 ELF files by default. It can’t run executables built for other architectures (like ARM) or other file types (like Windows  files or scripts).</p><p> is a kernel feature that allows us to specify an interpreter or program to handle certain files, based on their extension or on a magic byte sequence contained within the file. ARM Linux executables, for instance, have a distinguishable magic sequence:  We can configure  to use  whenever it encounters a file with that magic sequence. Similarly, we can configure it to use  when encountering files with a  extension.</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n</pre></td><td><pre> |  /proc/sys/fs/binfmt_misc/register\n\n\n./HelloWorld.jar \n +x /usr/local/bin/java-wrapper\n\n |  /proc/sys/fs/binfmt_misc/register\n\n./HelloWorld.jar\n</pre></td></tr></tbody></table></code></div></div><p>So,  lets the kernel specify a wrapper, interpreter, or command to run certain files based on their magic bytes or file extensions.</p><p>To recap:  allows us to execute binaries from other architectures, and  is the mechanism that maps those binaries (based on their magic bytes) to the appropriate QEMU user-space command.</p><p>In Docker’s <a href=\"https://docs.docker.com/build/building/multi-platform/#qemu\">documentation</a> about multi-platform builds, they explain that Docker Desktop supports multi-platform images with QEMU out of the box. (Docker Desktop is essentially a Linux VM tailored to run Docker, so it already has this configured.)</p><p>For Docker engine in Linux, we need to run:</p><div><div><code><table><tbody><tr><td><pre>docker run  tonistiigi/binfmt  all\n</pre></td></tr></tbody></table></code></div></div><p>This registers the  mappings (like we did above for Java and x86) but for all architectures.</p><p>The image <a href=\"https://github.com/tonistiigi/binfmt/blob/2062d3e3b27656ff1b19d762994567155b6fbdb2/cmd/binfmt/config.go#L22\">tonistiigi/binfmt</a> contains a Go binary that basically does what we demonstrated earlier, setting up mappings from ELF magic bytes to the appropriate QEMU binary for multiple architectures:</p><div><div><code><table><tbody><tr><td><pre>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n</pre></td><td><pre></pre></td></tr></tbody></table></code></div></div><p>In <a href=\"https://github.com/tonistiigi/binfmt/blob/2062d3e3b27656ff1b19d762994567155b6fbdb2/cmd/binfmt/main.go#L98\"></a>, we find this delightful snippet:</p><div><div><code><table><tbody><tr><td><pre></pre></td></tr></tbody></table></code></div></div><h2><a href=\"https://cefboud.com/posts/qemu-virtualzation-docker-multi-build/#the-final-piece-of-the-puzzle\"></a></h2><p>Ok, we know how foreign binaries are run. But how does it all tie together? How are we actually building these multi-platform images?</p><p>The Docker docs have a nice example:</p><div><div><code><table><tbody><tr><td><pre>\nFROM alpine\n\nRUN  /arch\n</pre></td></tr></tbody></table></code></div></div><div><div><code><table><tbody><tr><td><pre>docker buildx build linux/amd64,linux/arm64  letsgo:1.0 \n\ndocker run linux/arm64 letsgo:1.0  /arch\n\n\ndocker run linux/amd64 letsgo:1.0  /arch\n</pre></td></tr></tbody></table></code></div></div><p>It’s beautiful! So what happened exactly? By specifying <code>--platform=linux/amd64,linux/arm64</code>, we’re asking Docker to build two images, one for each platform. The pulled base layer () is platform-specific, and the binaries within it are built for each architecture. Let’s verify that:</p><div><div><code><table><tbody><tr><td><pre>docker run linux/arm64 letsgo:1.0 sh\napk add file \n\nfile  /bin/uname\n</pre></td></tr></tbody></table></code></div></div><p>Nice! The  runs , and that’s where the QEMU magic occurs. Under the hood, each binary in the image layers, compiled for its target architecture, is executed. Docker’s  instruction spawns a new process on the host (isolated within namespaces, but still just a process). Depending on the file’s magic bytes, the appropriate QEMU interpreter is invoked automatically via  and that RUN command works. If it was not for  and QEMU, we’d get a polite .</p><p>QEMU isn’t the only way to build multi-platform images. Docker Buildx supports using multiple builder nodes (a cluster), and you can use nodes with different architectures to build images natively for their respective platforms. There’s even a cloud offering built around this approach.</p><div><div><code><table><tbody><tr><td><pre>\ndocker buildx create  multiarch-builder unix:///var/run/docker.sock\n\ndocker buildx create  multiarch-builder ssh://user@arm64-host\ndocker buildx use multiarch-builder\n</pre></td></tr></tbody></table></code></div></div><p>For compilers that support cross-compilation (compiling code on one platform, the host, to create an executable for a different platform ,the target), like Go, which does so natively by specifying  and , you can build directly for each target architecture without relying on emulation. For example, in a Dockerfile build stage you might run:</p><div><div><code><table><tbody><tr><td><pre>RUN  go build  server </pre></td></tr></tbody></table></code></div></div><p>Then, you can simply copy the resulting binary into the runtime stage. Since the Go compiler supports cross-compilation, there’s no need to use QEMU here. Instead, we rely on the  and  environment variables provided automatically by Docker Buildx.</p>","contentLength":9470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olwc2d/understanding_multiplatform_docker_builds_with/"},{"title":"DigitalOcean is chasing me for $0.01: What it taught me about automation","url":"https://linuxblog.io/digitalocean-1-cent-automation/","date":1762014573,"author":"/u/modelop","guid":621,"unread":true,"content":"<p>There are three kinds of emails that can ruin a quiet Saturday: a security warning, an outage alert, and, apparently, a repeat reminder that you owe a cloud provider one cent, yes, $0.01. I’ve been using DigitalOcean since 2013. Personally, I don’t use it often, but I log in several times a week to support clients hosted there.</p><h2>A chuckle and twelve years of cloud love</h2><p>Over the past twelve years I have set up and managed countless droplets, and DigitalOcean’s support and uptime have been excellent; this isn’t that kind of post.</p><p>It’s a lighthearted look at what happens when automation churns out more notifications than the situation may warrant, and why even a penny‑sized bill can teach us bigger lessons about design and efficiency.</p><p>On Saturday, 25 October 2025, an email with the subject <em>“Payment required: Your pre‑payment has been used”</em> arrived in my inbox. It informed me that my prepaid credit was insufficient to cover the month’s usage and urged me to “make another payment or add an alternate payment method.”</p><p>There was just one catch: the outstanding balance was $0.01. I chuckled, and went on with my day only to receive the exact message two more times over the coming days. By the time Saturday rolled around, my inbox looked like this:</p><p>The inbox search screenshot above shows the cadence: identical “Payment required” messages on October 25th, 28th, and 31st, 2025, followed by an email on November 1, 2025, titled <em>“Your 2025‑10 invoice is available.”</em></p><p>The invoice email (screenshot also above) contains a table that lists the usage charges for October as $0.01, notes that the payment method will only be charged if the balance exceeds $3.00, and invites me to “View Invoice.” Here’s what those other three messages look like:</p><p>My immediate reaction was a bit of a chuckle, but by the fourth email, I was more curious than anything: Why<em> does an automated billing system send four emails about a 1-cent balance? </em><a href=\"https://docs.digitalocean.com/platform/billing/invoices/\" target=\"_blank\" rel=\"noopener\">DigitalOcean’s billing documentation</a> notes that invoices are generated monthly. In my case, the system sent several “action required” emails, maybe because I don’t have a payment method saved? But in any case, I rarely use my personal DigitalOcean account beyond just quick tests:</p><p>This experience of multiple emails for 1 cent owed, prompted me to think about the hidden costs of excessive email notifications and how we can design billing and alerting in a more thoughtful way.</p><h2>The True Cost of an Email</h2><p>Email feels free because individuals don’t pay per message, but providers do. A 2025 breakdown of email marketing costs notes that the typical cost for a business to send emails is <a href=\"https://www.mobiloud.com/blog/how-much-does-email-marketing-cost\" target=\"_blank\" rel=\"noopener\">$1–$2 per thousand messages</a>, translating to roughly $0.001–$0.002 per email. Amazon’s Simple Email Service charges $0.10 per 1,000 emails for outbound messages (sending or receiving) and a few cents per gigabyte for attachments.</p><p>This cost is likely less for DigitalOcean, with the three “Payment required” notices and one invoice with attachment costing the company at most between a tenth and two‑tenths of a cent to send. But multiply that by hundreds of thousands of customers, and it highlights how easy it is to use resources to clutter inboxes over microbalances.</p><p>The monetary cost is only part of the picture. Email has an environmental footprint because electricity powers servers, networks, and client devices. Researchers estimate that more than <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">306&nbsp;billion emails were sent in 2021</a>, and the total is expected to hit almost 400 billion this year, thanks to DigitalOcean. jk!!</p><p>According to <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">Mike Berners‑Lee</a>, a short text email can produce 0.2–0.3 g of CO₂, while a longer message with attachments can produce 17 g; an email blast to 100 people may generate 26 g or more. Email‑related emissions accounted for approximately 150 million tons of CO₂e in 2019. That’s <a href=\"https://carbonliteracy.com/the-carbon-cost-of-an-email/\" target=\"_blank\" rel=\"noopener\">about 0.3% of the world’s carbon footprint</a>. But more importantly, about 25% added to users’ annoyance levels – Source: </p><h2>Notification fatigue and design principles</h2><p>It isn’t just about costs or the environment. Usability tests consistently <a href=\"https://www.smashingmagazine.com/2025/07/design-guidelines-better-notifications-ux/\" target=\"_blank\" rel=\"noopener\">show that</a> frequent alerts are one of the top user complaints. In fact, it’s been proven by Facebook and others that sending fewer notifications can be better for both engagement and retention.</p><p>Good notification design also recognizes levels of severity: high‑attention alerts (e.g., security breaches or failed payments) should prompt immediate action, while low‑attention messages (informational updates) can be bundled or deferred. Services like Slack, for example, <a href=\"https://www.smashingmagazine.com/2025/07/design-guidelines-better-notifications-ux/\" target=\"_blank\" rel=\"noopener\">adapt notification frequency</a> automatically when channels become very active.</p><p>Looking at DigitalOcean’s billing reminders, it’s easy to see opportunities for improvement. A one‑cent balance does not warrant three emails + an invoice. The first message could have been informational (<em>“heads up, your balance is low”</em>), the second might wait until the balance crosses a predetermined threshold (say $1 or $3), and the third could be a month or 3 months later.</p><p>Alternatively, DigitalOcean could incorporate a small balance waiver similar to the one many credit card issuers use.&nbsp;Banks recognize that it’s not cost‑effective to chase pennies; they round down or apply a credit adjustment on the next statement. The same logic could help cloud providers reduce overhead and user frustration.</p><h2>It’s not just DigitalOcean: micro‑balances happen everywhere</h2><p>DigitalOcean isn’t alone in sending tiny bills. Back in 2013, an <a href=\"https://www.optus.com.au/\" target=\"_blank\" rel=\"noopener\">Optus</a> customer in Australia <a href=\"https://forums.whirlpool.net.au/archive/2174002#:~:text=posted%202013,27%2C%2010%3A26%20pm%20AEST\" target=\"_blank\" rel=\"noopener\">posted on Whirlpool forums</a> that a billing error left them with a one‑cent overdue notice after receiving a reimbursement. One commenter wrote that it would cost the company <em>“more in personnel overheads to deal with this stupid billing error, than what it’s worth”</em>, while another explained, <em>“It’s an automated system, mate. Just relax.”</em></p><p>The moral of the story is that most companies rely on automated billing scripts, and without sensible thresholds, they’ll dutifully produce statements for even the most trivial amounts.</p><p>In practice, if you owe 99 cents or less, <a href=\"https://www.doctorofcredit.com/small-balance-waiver-a-k-a-lots-of-free-99-cent-amazon-gcs/\" target=\"_blank\" rel=\"noopener\">many companies</a> apply a credit adjustment and report a zero balance. The banking industry has recognized that goodwill and efficiency outweigh the pennies left on the ledger. If major financial institutions can swallow a dollar, cloud platforms with higher margins can too.</p><h2>What this taught me and how I’ve been guilty too</h2><p>As someone who deploys systems and manages mail servers, I can’t throw stones without acknowledging my own missteps. Earlier this year, I built <a href=\"https://dewedda.com/\" target=\"_blank\" rel=\"noopener\">dewedda.com</a>, a storm‑watch website for the Eastern Caribbean. Part of which was to send automatic email alerts to subscribers when storms approached islands within specific distances and directions.</p><p>In testing, everything looked great: the algorithm computed wind fields, adjusted for intensity, and tracked dozens of scenarios. But the first time a real storm approached, my code started hammering subscribers with unnecessary alerts. It didn’t account for storms that curved away or systems that were still unnamed, resulting in duplicates, so people kept receiving warnings even when there was no threat or duplicate emails. I had to scramble to adjust the logic.</p><p>The experience taught me humility and the importance of edge cases, and it makes me more sympathetic to DigitalOcean’s engineers. Building resilient billing and notification systems is complex. Edge cases arise when accounts straddle billing cycles, use promotional credits, or move between team and personal billing. Legacy code and third‑party integrations can behave unpredictably. What matters is how we learn from these events.</p><h2>Conclusion (yes, I paid that one cent)</h2><p>I paid the 1 cent balance owed to DigitalOcean. But who covers that transaction cost?</p><p>In the end, I did what any responsible business owner would do: I logged into my account and paid the one‑cent balance. Because, it would sit there for months, I only used a droplet for ~1 hour to test something. My personal DigitalOcean account goes mostly unused. So paying this invoice also means no recurring emails to pay my bill. Maybe that’s their plan? Ha!</p><p>I hope this article highlights the hidden inefficiencies that creep into automation, whether it’s cloud invoices, marketing emails, or storm alerts.</p><p>I still recommend DigitalOcean to friends and clients. They offer a great product at a fair price, with transparent billing. Being able to spin up a droplet in a few seconds makes life easy for Linux nerds like me. This one‑cent episode doesn’t change that; it simply underscores the value of thoughtful notification design. There are no affiliate links in this post either.</p><p>In summary, as repeatedly proven, sending fewer, more relevant notifications improves user satisfaction and retention. The environmental data also shows that unnecessary emails carry hidden costs, and financial industry practices demonstrate that forgiving tiny balances can be cheaper than collecting them.</p><p>A bit of humor on a Saturday morning turned into a lesson for all of us on building better systems. And yes, just in case the automated script is listening, I can confirm that as of writing this, my DigitalOcean account balance is zero.</p>","contentLength":9237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ols6mk/digitalocean_is_chasing_me_for_001_what_it_taught/"},{"title":"Hard Rust requirements from May onward for all Debian ports","url":"https://lists.debian.org/debian-devel/2025/10/msg00285.html","date":1762000364,"author":"/u/pyeri","guid":620,"unread":true,"content":"<pre>Hi all,\n\nI plan to introduce hard Rust dependencies and Rust code into\nAPT, no earlier than May 2026. This extends at first to the\nRust compiler and standard library, and the Sequoia ecosystem.\n\nIn particular, our code to parse .deb, .ar, .tar, and the\nHTTP signature verification code would strongly benefit\nfrom memory safe languages and a stronger approach to\nunit testing.\n\nIf you maintain a port without a working Rust toolchain,\nplease ensure it has one within the next 6 months, or\nsunset the port.\n\nIt's important for the project as whole to be able to\nmove forward and rely on modern tools and technologies\nand not be held back by trying to shoehorn modern software\non retro computing devices.\n\nThank you for your understanding.\n-- \ndebian developer - deb.li/jak | jak-linux.org - free software dev\nubuntu core developer                              i speak de, en\n</pre>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olmnhx/hard_rust_requirements_from_may_onward_for_all/"},{"title":"Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)","url":"https://ar5iv.labs.arxiv.org/html/1901.09056","date":1761989253,"author":"/u/Zomgnerfenigma","guid":619,"unread":true,"content":"<h5>The Challenge of Benchmarking WebAssembly</h5><div><p>The aforementioned suite of 24 benchmarks is the PolybenchC benchmark\nsuite&nbsp;, which is designed to measure the effect of\npolyhedral loop optimizations in compilers. All the benchmarks in the\nsuite are small scientific computing kernels rather than full\napplications (e.g., matrix multiplication and LU Decomposition); each is\nroughly 100 LOC. While WebAssembly is designed to accelerate scientific\nkernels on the Web, it is also explicitly designed for a much richer set\nof full applications.</p></div><div><p>The WebAssembly documentation highlights several intended use\ncases&nbsp;, including scientific kernels, image editing,\nvideo editing, image recognition, scientific visualization, simulations,\nprogramming language interpreters, virtual machines, and POSIX applications.\nTherefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC\ndo not imply that it will perform well given a different kind of application.</p></div><div><p>We argue that a more comprehensive evaluation of WebAssembly should rely on an\nestablished benchmark suite of large programs, such as the SPEC CPU benchmark\nsuites. In fact, the SPEC CPU 2006 and 2017 suite of\nbenchmarks include several applications that fall under the intended use cases of\nWebAssembly: eight benchmarks are scientific applications (e.g., ,\n, , , and\n), two benchmarks involve image and video processing\n( and ), and all of the benchmarks are POSIX\napplications.</p></div><div><p>Unfortunately, it is not possible to simply compile a sophisticated\nnative program to WebAssembly. Native programs, including the programs in\nthe SPEC CPU suites, require operating system services, such as a\nfilesystem, synchronous I/O, and processes, which WebAssembly and the\nbrowser do not provide. The SPEC benchmarking harness itself requires\na file system, a shell, the ability to spawn processes, and other Unix\nfacilities. To overcome these limitations when porting native\napplications to the web, many programmers painstakingly modify their\nprograms to avoid or mimic missing operating system\nservices. Modifying well-known benchmarks, such as SPEC CPU, would not\nonly be time consuming but would also pose a serious threat to\nvalidity.</p></div><div><p>The standard approach to running these applications today is to use\nEmscripten, a toolchain for compiling C and C++ to\nWebAssembly&nbsp;. Unfortunately, Emscripten only supports\nthe most trivial system calls and does not scale up to large-scale\napplications. For example, to enable applications to use synchronous\nI/O, the default Emscripten  filesystem loads the entire\nfilesystem image into memory before the program begins executing. For\nSPEC, these files are too large to fit into memory.</p></div><div><p>A promising alternative is to use , a framework that enables\nrunning unmodified, full-featured Unix applications in the\nbrowser&nbsp;.  implements\na Unix-compatible kernel in JavaScript, with full support for\nprocesses, files, pipes, blocking I/O, and other Unix features.\nMoreover, it includes a C/C++ compiler (based on Emscripten)\nthat allows programs to run in the browser\nunmodified. The  case studies include complex applications,\nsuch as , which runs entirely in the browser without any\nsource code modifications.</p></div><div><p>Unfortunately,  is a JavaScript-only solution, since it was\nbuilt before the release of\nWebAssembly. Moreover,  suffers from high performance overhead,\nwhich would be a significant confounder while benchmarking. Using ,\nit would be difficult to tease apart the poorly performing benchmarks\nfrom performance degradation introduced by .</p></div>","contentLength":3526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1oljj3v/not_so_fast_analyzing_the_performance_of/"},{"title":"Programming Language Agnostic Naming Conventions","url":"https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide","date":1761982250,"author":"/u/Distinct-Panic-246","guid":618,"unread":true,"content":"<p>There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton</p><blockquote><p>\"There are only two hard things in Computer Science: cache invalidation and naming things\"</p></blockquote><p>(Or the slight variation of <em>\"There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors\"</em>)</p><p>But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.</p><h2>Why we spend time naming things correctly</h2><p>If you see a variable called , you can probably assume it is a boolean.  or  are not clear.</p><h3>Avoid Negative variable names:</h3><p>Negative names can lead to double negatives, which are confusing.</p><ul></ul><p>Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.</p><p>(Although  is probably an exception where it should always be used over ).</p><ul></ul><h3>Pick a language and always use that</h3><p>If you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .</p><p>I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)</p><ul></ul><h3>Make booleans obvious by using is/has prefix</h3><p>If you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a boolean</p><ul></ul><p>Words like , ,  are too generic. Try to avoid these terms</p><p>Pick a convention for naming things, and use those everywhere.calculateAmount</p><ul><li>Bad 👎:  and </li><li>Good 👍:  and </li></ul><p>Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice)</p><ul><li> for class names</li><li> for most other variables</li><li> for static constants</li></ul><h2>Common names for specific things</h2><p>If you're taking some data and  it to a different shape or different values then  is a common and accurate name.</p><pre><div><div><div><code></code></div></div></div></pre><p>If you need to check if data is valid/correct, then its almost always called a validator.</p><pre><div><div><div><code></code></div></div></div></pre><p>Used when describing the shape of some data structure. Often used with database designs.</p><pre><div><div><div><code></code></div></div></div></pre><p>When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very related</p><pre><div><div><div><code></code></div></div></div></pre><p>For code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that data</p><pre><div><div><div><code></code></div></div></div></pre><p>When you have some functionality with a specific interface, and you need to convert it to another interface/shape.</p><p>It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)</p><p>When you need to make data uniform in scale, format, or structure</p><pre><div><div><div><code></code></div></div></div></pre>","contentLength":3010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olht95/programming_language_agnostic_naming_conventions/"},{"title":"IRS open-sourced the fact graph it uses for tax law","url":"https://github.com/IRS-Public/fact-graph","date":1761958132,"author":"/u/R2_SWE2","guid":622,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1olb241/irs_opensourced_the_fact_graph_it_uses_for_tax_law/"}],"tags":["dev"]}