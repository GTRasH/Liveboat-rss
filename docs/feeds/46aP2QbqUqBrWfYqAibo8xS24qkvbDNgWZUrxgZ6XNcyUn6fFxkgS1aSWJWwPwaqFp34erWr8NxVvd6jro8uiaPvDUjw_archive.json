{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"EKS 1.33 cause networking issue when running very high mqtt traffic","url":"https://www.reddit.com/r/kubernetes/comments/1olx02b/eks_133_cause_networking_issue_when_running_very/","date":1762026136,"author":"/u/imuskie","guid":632,"unread":true,"content":"<p>Let's say I'm running some high workload on AWS EKS (mqtt traffic from devices). I'm using VerneMQ broker for this. Everything have worked fine until I've upgraded the cluster to 1.33.</p><p>The flow is like this: mqtt traffic -&gt; ALB (vernemq port) -&gt; vernemq kubernetes service -&gt; vernemq pods.</p><p>There is another pod which subscribes to a topic and reads something from vernemq (some basic stuff). The issue is that, after the upgrade, that pod fails to reach the vernemq pods. (pod crashes its liveness probe/timeouts). </p><p>This happens only when I get very high mqtt traffic on ALB (hundreds of thousands of requests). For low traffic everything works fine. One workaround I've found is to edit that container image code to connect to vernemq using external ALB instead of vernemq kubernetes service (with this change, the issue is fixed) but I don't want this. </p><p>I did not change anything on infrastructure/container code. I'm running on EKS since 1.27.</p><p>I don't know if the base AMI is the problem or not (like kernel configs have changed).</p><p>I'm running in AL2023, so with the base AMI on eks 1.32 works fine, but with 1.33 it does not.</p><p>I'm using amazon aws vpc cni plugin for networking.</p><p>Are there any tools to inspect the traffic/kernel calls or to better monitor this issue? </p>","contentLength":1262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"unsupportedConfigOverrides USAGE","url":"https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/","date":1762005147,"author":"/u/BigBprofessional","guid":631,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)","url":"https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/","date":1762003325,"author":"/u/Dependent_Concert446","guid":630,"unread":true,"content":"<p>I’m looking for some advice on how to organize our <strong>Helm charts and Bitbucket repos</strong> for a growing  setup.</p><p>We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).</p><p>For our , we created  that’s used for microservices. We <strong>don’t have separate repos for each microservice</strong> — all are managed under the same project.</p><p>Here’s a simplified view of the repo structure:</p><pre><code>app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments </code></pre><p>The following  are also in the same repo right now:</p><ul><li><strong>Project Contour (Ingress)</strong></li><li><em>(and other cluster-level tools like k3s, Longhorn, etc.)</em></li></ul><p>These are <strong>not tied to the application project</strong> — they’re might shared and deployed across <strong>multiple clusters and environments</strong>.</p><ol><li>Should I move these shared infra components into a <strong>separate “infra” Bitbucket repo</strong> (including their Helm charts, Terraform, and Ansible configs)?</li><li>For GitOps with , would it make more sense to split things like this: <ul><li> → all microservices + base Helm chart</li><li> → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)</li></ul></li><li>How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps?</li></ol><p> Used AI to help write and format this post for grammar and readability.</p>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created Open Source Kubernetes tool called Forkspacer to fork entire environments + dataplane, it is like git but for kubernetes.","url":"https://www.reddit.com/r/kubernetes/comments/1olk9we/i_created_open_source_kubernetes_tool_called/","date":1761992135,"author":"/u/Laughing-Dawg","guid":633,"unread":true,"content":"<p>I created an open-source tool that lets you create, fork, and hibernate entire Kubernetes environments.</p><p>With , you can fork your deployments while also migrating your data.. not just the manifests, but the entire data plane as well. We support different modes of forking: by default, every fork spins up a managed, dedicated virtual cluster, but you can also point the destination of your fork to a self-managed cluster. You can even set up multi-cloud environments and fork an environment from one provider (e.g., AWS) to another (e.g., GKE, AKE, or on-prem).</p><p>You can clone full setups, test changes in isolation, and automatically hibernate idle workspaces to save resources all declaratively, with GitOps-style reproducibility.</p><p>It’s especially useful for spinning up dev, test, pre-prod, and prod environments, and for teams where each developer needs a personal, forked environment from a shared baseline.</p><p><strong><em>License is Apace 2.0 and it is written in Go using Kubebuilder SDK</em></strong></p><p>Please give it a try let me know, thank you</p>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Certification help requests, vents, and brags","url":"https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/","date":1761991272,"author":"/u/thockin","guid":629,"unread":true,"content":"<p>Did you pass a cert? Congratulations, tell us about it!</p><p>Did you bomb a cert exam and want help? This is the thread for you.</p><p>Do you just hate the process? Complain here.</p><p>(Note: other certification related posts will be removed)</p>","contentLength":223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Who is hiring?","url":"https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/","date":1761991234,"author":"/u/gctaylor","guid":634,"unread":true,"content":"<div><p>This monthly post can be used to share Kubernetes-related job openings within  company. Please include:</p><ul><li>Location requirements (or lack thereof)</li><li>At least one of: a link to a job posting/application page or contact details</li></ul><p>If you are interested in a job, please contact the poster directly. </p><p>Common reasons for comment removal:</p><ul><li>Not meeting the above requirements</li><li>Recruiter post / recruiter listings</li><li>Negative, inflammatory, or abrasive tone</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}